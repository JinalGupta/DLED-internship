{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bDjVWKLe15C"
      },
      "outputs": [],
      "source": [
        "import yt_dlp\n",
        "import os\n",
        "\n",
        "# Step 1: Download Audio from YouTube with Cookies\n",
        "def download_audio(youtube_url, output_path=\"audio.mp3\"):\n",
        "    ydl_opts = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'outtmpl': output_path.replace('.mp3', ''),  # Remove .mp3 for yt-dlp to handle extension\n",
        "        'postprocessors': [{\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'mp3',\n",
        "            'preferredquality': '192',\n",
        "        }],\n",
        "        'cookiefile': 'cookies.txt'  # Use cookies to bypass restrictions\n",
        "    }\n",
        "\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        ydl.download([youtube_url])\n",
        "\n",
        "    # Fix the file name if it ends up as audio.mp3.mp3\n",
        "    downloaded_file = output_path.replace('.mp3', '.mp3.mp3')\n",
        "    if os.path.exists(downloaded_file):\n",
        "        os.rename(downloaded_file, output_path)\n",
        "\n",
        "# Replace with your YouTube link\n",
        "youtube_url = \"https://youtu.be/sK8SILOM37I\"\n",
        "download_audio(youtube_url)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install yt_dlp"
      ],
      "metadata": {
        "id": "rZp4FDUSe8Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "# Path to the downloaded audio file\n",
        "audio_file = \"audio.mp3\"\n",
        "\n",
        "# Play the audio\n",
        "Audio(audio_file)"
      ],
      "metadata": {
        "id": "g8igNtO3fIZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install youtube_transcript_api"
      ],
      "metadata": {
        "id": "VLVyvDCiiZeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pydub"
      ],
      "metadata": {
        "id": "TLFlGshFielD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install SpeechRecognition"
      ],
      "metadata": {
        "id": "69QBpSoJijPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pytube"
      ],
      "metadata": {
        "id": "3m9mOIDIimSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install deepmultilingualpunctuation"
      ],
      "metadata": {
        "id": "SMOsU3QmlhGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import urllib.parse\n",
        "import requests\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from pytube import YouTube\n",
        "import speech_recognition as sr\n",
        "from pydub import AudioSegment\n",
        "from deepmultilingualpunctuation import PunctuationModel\n",
        "import os\n",
        "# Initialize model once at the top\n",
        "model = PunctuationModel()\n",
        "\n",
        "def extract_video_id(video_url):\n",
        "    \"\"\"\n",
        "    Extracts the YouTube video ID from various URL formats.\n",
        "    \"\"\"\n",
        "    parsed_url = urllib.parse.urlparse(video_url)\n",
        "    query_params = urllib.parse.parse_qs(parsed_url.query)\n",
        "\n",
        "    if \"v\" in query_params:\n",
        "        return query_params[\"v\"][0]\n",
        "\n",
        "    match = re.search(r\"(youtu\\.be/|youtube\\.com/embed/|youtube\\.com/shorts/)([\\w-]+)\", video_url)\n",
        "    if match:\n",
        "        return match.group(2)\n",
        "\n",
        "    return None\n",
        "\n",
        "def download_audio(video_url):\n",
        "    \"\"\"\n",
        "    Downloads the audio using yt-dlp with cookies and returns the file path.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ydl_opts = {\n",
        "            'format': 'bestaudio/best',\n",
        "            'outtmpl': 'audio.%(ext)s',\n",
        "            'cookiefile': 'cookies (1).txt',  # Use the exported cookies\n",
        "            'postprocessors': [{\n",
        "                'key': 'FFmpegExtractAudio',\n",
        "                'preferredcodec': 'mp3',\n",
        "                'preferredquality': '192',\n",
        "            }],\n",
        "        }\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(video_url, download=True)\n",
        "            return \"audio.mp3\"\n",
        "    except Exception as e:\n",
        "        return f\"Error downloading audio: {str(e)}\"\n",
        "\n",
        "def convert_audio_to_wav(audio_file):\n",
        "    \"\"\"\n",
        "    Converts the downloaded MP3 audio to WAV format using pydub.\n",
        "    \"\"\"\n",
        "    wav_file = \"audio.wav\"\n",
        "    try:\n",
        "        AudioSegment.from_mp3(audio_file).export(wav_file, format=\"wav\")\n",
        "        return wav_file\n",
        "    except Exception as e:\n",
        "        return f\"Error converting to WAV: {str(e)}\"\n",
        "\n",
        "def transcribe_audio(audio_path, chunk_length=30):\n",
        "    \"\"\"\n",
        "    Splits audio into smaller chunks, transcribes each chunk separately,\n",
        "    and adds punctuation using deepmultilingualpunctuation library.\n",
        "    \"\"\"\n",
        "    recognizer = sr.Recognizer()\n",
        "    audio = AudioSegment.from_wav(audio_path)\n",
        "    total_duration = len(audio) / 1000  # Convert to seconds\n",
        "    transcribed_text = []\n",
        "\n",
        "    # Load punctuation model\n",
        "    model = PunctuationModel()\n",
        "\n",
        "    print(\"Transcribing audio in chunks...\")\n",
        "\n",
        "    # In transcribe_audio()\n",
        "    punctuated_chunks = []\n",
        "    for chunk_text in transcribed_text:\n",
        "        punctuated = model.restore_punctuation(chunk_text)\n",
        "        punctuated_chunks.append(punctuated)\n",
        "        return \" \".join(punctuated_chunks)\n",
        "\n",
        "    # Split and transcribe audio in chunks\n",
        "    for start in range(0, int(total_duration), chunk_length):\n",
        "        end = min(start + chunk_length, int(total_duration))\n",
        "        chunk = audio[start * 1000:end * 1000]  # Extract chunk in milliseconds\n",
        "        chunk.export(\"chunk.wav\", format=\"wav\")  # Save chunk temporarily\n",
        "\n",
        "        with sr.AudioFile(\"chunk.wav\") as source:\n",
        "            try:\n",
        "                audio_data = recognizer.record(source)\n",
        "                text = recognizer.recognize_google(audio_data)\n",
        "                transcribed_text.append(text)\n",
        "            except sr.UnknownValueError:\n",
        "                transcribed_text.append(\"[Unintelligible]\")\n",
        "            except sr.RequestError as e:\n",
        "                return f\"Error with the speech recognition service: {str(e)}\"\n",
        "\n",
        "    os.remove(\"chunk.wav\")  # Clean up temporary chunk file\n",
        "\n",
        "    # Combine chunks and add punctuation\n",
        "    combined_text = \" \".join(transcribed_text)\n",
        "    punctuated_text = model.restore_punctuation(combined_text)\n",
        "\n",
        "    return punctuated_text\n",
        "\n",
        "def get_transcript_unlisted(video_url):\n",
        "    \"\"\"\n",
        "    Tries to fetch the transcript using youtube_transcript_api first,\n",
        "    then falls back to downloading and transcribing audio if necessary.\n",
        "    \"\"\"\n",
        "    model = PunctuationModel()  # Initialize once\n",
        "    video_id = extract_video_id(video_url)\n",
        "\n",
        "    if not video_id:\n",
        "        return \"Invalid YouTube URL.\"\n",
        "\n",
        "    # Try API path with punctuation\n",
        "    try:\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        raw_text = \" \".join([item['text'] for item in transcript])\n",
        "        return model.restore_punctuation(raw_text)  # <-- Critical fix\n",
        "    except:\n",
        "        print(\"Transcript not available via API, attempting audio transcription...\")\n",
        "\n",
        "    # Audio fallback path (existing implementation)\n",
        "    # ... rest of audio processing code ...\n",
        "    # Download and transcribe audio if no transcript is available\n",
        "    audio_file = download_audio(video_url)\n",
        "    if \"Error\" in audio_file:\n",
        "        return audio_file\n",
        "\n",
        "    wav_file = convert_audio_to_wav(audio_file)\n",
        "    if \"Error\" in wav_file:\n",
        "        return wav_file\n",
        "\n",
        "    transcription = transcribe_audio(wav_file)\n",
        "\n",
        "    # Cleanup temporary files\n",
        "    os.remove(audio_file)\n",
        "    os.remove(wav_file)\n",
        "\n",
        "    return transcription\n",
        "\n",
        "# Example usage\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    video_url = input(\"Enter the YouTube video URL: \")\n",
        "    transcript = get_transcript_unlisted(video_url)\n",
        "\n",
        "    # Save transcript to a text file\n",
        "    if \"Error\" not in transcript and \"Invalid YouTube URL.\" not in transcript:\n",
        "        output_file = \"transcript.txt\"\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(transcript)\n",
        "        print(f\"\\nTranscript saved successfully to {output_file}\")\n",
        "    else:\n",
        "        print(\"\\n\", transcript)"
      ],
      "metadata": {
        "id": "Uv_sgZfhfcHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "def format_transcript_sentences(input_file, output_file=None):\n",
        "    \"\"\"\n",
        "    Processes a transcript text file to add line breaks after sentences.\n",
        "\n",
        "    Args:\n",
        "        input_file: Path to the original transcript file\n",
        "        output_file: Path for formatted file (default: adds '_formatted' suffix)\n",
        "\n",
        "    Returns:\n",
        "        Path to the formatted file or error message\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read input file\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            raw_text = f.read().replace('\\n', ' ')  # Remove existing newlines\n",
        "\n",
        "        # Split into sentences using punctuation followed by whitespace\n",
        "        sentences = re.split(r'(?<=[.!?]) +', raw_text)\n",
        "\n",
        "        # Format with each sentence on new line and proper capitalization\n",
        "        formatted_text = []\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            if sentence:\n",
        "                # Capitalize first letter of each sentence\n",
        "                formatted_sentence = sentence[0].upper() + sentence[1:]\n",
        "                formatted_text.append(formatted_sentence)\n",
        "\n",
        "        formatted_text = '\\n'.join(formatted_text)\n",
        "\n",
        "        # Create output filename if not provided\n",
        "        if not output_file:\n",
        "            base, ext = os.path.splitext(input_file)\n",
        "            output_file = f\"{base}_formatted{ext}\"\n",
        "\n",
        "        # Write formatted text\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(formatted_text)\n",
        "\n",
        "        return output_file\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        return f\"Error: File '{input_file}' not found\"\n",
        "    except Exception as e:\n",
        "        return f\"Error processing file: {str(e)}\"\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    input_path = input(\"Enter path to transcript file: \").strip()\n",
        "    result = format_transcript_sentences(input_path)\n",
        "\n",
        "    if \"Error\" in result:\n",
        "        print(f\"\\n{result}\")\n",
        "    else:\n",
        "        print(f\"\\nFormatted transcript saved to: {result}\")\n",
        "        print(\"\\nFirst 5 lines of formatted text:\")\n",
        "        with open(result, 'r', encoding='utf-8') as f:\n",
        "            print(''.join(f.readlines()[:5]))"
      ],
      "metadata": {
        "id": "R3GkXicujGgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def semantic_segmentation(input_file, output_file=None, min_length=3, threshold=0.65):\n",
        "    \"\"\"\n",
        "    Segments text into meaningful chunks with semantic coherence and keywords.\n",
        "\n",
        "    Args:\n",
        "        input_file: Path to formatted transcript file\n",
        "        output_file: Output path (default: adds '_segmented' suffix)\n",
        "        min_length: Minimum sentences per segment\n",
        "        threshold: Semantic similarity threshold (0-1)\n",
        "\n",
        "    Returns:\n",
        "        Path to segmented file or error message\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load ML models\n",
        "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        # Read and split sentences\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            sentences = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "        if len(sentences) < min_length:\n",
        "            return f\"Need at least {min_length} sentences for segmentation\"\n",
        "\n",
        "        # Generate sentence embeddings\n",
        "        embeddings = model.encode(sentences)\n",
        "\n",
        "        # Create segments with semantic coherence\n",
        "        segments = []\n",
        "        current_segment = []\n",
        "        current_emb = None\n",
        "\n",
        "        for sent, emb in zip(sentences, embeddings):\n",
        "            emb = emb.reshape(1, -1)\n",
        "            if not current_segment:\n",
        "                current_segment.append(sent)\n",
        "                current_emb = emb\n",
        "                continue\n",
        "\n",
        "            similarity = cosine_similarity(current_emb, emb)[0][0]\n",
        "            if similarity >= threshold and len(current_segment) < 5:\n",
        "                current_segment.append(sent)\n",
        "                current_emb = (current_emb * len(current_segment) + emb) / (len(current_segment) + 1)\n",
        "            else:\n",
        "                if len(current_segment) >= min_length:\n",
        "                    segments.append(current_segment)\n",
        "                current_segment = [sent]\n",
        "                current_emb = emb\n",
        "\n",
        "        # Finalize remaining sentences\n",
        "        if current_segment:\n",
        "            if segments and len(current_segment) < min_length:\n",
        "                segments[-1].extend(current_segment)\n",
        "            else:\n",
        "                segments.append(current_segment)\n",
        "\n",
        "        # Extract keywords for each segment\n",
        "        results = []\n",
        "        for seg in segments:\n",
        "            vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
        "            X = vectorizer.fit_transform([' '.join(seg)])\n",
        "            features = vectorizer.get_feature_names_out()\n",
        "            keywords = features[np.argsort(X.toarray())[0][-3:]][::-1]\n",
        "\n",
        "            results.append({\n",
        "                'sentences': seg,\n",
        "                'keywords': keywords,\n",
        "                'count': len(seg)\n",
        "            })\n",
        "\n",
        "        # Create output filename\n",
        "        if not output_file:\n",
        "            base, ext = os.path.splitext(input_file)\n",
        "            output_file = f\"{base}_segmented{ext}\"\n",
        "\n",
        "        # Write segmented output\n",
        "        with open(output_file, \"w\") as f:\n",
        "            for i, seg in enumerate(results, 1):\n",
        "                f.write(f\"Segment {i} ({seg['count']} sentences | Keywords: {', '.join(seg['keywords'])})\\n\")\n",
        "                f.write('\\n'.join(seg['sentences']) + '\\n\\n')\n",
        "\n",
        "        return output_file\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error during segmentation: {str(e)}\"\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    input_path = input(\"Enter path to formatted transcript file: \").strip()\n",
        "    result = semantic_segmentation(input_path)\n",
        "\n",
        "    if \"Error\" in result:\n",
        "        print(f\"\\n{result}\")\n",
        "    else:\n",
        "        print(f\"\\nSegmented transcript saved to: {result}\")"
      ],
      "metadata": {
        "id": "HetRhlMv1ySw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "\n",
        "def semantic_segmentation(input_file, output_file=None, min_length=3, threshold=0.65):\n",
        "    \"\"\"\n",
        "    Segments text into meaningful chunks with semantic coherence and keywords.\n",
        "\n",
        "    Args:\n",
        "        input_file: Path to formatted transcript file\n",
        "        output_file: Output path (default: adds '_segmented' suffix)\n",
        "        min_length: Minimum sentences per segment\n",
        "        threshold: Semantic similarity threshold (0-1)\n",
        "\n",
        "    Returns:\n",
        "        Path to segmented file or error message\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load ML models\n",
        "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        # Read and split sentences\n",
        "        with open(input_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            sentences = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "        if len(sentences) < min_length:\n",
        "            return f\"Need at least {min_length} sentences for segmentation\"\n",
        "\n",
        "        # Generate sentence embeddings\n",
        "        embeddings = model.encode(sentences)\n",
        "\n",
        "        # Create segments with semantic coherence\n",
        "        segments = []\n",
        "        current_segment = []\n",
        "        current_emb = None\n",
        "\n",
        "        for i, (sent, emb) in enumerate(zip(sentences, embeddings)):\n",
        "            emb = emb.reshape(1, -1)\n",
        "            if not current_segment:\n",
        "                current_segment.append(sent)\n",
        "                current_emb = emb\n",
        "                continue\n",
        "\n",
        "            similarity = cosine_similarity(current_emb, emb)[0][0]\n",
        "            if similarity >= threshold and len(current_segment) < 5:\n",
        "                current_segment.append(sent)\n",
        "                current_emb = (current_emb * len(current_segment) + emb) / (len(current_segment) + 1)\n",
        "            else:\n",
        "                # Finalize segment if it meets minimum length\n",
        "                if len(current_segment) >= min_length:\n",
        "                    segments.append(current_segment)\n",
        "                else:\n",
        "                    # If too short, append to previous segment if possible\n",
        "                    if segments:\n",
        "                        segments[-1].extend(current_segment)\n",
        "                    else:\n",
        "                        segments.append(current_segment)\n",
        "                current_segment = [sent]\n",
        "                current_emb = emb\n",
        "\n",
        "            # Force finalize segment at the end of the file\n",
        "            if i == len(sentences) - 1:\n",
        "                if len(current_segment) >= min_length:\n",
        "                    segments.append(current_segment)\n",
        "                else:\n",
        "                    # If too short, append to previous segment if possible\n",
        "                    if segments:\n",
        "                        segments[-1].extend(current_segment)\n",
        "                    else:\n",
        "                        segments.append(current_segment)\n",
        "\n",
        "        # Extract keywords for each segment\n",
        "        results = []\n",
        "        for seg in segments:\n",
        "            vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
        "            X = vectorizer.fit_transform([' '.join(seg)])\n",
        "            features = vectorizer.get_feature_names_out()\n",
        "            keywords = features[np.argsort(X.toarray())[0][-3:]][::-1]\n",
        "\n",
        "            results.append({\n",
        "                'sentences': seg,\n",
        "                'keywords': keywords,\n",
        "                'count': len(seg)\n",
        "            })\n",
        "\n",
        "        # Create output filename\n",
        "        if not output_file:\n",
        "            base, ext = os.path.splitext(input_file)\n",
        "            output_file = f\"{base}_segmented{ext}\"\n",
        "\n",
        "        # Write segmented output\n",
        "        with open(output_file, \"w\") as f:\n",
        "            for i, seg in enumerate(results, 1):\n",
        "                f.write(f\"Segment {i} ({seg['count']} sentences | Keywords: {', '.join(seg['keywords'])})\\n\")\n",
        "                f.write('\\n'.join(seg['sentences']) + '\\n\\n')\n",
        "\n",
        "        return output_file\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error during segmentation: {str(e)}\"\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    input_path = input(\"Enter path to formatted transcript file: \").strip()\n",
        "    result = semantic_segmentation(input_path)\n",
        "\n",
        "    if \"Error\" in result:\n",
        "        print(f\"\\n{result}\")\n",
        "    else:\n",
        "        print(f\"\\nSegmented transcript saved to: {result}\")"
      ],
      "metadata": {
        "id": "Rq-nhIaB4bTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now doing the transcription with timestamps"
      ],
      "metadata": {
        "id": "80FkWpsn5BMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import urllib.parse\n",
        "import requests\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from pytube import YouTube\n",
        "import speech_recognition as sr\n",
        "from pydub import AudioSegment\n",
        "from deepmultilingualpunctuation import PunctuationModel\n",
        "import os\n",
        "# Initialize model once at the top\n",
        "model = PunctuationModel()\n",
        "\n",
        "def extract_video_id(video_url):\n",
        "    \"\"\"\n",
        "    Extracts the YouTube video ID from various URL formats.\n",
        "    \"\"\"\n",
        "    parsed_url = urllib.parse.urlparse(video_url)\n",
        "    query_params = urllib.parse.parse_qs(parsed_url.query)\n",
        "\n",
        "    if \"v\" in query_params:\n",
        "        return query_params[\"v\"][0]\n",
        "\n",
        "    match = re.search(r\"(youtu\\.be/|youtube\\.com/embed/|youtube\\.com/shorts/)([\\w-]+)\", video_url)\n",
        "    if match:\n",
        "        return match.group(2)\n",
        "\n",
        "    return None\n",
        "\n",
        "def download_audio(video_url):\n",
        "    \"\"\"\n",
        "    Downloads the audio using yt-dlp with cookies and returns the file path.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ydl_opts = {\n",
        "            'format': 'bestaudio/best',\n",
        "            'outtmpl': 'audio.%(ext)s',\n",
        "            'cookiefile': 'cookies (1).txt',  # Use the exported cookies\n",
        "            'postprocessors': [{\n",
        "                'key': 'FFmpegExtractAudio',\n",
        "                'preferredcodec': 'mp3',\n",
        "                'preferredquality': '192',\n",
        "            }],\n",
        "        }\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(video_url, download=True)\n",
        "            return \"audio.mp3\"\n",
        "    except Exception as e:\n",
        "        return f\"Error downloading audio: {str(e)}\"\n",
        "\n",
        "def convert_audio_to_wav(audio_file):\n",
        "    \"\"\"\n",
        "    Converts the downloaded MP3 audio to WAV format using pydub.\n",
        "    \"\"\"\n",
        "    wav_file = \"audio.wav\"\n",
        "    try:\n",
        "        AudioSegment.from_mp3(audio_file).export(wav_file, format=\"wav\")\n",
        "        return wav_file\n",
        "    except Exception as e:\n",
        "        return f\"Error converting to WAV: {str(e)}\"\n",
        "\n",
        "def transcribe_audio(audio_path, chunk_length=30):\n",
        "    \"\"\"\n",
        "    Splits audio into smaller chunks, transcribes each chunk separately,\n",
        "    and adds punctuation using deepmultilingualpunctuation library.\n",
        "    \"\"\"\n",
        "    recognizer = sr.Recognizer()\n",
        "    audio = AudioSegment.from_wav(audio_path)\n",
        "    total_duration = len(audio) / 1000  # Convert to seconds\n",
        "    transcribed_text = []\n",
        "\n",
        "    # Load punctuation model\n",
        "    model = PunctuationModel()\n",
        "\n",
        "    print(\"Transcribing audio in chunks...\")\n",
        "\n",
        "    # In transcribe_audio()\n",
        "    punctuated_chunks = []\n",
        "    for chunk_text in transcribed_text:\n",
        "        punctuated = model.restore_punctuation(chunk_text)\n",
        "        punctuated_chunks.append(punctuated)\n",
        "        return \" \".join(punctuated_chunks)\n",
        "\n",
        "    # Split and transcribe audio in chunks\n",
        "    for start in range(0, int(total_duration), chunk_length):\n",
        "        end = min(start + chunk_length, int(total_duration))\n",
        "        chunk = audio[start * 1000:end * 1000]  # Extract chunk in milliseconds\n",
        "        chunk.export(\"chunk.wav\", format=\"wav\")  # Save chunk temporarily\n",
        "\n",
        "        with sr.AudioFile(\"chunk.wav\") as source:\n",
        "            try:\n",
        "                audio_data = recognizer.record(source)\n",
        "                text = recognizer.recognize_google(audio_data)\n",
        "                transcribed_text.append(text)\n",
        "            except sr.UnknownValueError:\n",
        "                transcribed_text.append(\"[Unintelligible]\")\n",
        "            except sr.RequestError as e:\n",
        "                return f\"Error with the speech recognition service: {str(e)}\"\n",
        "\n",
        "    os.remove(\"chunk.wav\")  # Clean up temporary chunk file\n",
        "\n",
        "    # Combine chunks and add punctuation\n",
        "    combined_text = \" \".join(transcribed_text)\n",
        "    punctuated_text = model.restore_punctuation(combined_text)\n",
        "\n",
        "    return punctuated_text\n",
        "\n",
        "def get_transcript_unlisted(video_url):\n",
        "    \"\"\"\n",
        "    Tries to fetch the transcript using youtube_transcript_api first,\n",
        "    then falls back to downloading and transcribing audio if necessary.\n",
        "    \"\"\"\n",
        "    model = PunctuationModel()  # Initialize once\n",
        "    video_id = extract_video_id(video_url)\n",
        "\n",
        "    if not video_id:\n",
        "        return \"Invalid YouTube URL.\"\n",
        "\n",
        "    # Try API path with punctuation and timestamps\n",
        "    try:\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        formatted_transcript = []\n",
        "        for item in transcript:\n",
        "            start_time = convert_time(item['start'])\n",
        "            end_time = convert_time(item['start'] + item['duration'])\n",
        "            formatted_transcript.append(f\"[{start_time}-{end_time}] {item['text']}\")\n",
        "        return model.restore_punctuation(\" \".join(formatted_transcript))\n",
        "    except:\n",
        "        print(\"Transcript not available via API, attempting audio transcription...\")\n",
        "\n",
        "    # Audio fallback path (existing implementation)\n",
        "    # ... rest of audio processing code ...\n",
        "    # Download and transcribe audio if no transcript is available\n",
        "    audio_file = download_audio(video_url)\n",
        "    if \"Error\" in audio_file:\n",
        "        return audio_file\n",
        "\n",
        "    wav_file = convert_audio_to_wav(audio_file)\n",
        "    if \"Error\" in wav_file:\n",
        "        return wav_file\n",
        "\n",
        "    transcription = transcribe_audio(wav_file)\n",
        "\n",
        "    # Cleanup temporary files\n",
        "    os.remove(audio_file)\n",
        "    os.remove(wav_file)\n",
        "\n",
        "    # For audio fallback, timestamps are not directly available\n",
        "    # You might need to manually add timestamps or use a different approach\n",
        "    return transcription\n",
        "\n",
        "def convert_time(seconds):\n",
        "    \"\"\"Converts seconds to [hrs:mins:seconds] format.\"\"\"\n",
        "    hrs = int(seconds // 3600)\n",
        "    mins = int((seconds % 3600) // 60)\n",
        "    secs = round(seconds % 60, 2)\n",
        "    return f\"{hrs:02d}:{mins:02d}:{secs:05.2f}\"\n",
        "\n",
        "# Example usage\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    video_url = input(\"Enter the YouTube video URL: \")\n",
        "    transcript = get_transcript_unlisted(video_url)\n",
        "\n",
        "    # Save transcript to a text file\n",
        "    if \"Error\" not in transcript and \"Invalid YouTube URL.\" not in transcript:\n",
        "        output_file = \"transcript.txt\"\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(transcript)\n",
        "        print(f\"\\nTranscript saved successfully to {output_file}\")\n",
        "    else:\n",
        "        print(\"\\n\", transcript)\n",
        "\n",
        "# Example usage\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    video_url = input(\"Enter the YouTube video URL: \")\n",
        "    transcript = get_transcript_unlisted(video_url)\n",
        "\n",
        "    # Save transcript to a text file\n",
        "    if \"Error\" not in transcript and \"Invalid YouTube URL.\" not in transcript:\n",
        "        output_file = \"transcript.txt\"\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(transcript)\n",
        "        print(f\"\\nTranscript saved successfully to {output_file}\")\n",
        "    else:\n",
        "        print(\"\\n\", transcript)"
      ],
      "metadata": {
        "id": "xFSwjKsN54Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk scikit-learn"
      ],
      "metadata": {
        "id": "ke1kk7Z06zUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from itertools import combinations\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "# Ensure required resources are downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to read transcript from a .txt file\n",
        "def read_transcript(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        transcript = file.read()\n",
        "    return transcript\n",
        "\n",
        "# Function to split transcript into individual sentences\n",
        "def split_into_sentences(transcript):\n",
        "    sentences = nltk.sent_tokenize(transcript)\n",
        "    return sentences\n",
        "\n",
        "# Function to compute cosine similarity between sentence pairs\n",
        "def compute_cosine_similarity(sentences):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "    return similarity_matrix\n",
        "\n",
        "# Function to find similar sentence triplets based on cosine similarity\n",
        "def find_similar_triplets(sentences, similarity_matrix, threshold=0.5):\n",
        "    triplets = []\n",
        "    n = len(sentences)\n",
        "\n",
        "    # Generate all combinations of triplets\n",
        "    for comb in combinations(range(n), 3):\n",
        "        i, j, k = comb\n",
        "        # Check if all pairs within the triplet are similar\n",
        "        if (similarity_matrix[i][j] > threshold and\n",
        "            similarity_matrix[j][k] > threshold and\n",
        "            similarity_matrix[i][k] > threshold):\n",
        "            triplets.append([sentences[i], sentences[j], sentences[k]])\n",
        "\n",
        "    return triplets\n",
        "\n",
        "# Function to write the segmented sentences to a new .txt file\n",
        "def write_segments_to_file(triplets, output_file):\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        for idx, triplet in enumerate(triplets, 1):\n",
        "            file.write(f\"Segment {idx}:\\n\")\n",
        "            for sentence in triplet:\n",
        "                file.write(sentence + \"\\n\")\n",
        "            file.write(\"\\n\")\n",
        "\n",
        "def main():\n",
        "    input_file = '/content/transcript (5).txt'  # Input .txt file path\n",
        "    output_file = 'segmented_transcript.txt'  # Output .txt file path\n",
        "\n",
        "    # Reading and processing transcript\n",
        "    transcript = read_transcript(input_file)\n",
        "    sentences = split_into_sentences(transcript)\n",
        "    similarity_matrix = compute_cosine_similarity(sentences)\n",
        "\n",
        "    # Finding similar sentence triplets\n",
        "    triplets = find_similar_triplets(sentences, similarity_matrix, threshold=0.5)\n",
        "\n",
        "    # Writing segments to output file\n",
        "    write_segments_to_file(triplets, output_file)\n",
        "\n",
        "    print(f\"Segmented transcript saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "FR3H2UzALWHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk scikit-learn numpy"
      ],
      "metadata": {
        "id": "2Ar127ZrLdnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "yENfJk1lZ__U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "1terf6iyaKoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_transcript(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        transcript = file.read()\n",
        "    return transcript\n",
        "\n",
        "def split_into_sentences(transcript):\n",
        "    return sent_tokenize(transcript)\n",
        "\n",
        "def calculate_cosine_similarity(sentences):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "    return cosine_sim\n",
        "\n",
        "def group_sentences(sentences, cosine_sim, min_sentences=3, max_sentences=10):\n",
        "    grouped_sentences = []\n",
        "    used_indices = set()\n",
        "\n",
        "    for i in range(len(sentences)):\n",
        "        if i in used_indices:\n",
        "            continue\n",
        "        group = [sentences[i]]\n",
        "        used_indices.add(i)\n",
        "        for j in range(i + 1, len(sentences)):\n",
        "            if j in used_indices:\n",
        "                continue\n",
        "            if cosine_sim[i][j] > 0.5:  # Adjust the threshold as needed\n",
        "                group.append(sentences[j])\n",
        "                used_indices.add(j)\n",
        "                if len(group) >= max_sentences:\n",
        "                    break\n",
        "        if len(group) >= min_sentences:\n",
        "            grouped_sentences.append(group)\n",
        "    return grouped_sentences\n",
        "\n",
        "def process_transcript(file_path):\n",
        "    transcript = read_transcript(file_path)\n",
        "    sentences = split_into_sentences(transcript)\n",
        "    cosine_sim = calculate_cosine_similarity(sentences)\n",
        "    grouped_sentences = group_sentences(sentences, cosine_sim)\n",
        "    return grouped_sentences"
      ],
      "metadata": {
        "id": "C-VIH2XzadKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/transcript (5).txt'\n",
        "grouped_sentences = process_transcript(file_path)\n",
        "\n",
        "for i, group in enumerate(grouped_sentences):\n",
        "    print(f\"Segment {i + 1}:\")\n",
        "    for sentence in group:\n",
        "        print(sentence)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "z3vEr2ByagYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "def read_transcript(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            transcript = file.read()\n",
        "        return transcript\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return None\n",
        "\n",
        "def split_into_sentences(transcript):\n",
        "    return sent_tokenize(transcript)\n",
        "\n",
        "def calculate_cosine_similarity(sentences):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "    return cosine_sim\n",
        "\n",
        "def group_sentences(sentences, cosine_sim, min_sentences=3, max_sentences=10, similarity_threshold=0.5):\n",
        "    grouped_sentences = []\n",
        "    used_indices = set()\n",
        "\n",
        "    for i in range(len(sentences)):\n",
        "        if i in used_indices:\n",
        "            continue\n",
        "        group = [sentences[i]]\n",
        "        used_indices.add(i)\n",
        "        for j in range(i + 1, len(sentences)):\n",
        "            if j in used_indices:\n",
        "                continue\n",
        "            if cosine_sim[i][j] > similarity_threshold:  # Adjust the threshold as needed\n",
        "                group.append(sentences[j])\n",
        "                used_indices.add(j)\n",
        "                if len(group) >= max_sentences:\n",
        "                    break\n",
        "        if len(group) >= min_sentences:\n",
        "            grouped_sentences.append(group)\n",
        "    return grouped_sentences\n",
        "\n",
        "def process_transcript(file_path):\n",
        "    transcript = read_transcript(file_path)\n",
        "    if transcript is None:\n",
        "        return []  # Return an empty list if the file couldn't be read\n",
        "\n",
        "    sentences = split_into_sentences(transcript)\n",
        "    cosine_sim = calculate_cosine_similarity(sentences)\n",
        "    grouped_sentences = group_sentences(sentences, cosine_sim, similarity_threshold=0.6)  # Adjusted threshold\n",
        "    return grouped_sentences\n",
        "\n",
        "# Replace with your actual file path\n",
        "file_path = '/content/transcript (5).txt'\n",
        "grouped_sentences = process_transcript(file_path)\n",
        "\n",
        "if grouped_sentences:\n",
        "    for i, group in enumerate(grouped_sentences):\n",
        "        print(f\"Segment {i + 1}:\")\n",
        "        for sentence in group:\n",
        "            print(sentence)\n",
        "        print(\"\\n\")\n",
        "else:\n",
        "    print(\"No sentences were processed. Check the file path and file content.\")"
      ],
      "metadata": {
        "id": "gYbBl57Vam-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "def read_transcript(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            transcript = file.read()\n",
        "        return transcript\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return None\n",
        "\n",
        "def split_into_sentences(transcript):\n",
        "    return sent_tokenize(transcript)\n",
        "\n",
        "def calculate_cosine_similarity(sentences):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "    return cosine_sim\n",
        "\n",
        "def group_sentences(sentences, cosine_sim, min_sentences=3, max_sentences=10, similarity_threshold=0.5):\n",
        "    grouped_sentences = []\n",
        "    used_indices = set()\n",
        "\n",
        "    for i in range(len(sentences)):\n",
        "        if i in used_indices:\n",
        "            continue\n",
        "        group = [sentences[i]]\n",
        "        used_indices.add(i)\n",
        "        for j in range(i + 1, len(sentences)):\n",
        "            if j in used_indices:\n",
        "                continue\n",
        "            if cosine_sim[i][j] > similarity_threshold:  # Adjust the threshold as needed\n",
        "                group.append(sentences[j])\n",
        "                used_indices.add(j)\n",
        "                if len(group) >= max_sentences:\n",
        "                    break\n",
        "        if len(group) >= min_sentences:\n",
        "            grouped_sentences.append(group)\n",
        "    return grouped_sentences\n",
        "\n",
        "def process_transcript(file_path):\n",
        "    transcript = read_transcript(file_path)\n",
        "    if transcript is None:\n",
        "        return []  # Return an empty list if the file couldn't be read\n",
        "\n",
        "    sentences = split_into_sentences(transcript)\n",
        "    cosine_sim = calculate_cosine_similarity(sentences)\n",
        "    grouped_sentences = group_sentences(sentences, cosine_sim, similarity_threshold=0.6)  # Adjusted threshold\n",
        "    return grouped_sentences\n",
        "\n",
        "# Replace with your actual file path\n",
        "file_path = '/content/transcript (5).txt'\n",
        "grouped_sentences = process_transcript(file_path)\n",
        "\n",
        "if grouped_sentences:\n",
        "    for i, group in enumerate(grouped_sentences):\n",
        "        print(f\"Segment {i + 1}:\")\n",
        "        for sentence in group:\n",
        "            print(sentence)\n",
        "        print(\"\\n\")\n",
        "else:\n",
        "    print(\"No sentences were processed. Check the file path and file content.\")"
      ],
      "metadata": {
        "id": "i6MgIsa6cl6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cosine Similarity approach"
      ],
      "metadata": {
        "id": "mhut8gxV3ZIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "def read_transcript(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            transcript = file.read()\n",
        "        return transcript\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return None\n",
        "\n",
        "def split_into_sentences(transcript):\n",
        "    return sent_tokenize(transcript)\n",
        "\n",
        "def calculate_cosine_similarity(sentences):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "    return cosine_sim\n",
        "\n",
        "def save_cosine_similarity_to_csv(cosine_sim, sentences, output_file):\n",
        "    # Create a DataFrame for the cosine similarity matrix\n",
        "    df = pd.DataFrame(cosine_sim, index=sentences, columns=sentences)\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    df.to_csv(output_file)\n",
        "    print(f\"Cosine similarity matrix saved to {output_file}\")\n",
        "\n",
        "def process_transcript(file_path, output_file):\n",
        "    # Read the transcript\n",
        "    transcript = read_transcript(file_path)\n",
        "    if transcript is None:\n",
        "        return\n",
        "\n",
        "    # Split the transcript into sentences\n",
        "    sentences = split_into_sentences(transcript)\n",
        "    print(\"Sentences extracted:\")\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        print(f\"{i + 1}: {sentence}\")\n",
        "\n",
        "    # Calculate cosine similarity between sentences\n",
        "    cosine_sim = calculate_cosine_similarity(sentences)\n",
        "\n",
        "    # Save the cosine similarity matrix to a CSV file\n",
        "    save_cosine_similarity_to_csv(cosine_sim, sentences, output_file)\n",
        "\n",
        "# File paths (adjust as needed)\n",
        "file_path = '/content/transcript (5).txt'  # Path to your uploaded transcript file\n",
        "output_file = '/content/cosine_similarity_matrix.csv'  # Output CSV file name\n",
        "\n",
        "# Process the transcript and generate the CSV file\n",
        "process_transcript(file_path, output_file)"
      ],
      "metadata": {
        "id": "O4xS-QdOdMm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "def read_transcript(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            transcript = file.read()\n",
        "        return transcript\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return None\n",
        "\n",
        "def split_into_sentences(transcript):\n",
        "    return sent_tokenize(transcript)\n",
        "\n",
        "def calculate_cosine_similarity(sentences):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "    return cosine_sim\n",
        "\n",
        "def save_cosine_similarity_to_csv(cosine_sim, sentences, output_file):\n",
        "    # Create sentence names (S1, S2, S3, ...)\n",
        "    sentence_names = [f\"S{i+1}\" for i in range(len(sentences))]\n",
        "\n",
        "    # Create a DataFrame for the cosine similarity matrix\n",
        "    df = pd.DataFrame(cosine_sim, index=sentence_names, columns=sentence_names)\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    df.to_csv(output_file)\n",
        "    print(f\"Cosine similarity matrix saved to {output_file}\")\n",
        "\n",
        "def process_transcript(file_path, output_file):\n",
        "    # Read the transcript\n",
        "    transcript = read_transcript(file_path)\n",
        "    if transcript is None:\n",
        "        return\n",
        "\n",
        "    # Split the transcript into sentences\n",
        "    sentences = split_into_sentences(transcript)\n",
        "    print(\"Sentences extracted:\")\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        print(f\"S{i + 1}: {sentence}\")\n",
        "\n",
        "    # Calculate cosine similarity between sentences\n",
        "    cosine_sim = calculate_cosine_similarity(sentences)\n",
        "\n",
        "    # Save the cosine similarity matrix to a CSV file\n",
        "    save_cosine_similarity_to_csv(cosine_sim, sentences, output_file)\n",
        "\n",
        "# File paths (adjust as needed)\n",
        "file_path = '/content/transcript (5).txt'  # Path to your uploaded transcript file\n",
        "output_file = '/content/cosine_similarity_matrix1.csv'  # Output CSV file name\n",
        "\n",
        "# Process the transcript and generate the CSV file\n",
        "process_transcript(file_path, output_file)"
      ],
      "metadata": {
        "id": "GPo7HwsyndNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/cosine_similarity_matrix1.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "r4-Pniydomhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the uploaded CSV file\n",
        "file_path = '/content/cosine_similarity_matrix1.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Set the first column as the index and remove the 'Unnamed: 0' column\n",
        "data.set_index('Unnamed: 0', inplace=True)\n",
        "\n",
        "# Generate the correlation matrix\n",
        "correlation_matrix = data.corr()\n",
        "\n",
        "# Print the correlation matrix summary\n",
        "print(\"Correlation Matrix Summary:\")\n",
        "print(correlation_matrix.describe())\n",
        "\n",
        "# Visualize the correlation matrix using a heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xsJWY89dpLj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "def read_transcript(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            transcript = file.read()\n",
        "        return transcript\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return None\n",
        "\n",
        "def split_into_sentences(transcript):\n",
        "    return sent_tokenize(transcript)\n",
        "\n",
        "def calculate_cosine_similarity(sentences):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "    return cosine_sim\n",
        "\n",
        "def segment_sentences(sentences, cosine_sim, threshold=0.5):\n",
        "    visited = [False] * len(sentences)\n",
        "    segments = []\n",
        "\n",
        "    for i in range(len(sentences)):\n",
        "        if not visited[i]:\n",
        "            segment = [sentences[i]]\n",
        "            visited[i] = True\n",
        "\n",
        "            # Check for similar sentences\n",
        "            for j in range(i + 1, len(sentences)):\n",
        "                if not visited[j] and cosine_sim[i][j] >= threshold:\n",
        "                    segment.append(sentences[j])\n",
        "                    visited[j] = True\n",
        "\n",
        "            segments.append(segment)\n",
        "\n",
        "    return segments\n",
        "\n",
        "def print_segments(segments):\n",
        "    for idx, segment in enumerate(segments, start=1):\n",
        "        print(f\"\\nSegment {idx}:\")\n",
        "        for sentence in segment:\n",
        "            print(f\" - {sentence}\")\n",
        "\n",
        "def process_transcript(file_path, threshold=0.5):\n",
        "    # Read the transcript\n",
        "    transcript = read_transcript(file_path)\n",
        "    if transcript is None:\n",
        "        return\n",
        "\n",
        "    # Split the transcript into sentences\n",
        "    sentences = split_into_sentences(transcript)\n",
        "    print(\"Sentences extracted:\")\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        print(f\"S{i + 1}: {sentence}\")\n",
        "\n",
        "    # Calculate cosine similarity between sentences\n",
        "    cosine_sim = calculate_cosine_similarity(sentences)\n",
        "\n",
        "    # Segment sentences based on similarity\n",
        "    segments = segment_sentences(sentences, cosine_sim, threshold)\n",
        "\n",
        "    # Print segmented sentences\n",
        "    print(\"\\nSegmented Sentences:\")\n",
        "    print_segments(segments)\n",
        "\n",
        "# File path to the transcript\n",
        "file_path = '/content/transcript (8).txt'  # Replace with your file path\n",
        "\n",
        "# Process the transcript and print segments with a similarity threshold of 0.5\n",
        "process_transcript(file_path, threshold=0.1)"
      ],
      "metadata": {
        "id": "MeW6kCiip6tV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "def read_transcript(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            transcript = file.read()\n",
        "        return transcript\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return None\n",
        "\n",
        "def split_into_sentences(transcript):\n",
        "    return sent_tokenize(transcript)\n",
        "\n",
        "def calculate_cosine_similarity(sentences):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "    return cosine_sim\n",
        "\n",
        "def segment_sentences(sentences, cosine_sim, threshold=0.5):\n",
        "    visited = [False] * len(sentences)\n",
        "    segments = []\n",
        "\n",
        "    for i in range(len(sentences)):\n",
        "        if not visited[i]:\n",
        "            segment = [sentences[i]]\n",
        "            visited[i] = True\n",
        "\n",
        "            # Check for similar sentences\n",
        "            for j in range(i + 1, len(sentences)):\n",
        "                if not visited[j] and cosine_sim[i][j] >= threshold:\n",
        "                    segment.append(sentences[j])\n",
        "                    visited[j] = True\n",
        "\n",
        "            segments.append(segment)\n",
        "\n",
        "    return segments\n",
        "\n",
        "def print_segments(segments):\n",
        "    for idx, segment in enumerate(segments, start=1):\n",
        "        print(f\"\\nSegment {idx}:\")\n",
        "        for sentence in segment:\n",
        "            print(f\" - {sentence}\")\n",
        "\n",
        "#  New Function to Save Segments to a Text File\n",
        "def save_segments_to_file(segments, output_file):\n",
        "    try:\n",
        "        with open(output_file, 'w', encoding='utf-8') as file:\n",
        "            for idx, segment in enumerate(segments, start=1):\n",
        "                file.write(f\"Segment {idx}:\\n\")\n",
        "                for sentence in segment:\n",
        "                    file.write(f\" - {sentence}\\n\")\n",
        "                file.write(\"\\n\")\n",
        "        print(f\"Segmented output saved to {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving file: {e}\")\n",
        "\n",
        "def process_transcript(file_path, threshold=0.5, output_file='segmented_output.txt'):\n",
        "    # Read the transcript\n",
        "    transcript = read_transcript(file_path)\n",
        "    if transcript is None:\n",
        "        return\n",
        "\n",
        "    # Split the transcript into sentences\n",
        "    sentences = split_into_sentences(transcript)\n",
        "    print(\"Sentences extracted:\")\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        print(f\"S{i + 1}: {sentence}\")\n",
        "\n",
        "    # Calculate cosine similarity between sentences\n",
        "    cosine_sim = calculate_cosine_similarity(sentences)\n",
        "\n",
        "    # Segment sentences based on similarity\n",
        "    segments = segment_sentences(sentences, cosine_sim, threshold)\n",
        "\n",
        "    # Print segmented sentences\n",
        "    print(\"\\nSegmented Sentences:\")\n",
        "    print_segments(segments)\n",
        "\n",
        "    # Save segments to a text file\n",
        "    save_segments_to_file(segments, output_file)\n",
        "\n",
        "# File paths\n",
        "file_path = '/content/transcript (5).txt'  # Replace with your file path\n",
        "output_file = '/content/segmented_output.txt'  # Output text file path\n",
        "\n",
        "# Process the transcript, segment it, and save to a text file\n",
        "process_transcript(file_path, threshold=0.15, output_file=output_file)"
      ],
      "metadata": {
        "id": "O7Dlg0Hhxpzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def read_transcript(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            transcript = file.read()\n",
        "        return transcript\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return None\n",
        "\n",
        "def split_into_sentences(transcript):\n",
        "    return sent_tokenize(transcript)\n",
        "\n",
        "def calculate_cosine_similarity(sentences):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "    return cosine_sim\n",
        "\n",
        "def segment_sentences(sentences, cosine_sim, threshold=0.5):\n",
        "    visited = [False] * len(sentences)\n",
        "    segments = []\n",
        "    for i in range(len(sentences)):\n",
        "        if not visited[i]:\n",
        "            segment = [sentences[i]]\n",
        "            visited[i] = True\n",
        "            for j in range(i + 1, len(sentences)):\n",
        "                if not visited[j] and cosine_sim[i][j] >= threshold:\n",
        "                    segment.append(sentences[j])\n",
        "                    visited[j] = True\n",
        "            segments.append(segment)\n",
        "    return segments\n",
        "\n",
        "def remove_stopwords(segment):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = []\n",
        "    for sentence in segment:\n",
        "        words = word_tokenize(sentence)\n",
        "        filtered_words.extend([word.lower() for word in words if word.isalnum() and word.lower() not in stop_words])\n",
        "    return filtered_words\n",
        "\n",
        "def find_keywords(filtered_words):\n",
        "    if len(filtered_words) < 2:\n",
        "        return None, None, 0  # Not enough words for comparison\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(filtered_words)\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "    max_sim = 0\n",
        "    keyword_pair = (None, None)\n",
        "    for i in range(len(filtered_words)):\n",
        "        for j in range(i + 1, len(filtered_words)):\n",
        "            if cosine_sim[i][j] > max_sim:\n",
        "                max_sim = cosine_sim[i][j]\n",
        "                keyword_pair = (filtered_words[i], filtered_words[j])\n",
        "    return keyword_pair[0], keyword_pair[1], max_sim\n",
        "\n",
        "def save_segments_to_csv(segments, output_file):\n",
        "    with pd.ExcelWriter(output_file) as writer:\n",
        "        for idx, segment in enumerate(segments, start=1):\n",
        "            filtered_words = remove_stopwords(segment)\n",
        "            word1, word2, max_sim = find_keywords(filtered_words)\n",
        "\n",
        "            # Create a DataFrame for the segment\n",
        "            df = pd.DataFrame({\n",
        "                'Word 1': [word1] if word1 else [],\n",
        "                'Word 2': [word2] if word2 else [],\n",
        "                'Cosine Similarity': [max_sim] if word1 and word2 else [],\n",
        "                'Keyword': [f\"{word1}, {word2}\"] if word1 and word2 else []\n",
        "            })\n",
        "\n",
        "            # Save each segment as a separate sheet\n",
        "            df.to_excel(writer, sheet_name=f'Segment {idx}', index=False)\n",
        "\n",
        "    print(f\"Segmented keywords saved to {output_file}\")\n",
        "\n",
        "def process_transcript(file_path, threshold=0.5, output_file='segmented_keywords.xlsx'):\n",
        "    transcript = read_transcript(file_path)\n",
        "    if transcript is None:\n",
        "        return\n",
        "\n",
        "    sentences = split_into_sentences(transcript)\n",
        "    cosine_sim = calculate_cosine_similarity(sentences)\n",
        "    segments = segment_sentences(sentences, cosine_sim, threshold)\n",
        "\n",
        "    save_segments_to_csv(segments, output_file)\n",
        "\n",
        "    print(\"\\nChosen Keywords for Each Segment:\")\n",
        "    for idx, segment in enumerate(segments, start=1):\n",
        "        filtered_words = remove_stopwords(segment)\n",
        "        word1, word2, max_sim = find_keywords(filtered_words)\n",
        "        if word1 and word2:\n",
        "            print(f\"Segment {idx}: Keywords = {word1}, {word2} (Similarity: {max_sim:.2f})\")\n",
        "        else:\n",
        "            print(f\"Segment {idx}: Not enough data for keywords\")\n",
        "\n",
        "# File paths\n",
        "file_path = '/content/transcript (5).txt'  # Replace with your file path\n",
        "output_file = '/content/segmented_keywords.xlsx'  # Output Excel file path\n",
        "\n",
        "# Run the process\n",
        "process_transcript(file_path, threshold=0.1, output_file=output_file)\n"
      ],
      "metadata": {
        "id": "I6aknEXc1CdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def read_segmented_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "            # Split the file content into segments based on a delimiter (e.g., \"\\n\\n\")\n",
        "            segments = [segment.strip() for segment in content.split(\"\\n\\n\") if segment.strip()]\n",
        "        return segments\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return None\n",
        "\n",
        "def remove_stopwords(segment):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(segment)\n",
        "    return [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
        "\n",
        "def calculate_word_cosine_similarity(filtered_words):\n",
        "    if len(filtered_words) < 2:\n",
        "        return None, None, 0, None  # Not enough words for comparison\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(filtered_words)\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "    max_sim = 0\n",
        "    keyword_pair = (None, None)\n",
        "    for i in range(len(filtered_words)):\n",
        "        for j in range(i + 1, len(filtered_words)):\n",
        "            if cosine_sim[i][j] > max_sim:\n",
        "                max_sim = cosine_sim[i][j]\n",
        "                keyword_pair = (filtered_words[i], filtered_words[j])\n",
        "\n",
        "    return keyword_pair[0], keyword_pair[1], max_sim, cosine_sim\n",
        "\n",
        "def save_word_similarity_to_excel(cosine_sim, filtered_words, sheet_name, writer):\n",
        "    if cosine_sim is not None:\n",
        "        df = pd.DataFrame(cosine_sim, index=filtered_words, columns=filtered_words)\n",
        "        df.to_excel(writer, sheet_name=sheet_name)\n",
        "    else:\n",
        "        df = pd.DataFrame(columns=['Info'])\n",
        "        df.loc[0] = [\"Not enough words for comparison\"]\n",
        "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "def save_keywords_summary_to_csv(keywords, output_file):\n",
        "    summary_df = pd.DataFrame(keywords, columns=['Segment', 'Word 1', 'Word 2', 'Cosine Similarity', 'Keyword'])\n",
        "    summary_df.to_csv(output_file, index=False)\n",
        "    print(f\"Keywords summary saved to {output_file}\")\n",
        "\n",
        "def process_segments(file_path, output_excel='segment_word_similarity.xlsx', summary_csv='segment_keywords_summary.csv'):\n",
        "    segments = read_segmented_file(file_path)\n",
        "    if segments is None:\n",
        "        return\n",
        "\n",
        "    keywords_summary = []\n",
        "\n",
        "    with pd.ExcelWriter(output_excel) as writer:\n",
        "        for idx, segment in enumerate(segments, start=1):\n",
        "            # Remove stopwords\n",
        "            filtered_words = remove_stopwords(segment)\n",
        "\n",
        "            # Calculate cosine similarity between words\n",
        "            word1, word2, max_sim, cosine_sim = calculate_word_cosine_similarity(filtered_words)\n",
        "\n",
        "            # Save word similarity matrix for each segment to Excel\n",
        "            save_word_similarity_to_excel(cosine_sim, filtered_words, f'Segment {idx}', writer)\n",
        "\n",
        "            # Prepare summary data\n",
        "            if word1 and word2:\n",
        "                keyword = f\"{word1}, {word2}\"\n",
        "                keywords_summary.append([f\"Segment {idx}\", word1, word2, max_sim, keyword])\n",
        "            else:\n",
        "                keywords_summary.append([f\"Segment {idx}\", \"N/A\", \"N/A\", 0, \"N/A\"])\n",
        "\n",
        "    # Save summary of keywords to CSV\n",
        "    save_keywords_summary_to_csv(keywords_summary, summary_csv)\n",
        "\n",
        "    # Print chosen keywords for each segment\n",
        "    print(\"\\nChosen Keywords for Each Segment:\")\n",
        "    for row in keywords_summary:\n",
        "        print(f\"{row[0]}: Keywords = {row[4]} (Similarity: {row[3]:.2f})\")\n",
        "\n",
        "# File paths\n",
        "segmented_file_path = '/content/segmented_output.txt'  # Replace with your segmented text file path\n",
        "output_excel = '/content/segment_word_similarity.xlsx'  # Excel file for word similarity matrices\n",
        "summary_csv = '/content/segment_keywords_summary.csv'  # CSV file for keywords summary\n",
        "\n",
        "# Run the process\n",
        "process_segments(segmented_file_path, output_excel=output_excel, summary_csv=summary_csv)\n"
      ],
      "metadata": {
        "id": "crUfoohOHXJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Download NLTK data if not already available\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Define filler words to remove\n",
        "FILLER_WORDS = {'umm', 'uh', 'oh', 'okay', 'like', 'you know', 'actually', 'basically', 'literally', 'well', 'so', 'just', 'i mean', 'sort of', 'kind of'}\n",
        "\n",
        "def read_segmented_file(file_path):\n",
        "    \"\"\"Read segmented text file and split into segments.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "            # Split the file content into segments based on double newlines\n",
        "            segments = [segment.strip() for segment in content.split(\"\\n\\n\") if segment.strip()]\n",
        "        return segments\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return None\n",
        "\n",
        "def clean_text(segment):\n",
        "    \"\"\"Remove stopwords, fillers, and dates from a segment.\"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Remove dates (e.g., 12/03/2023, March 12, 2023)\n",
        "    segment = re.sub(r'\\b(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\b\\s\\d{1,2},?\\s?\\d{2,4}?)\\b', '', segment)\n",
        "\n",
        "    # Tokenize and clean words\n",
        "    words = word_tokenize(segment)\n",
        "    filtered_words = [\n",
        "        word.lower() for word in words\n",
        "        if word.isalnum() and word.lower() not in stop_words and word.lower() not in FILLER_WORDS\n",
        "    ]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "def save_to_csv(segments, cleaned_segments, output_file):\n",
        "    \"\"\"Save original and cleaned segments to a CSV file.\"\"\"\n",
        "    df = pd.DataFrame({\n",
        "        'Original Segment': segments,\n",
        "        'Cleaned Segment (No Stopwords or Fillers)': cleaned_segments\n",
        "    })\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Cleaned segments saved to {output_file}\")\n",
        "\n",
        "def process_segments(file_path, output_csv='cleaned_segments.csv'):\n",
        "    \"\"\"Process the segmented text file.\"\"\"\n",
        "    segments = read_segmented_file(file_path)\n",
        "    if segments is None:\n",
        "        return\n",
        "\n",
        "    cleaned_segments = [clean_text(segment) for segment in segments]\n",
        "    save_to_csv(segments, cleaned_segments, output_csv)\n",
        "\n",
        "    # Print a preview of cleaned segments\n",
        "    print(\"\\nPreview of Cleaned Segments:\")\n",
        "    for idx, (orig, clean) in enumerate(zip(segments, cleaned_segments), start=1):\n",
        "        print(f\"\\nSegment {idx} (Original): {orig}\")\n",
        "        print(f\"Segment {idx} (Cleaned): {clean}\")\n",
        "\n",
        "# File paths\n",
        "segmented_file_path = '/content/segmented_output.txt'  # Replace with your segmented text file path\n",
        "output_csv = '/content/cleaned_segments.csv'  # CSV file for cleaned segments\n",
        "\n",
        "# Run the process\n",
        "process_segments(segmented_file_path, output_csv=output_csv)\n"
      ],
      "metadata": {
        "id": "-fqm3n0FQHcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def load_transcript_from_file(file_path):\n",
        "    \"\"\"\n",
        "    Loads the transcript from a file.\n",
        "    Args:\n",
        "        file_path (str): Path to the transcript file.\n",
        "    Returns:\n",
        "        list: List of dictionaries with 'start', 'end', and 'text' keys.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            if file_path.endswith(\".json\"):\n",
        "                transcript = json.load(file)\n",
        "            else:\n",
        "                # For plain text files, assume each line is in the format: [start] - [end]: [text]\n",
        "                transcript = []\n",
        "                for line in file:\n",
        "                    match = re.match(r\"(\\d+\\.\\d+) - (\\d+\\.\\d+): (.+)\", line.strip())\n",
        "                    if match:\n",
        "                        start, end, text = match.groups()\n",
        "                        transcript.append({\n",
        "                            \"start\": float(start),\n",
        "                            \"end\": float(end),\n",
        "                            \"text\": text\n",
        "                        })\n",
        "            return transcript\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading transcript: {e}\")\n",
        "        return None\n",
        "\n",
        "def save_transcript_with_timestamps(transcript, output_path=\"transcript_with_timestamps.txt\"):\n",
        "    \"\"\"\n",
        "    Saves the transcript with timestamps to a text file.\n",
        "    Args:\n",
        "        transcript (list): List of dictionaries with 'start', 'end', and 'text'.\n",
        "        output_path (str): The path to save the output file.\n",
        "    \"\"\"\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        for segment in transcript:\n",
        "            file.write(f\"{segment['start']} - {segment['end']}: {segment['text']}\\n\")\n",
        "    print(f\"Transcript with timestamps saved to {output_path}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Ask the user for the transcript file path\n",
        "    transcript_path = input(\"Enter the path to the transcript file: \")\n",
        "    output_path = input(\"Enter the path to save the transcript with timestamps (default: transcript_with_timestamps.txt): \") or \"transcript_with_timestamps.txt\"\n",
        "\n",
        "    # Load the transcript from the file\n",
        "    transcript = load_transcript_from_file(transcript_path)\n",
        "    if not transcript:\n",
        "        print(\"Failed to load transcript. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # Print the transcript with timestamps\n",
        "    print(\"\\nTranscript with Timestamps:\")\n",
        "    for segment in transcript:\n",
        "        print(f\"{segment['start']} - {segment['end']}: {segment['text']}\")\n",
        "\n",
        "    # Save the transcript with timestamps to a file\n",
        "    save_transcript_with_timestamps(transcript, output_path)"
      ],
      "metadata": {
        "id": "utL982IiQ9pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import urllib.parse\n",
        "import requests\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from pytube import YouTube\n",
        "import speech_recognition as sr\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "import yt_dlp\n",
        "def extract_video_id(video_url):\n",
        "    \"\"\"\n",
        "    Extracts the YouTube video ID from various URL formats.\n",
        "    \"\"\"\n",
        "    parsed_url = urllib.parse.urlparse(video_url)\n",
        "    query_params = urllib.parse.parse_qs(parsed_url.query)\n",
        "\n",
        "    if \"v\" in query_params:\n",
        "        return query_params[\"v\"][0]\n",
        "\n",
        "    match = re.search(r\"(youtu\\.be/|youtube\\.com/embed/|youtube\\.com/shorts/)([\\w-]+)\", video_url)\n",
        "    if match:\n",
        "        return match.group(2)\n",
        "\n",
        "    return None\n",
        "\n",
        "def download_audio(video_url):\n",
        "    \"\"\"\n",
        "    Downloads the audio using yt-dlp with cookies and returns the file path.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ydl_opts = {\n",
        "            'format': 'bestaudio/best',\n",
        "            'outtmpl': 'audio.%(ext)s',\n",
        "            'cookiefile': '/content/cookies (2).txt',  # Use the exported cookies\n",
        "            'postprocessors': [{\n",
        "                'key': 'FFmpegExtractAudio',\n",
        "                'preferredcodec': 'mp3',\n",
        "                'preferredquality': '192',\n",
        "            }],\n",
        "        }\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(video_url, download=True)\n",
        "            return \"audio.mp3\"\n",
        "    except Exception as e:\n",
        "        return f\"Error downloading audio: {str(e)}\"\n",
        "\n",
        "def convert_audio_to_wav(audio_file):\n",
        "    \"\"\"\n",
        "    Converts the downloaded MP3 audio to WAV format using pydub.\n",
        "    \"\"\"\n",
        "    wav_file = \"audio.wav\"\n",
        "    try:\n",
        "        AudioSegment.from_mp3(audio_file).export(wav_file, format=\"wav\")\n",
        "        return wav_file\n",
        "    except Exception as e:\n",
        "        return f\"Error converting to WAV: {str(e)}\"\n",
        "\n",
        "def transcribe_audio(audio_path, chunk_length=30):\n",
        "    \"\"\"\n",
        "    Splits audio into smaller chunks and transcribes each chunk separately.\n",
        "    Args:\n",
        "        audio_path (str): Path to the audio file.\n",
        "        chunk_length (int): Length of each chunk in seconds (default: 30).\n",
        "    Returns:\n",
        "        list: List of dictionaries containing transcribed text and timestamps.\n",
        "    \"\"\"\n",
        "    recognizer = sr.Recognizer()\n",
        "    audio = AudioSegment.from_wav(audio_path)\n",
        "    total_duration = len(audio) / 1000  # Convert to seconds\n",
        "    transcribed_segments = []\n",
        "\n",
        "    print(\"Transcribing audio in chunks...\")\n",
        "\n",
        "    # Split and transcribe audio in chunks\n",
        "    for start in range(0, int(total_duration), chunk_length):\n",
        "        end = min(start + chunk_length, int(total_duration))\n",
        "        chunk = audio[start * 1000:end * 1000]  # Extract chunk in milliseconds\n",
        "        chunk.export(\"chunk.wav\", format=\"wav\")  # Save chunk temporarily\n",
        "\n",
        "        with sr.AudioFile(\"chunk.wav\") as source:\n",
        "            try:\n",
        "                audio_data = recognizer.record(source)\n",
        "                text = recognizer.recognize_google(audio_data)\n",
        "                transcribed_segments.append({\n",
        "                    \"start\": start,\n",
        "                    \"end\": end,\n",
        "                    \"text\": text\n",
        "                })\n",
        "            except sr.UnknownValueError:\n",
        "                transcribed_segments.append({\n",
        "                    \"start\": start,\n",
        "                    \"end\": end,\n",
        "                    \"text\": \"[Unintelligible]\"\n",
        "                })\n",
        "            except sr.RequestError as e:\n",
        "                return f\"Error with the speech recognition service: {str(e)}\"\n",
        "\n",
        "    os.remove(\"chunk.wav\")  # Clean up temporary chunk file\n",
        "    return transcribed_segments\n",
        "\n",
        "def get_transcript_unlisted(video_url):\n",
        "    \"\"\"\n",
        "    Tries to fetch the transcript using youtube_transcript_api first,\n",
        "    then falls back to downloading and transcribing audio if necessary.\n",
        "    \"\"\"\n",
        "    video_id = extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "        return \"Invalid YouTube URL.\"\n",
        "\n",
        "    # Try to fetch transcript using youtube_transcript_api\n",
        "    try:\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        # Add 'end' time to each segment\n",
        "        for segment in transcript:\n",
        "            segment[\"end\"] = segment[\"start\"] + segment[\"duration\"]\n",
        "        return transcript  # Return transcript with timestamps\n",
        "    except:\n",
        "        print(\"Transcript not available via API, attempting audio transcription...\")\n",
        "\n",
        "    # Download and transcribe audio if no transcript is available\n",
        "    audio_file = download_audio(video_url)\n",
        "    if \"Error\" in audio_file:\n",
        "        return audio_file\n",
        "\n",
        "    wav_file = convert_audio_to_wav(audio_file)\n",
        "    if \"Error\" in wav_file:\n",
        "        return wav_file\n",
        "\n",
        "    transcription = transcribe_audio(wav_file)\n",
        "\n",
        "    # Cleanup temporary files\n",
        "    os.remove(audio_file)\n",
        "    os.remove(wav_file)\n",
        "\n",
        "    return transcription\n",
        "\n",
        "def save_transcript_to_file(transcript, filename=\"transcript.txt\"):\n",
        "    \"\"\"\n",
        "    Saves the transcript to a text file.\n",
        "    Args:\n",
        "        transcript (list or str): The transcript to save.\n",
        "        filename (str): The name of the output file.\n",
        "    \"\"\"\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
        "        if isinstance(transcript, list):\n",
        "            for segment in transcript:\n",
        "                file.write(f\"{segment['start']} - {segment['end']}: {segment['text']}\\n\")\n",
        "        else:\n",
        "            file.write(transcript)\n",
        "    print(f\"Transcript saved to {filename}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    video_url = input(\"Enter the YouTube video URL: \")\n",
        "    transcript = get_transcript_unlisted(video_url)\n",
        "\n",
        "    if isinstance(transcript, list):\n",
        "        print(\"\\nTranscript with Timestamps:\")\n",
        "        for segment in transcript:\n",
        "            print(f\"{segment['start']} - {segment['end']}: {segment['text']}\")\n",
        "    else:\n",
        "        print(\"\\nTranscript:\\n\", transcript)\n",
        "\n",
        "    # Save transcript to a text file\n",
        "    save_transcript_to_file(transcript, \"transcript.txt\")"
      ],
      "metadata": {
        "id": "pMd40cEwfTbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install yt-dlp"
      ],
      "metadata": {
        "id": "47V3nJPtiJrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install youtube-transcript-api"
      ],
      "metadata": {
        "id": "00t1FZ2jiQfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install SpeechRecognition"
      ],
      "metadata": {
        "id": "YLu_03xjiR1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pydub"
      ],
      "metadata": {
        "id": "f-1iwEiDiZ7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytube"
      ],
      "metadata": {
        "id": "7T8TESC7ieBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "def read_transcript(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            transcript = file.read()\n",
        "        return transcript\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return None\n",
        "\n",
        "def split_into_sentences(transcript):\n",
        "    return sent_tokenize(transcript)\n",
        "\n",
        "def calculate_cosine_similarity(sentences):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "    return cosine_sim\n",
        "\n",
        "def segment_sentences(sentences, cosine_sim, threshold=0.5, min_sentences=5):\n",
        "    visited = [False] * len(sentences)\n",
        "    segments = []\n",
        "\n",
        "    for i in range(len(sentences)):\n",
        "        if not visited[i]:\n",
        "            segment = [sentences[i]]\n",
        "            visited[i] = True\n",
        "\n",
        "            for j in range(i + 1, len(sentences)):\n",
        "                if not visited[j] and cosine_sim[i][j] >= threshold:\n",
        "                    segment.append(sentences[j])\n",
        "                    visited[j] = True\n",
        "\n",
        "            segments.append(segment)\n",
        "\n",
        "    # Merge smaller segments\n",
        "    merged_segments = []\n",
        "    temp_segment = []\n",
        "\n",
        "    for segment in segments:\n",
        "        temp_segment.extend(segment)\n",
        "        if len(temp_segment) >= min_sentences:\n",
        "            merged_segments.append(temp_segment)\n",
        "            temp_segment = []\n",
        "\n",
        "    if temp_segment:\n",
        "        if merged_segments:\n",
        "            merged_segments[-1].extend(temp_segment)\n",
        "        else:\n",
        "            merged_segments.append(temp_segment)\n",
        "\n",
        "    return merged_segments\n",
        "\n",
        "def print_segments(segments):\n",
        "    for idx, segment in enumerate(segments, start=1):\n",
        "        print(f\"\\nSegment {idx}:\")\n",
        "        for sentence in segment:\n",
        "            print(f\" - {sentence}\")\n",
        "\n",
        "def save_segments_to_file(segments, output_file):\n",
        "    try:\n",
        "        with open(output_file, 'w', encoding='utf-8') as file:\n",
        "            for idx, segment in enumerate(segments, start=1):\n",
        "                file.write(f\"Segment {idx}:\\n\")\n",
        "                for sentence in segment:\n",
        "                    file.write(f\" - {sentence}\\n\")\n",
        "                file.write(\"\\n\")\n",
        "        print(f\"Segmented output saved to {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving file: {e}\")\n",
        "\n",
        "def process_transcript(file_path, threshold=0.5, min_sentences=5, output_file='segmented_output.txt'):\n",
        "    transcript = read_transcript(file_path)\n",
        "    if transcript is None:\n",
        "        return\n",
        "\n",
        "    sentences = split_into_sentences(transcript)\n",
        "    print(\"Sentences extracted:\")\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        print(f\"S{i + 1}: {sentence}\")\n",
        "\n",
        "    cosine_sim = calculate_cosine_similarity(sentences)\n",
        "\n",
        "    segments = segment_sentences(sentences, cosine_sim, threshold, min_sentences)\n",
        "\n",
        "    print(\"\\nSegmented Sentences:\")\n",
        "    print_segments(segments)\n",
        "\n",
        "    save_segments_to_file(segments, output_file)\n",
        "\n",
        "file_path = '/content/transcript.txt'  # Replace with your file path\n",
        "output_file = '/content/segmented_output.txt'\n",
        "process_transcript(file_path, threshold=0.15, min_sentences=5, output_file=output_file)\n"
      ],
      "metadata": {
        "id": "Do9GGqvYin0Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}