{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQmEpxbY8Q4D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import spacy\n",
        "import numpy as np\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "# Load spaCy tokenizer\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def load_text_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    sentences = [sent.text for sent in nlp(text).sents]  # Split text into sentences\n",
        "    tokens = [token.text for token in nlp(text)]\n",
        "    return sentences, tokens, text\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bigru = nn.GRU(input_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h, _ = self.bigru(x)\n",
        "        return h  # h ∈ R^(N × 2H)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.gru = nn.GRU(hidden_dim * 2, hidden_dim, batch_first=True)  # Match encoder output dim\n",
        "\n",
        "    def forward(self, x, hidden_state):\n",
        "        d, hidden_state = self.gru(x, hidden_state)\n",
        "        return d, hidden_state  # d ∈ R^(M × H)\n",
        "\n",
        "class Pointer(nn.Module):\n",
        "    def __init__(self, encoder_hidden_dim, decoder_hidden_dim):\n",
        "        super(Pointer, self).__init__()\n",
        "        self.W1 = nn.Linear(encoder_hidden_dim, decoder_hidden_dim)  # 2H → H\n",
        "        self.W2 = nn.Linear(decoder_hidden_dim, decoder_hidden_dim)  # H → H\n",
        "        self.v = nn.Linear(decoder_hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, encoder_outputs, decoder_state):\n",
        "        scores = self.v(torch.tanh(self.W1(encoder_outputs) + self.W2(decoder_state)))\n",
        "        attention_weights = F.softmax(scores, dim=1)  # softmax over input sequence positions\n",
        "        return attention_weights\n",
        "\n",
        "class SEGBOT(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(SEGBOT, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, hidden_dim)\n",
        "        self.decoder = Decoder(hidden_dim)\n",
        "        self.pointer = Pointer(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "    def forward(self, x, start_units):\n",
        "        encoder_outputs = self.encoder(x)  # Shape: (batch, seq_len, 2H)\n",
        "        decoder_hidden = torch.zeros(1, x.size(0), self.decoder.hidden_dim).to(x.device)\n",
        "        decoder_inputs = encoder_outputs[:, start_units, :].unsqueeze(1)  # Shape: (batch, 1, 2H)\n",
        "        decoder_outputs, _ = self.decoder(decoder_inputs, decoder_hidden)  # Shape: (batch, 1, H)\n",
        "        attention_weights = self.pointer(encoder_outputs, decoder_outputs.squeeze(1))  # Shape: (batch, seq_len, 1)\n",
        "        return attention_weights\n",
        "\n",
        "    def segment_text(self, sentences, tokens, attention_weights):\n",
        "        attention_weights = attention_weights.squeeze().detach().cpu().numpy()\n",
        "\n",
        "        # Normalize attention weights\n",
        "        attention_weights = (attention_weights - np.min(attention_weights)) / (np.max(attention_weights) - np.min(attention_weights))\n",
        "\n",
        "        # Find peaks in attention scores\n",
        "        peak_indices, _ = find_peaks(attention_weights, height=0.5, distance=5)  # Adjust height and distance for better segmentation\n",
        "\n",
        "        if len(peak_indices) == 0:\n",
        "            return [\" \".join(sentences)]  # Return full text if no peaks found\n",
        "\n",
        "        segments = []\n",
        "        start = 0\n",
        "        for i in peak_indices:\n",
        "            if i - start >= 5:  # Ensure at least 5 sentences per segment\n",
        "                segment = \" \".join(sentences[start:i]).strip()\n",
        "                if segment:\n",
        "                    segments.append(segment)\n",
        "                start = i\n",
        "\n",
        "        last_segment = \" \".join(sentences[start:]).strip()\n",
        "        if last_segment:\n",
        "            segments.append(last_segment)  # Add last segment\n",
        "\n",
        "        return segments if segments else None  # Return None if all segments are empty\n",
        "\n",
        "# Model Hyperparameters\n",
        "input_dim = 128  # Example input size\n",
        "hidden_dim = 256  # Hidden layer size\n",
        "model = SEGBOT(input_dim, hidden_dim)\n",
        "\n",
        "# Load text file and process\n",
        "file_path = \"/content/transcript (15).txt\"\n",
        "sentences, tokens, full_text = load_text_file(file_path)\n",
        "\n",
        "# Example Input (Dummy Tensor)\n",
        "x = torch.randn(1, len(tokens), input_dim)  # Batch size of 1, sequence length based on text\n",
        "start_units = 0  # Corrected variable type (integer index)\n",
        "output = model(x, start_units)\n",
        "\n",
        "# Segment the text\n",
        "segments = model.segment_text(sentences, tokens, output)\n",
        "if segments:\n",
        "    with open(\"segmented_transcript_new.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, segment in enumerate(segments):\n",
        "            f.write(f\"Segment {i+1}:\\n{segment}\\n\\n\")\n",
        "    print(\"Segmented transcript saved successfully.\")\n",
        "else:\n",
        "    print(\"No valid segments found. Terminating execution.\")"
      ]
    }
  ]
}