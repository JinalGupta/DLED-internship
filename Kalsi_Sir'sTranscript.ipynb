{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# The exclamation mark (!) is used to run shell commands directly from Jupyter Notebook or Google Colab.\n",
        "# This command installs the SpeechRecognition library, which is used for converting speech to text in Python.\n",
        "#!pip install speechrecognition"
      ],
      "metadata": {
        "id": "K3fGhYMYFMEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The exclamation mark (!) is used to run shell commands directly from Jupyter Notebook or Google Colab.\n",
        "# This command installs the pydub library, which is used for audio processing tasks like converting, slicing, and merging audio files.\n",
        "#!pip install pydub"
      ],
      "metadata": {
        "id": "8T19YzuSFP5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This command installs the openai-whisper library, which is a speech recognition model by OpenAI.\n",
        "# Whisper is capable of transcribing audio into text and supports multiple languages.\n",
        "# The exclamation mark (!) is used to run shell commands directly from Jupyter Notebook or Google Colab.\n",
        "#!pip install openai-whisper"
      ],
      "metadata": {
        "id": "NCiEsuW9E9aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gAvLbMHCwAJ"
      },
      "outputs": [],
      "source": [
        "'''# Import the AudioSegment class from the pydub library for audio processing tasks.\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Define a function to convert a WAV file to MP3 format.\n",
        "# Parameters:\n",
        "# - wav_file: The path to the input WAV file.\n",
        "# - mp3_file: (Optional) The path to the output MP3 file. If not provided, it will be generated automatically.\n",
        "def convert_wav_to_mp3(wav_file, mp3_file=None):\n",
        "    # Load the WAV file as an AudioSegment object.\n",
        "    audio = AudioSegment.from_wav(wav_file)\n",
        "\n",
        "    # If the output MP3 file name is not provided, generate it by replacing the .wav extension with .mp3.\n",
        "    if not mp3_file:\n",
        "        mp3_file = wav_file.rsplit('.', 1)[0] + \".mp3\"\n",
        "\n",
        "    # Export the audio data as an MP3 file.\n",
        "    audio.export(mp3_file, format=\"mp3\")\n",
        "\n",
        "    # Print a message indicating that the conversion is complete.\n",
        "    print(f\"Conversion complete: {mp3_file}\")\n",
        "\n",
        "# Example usage of the function to convert a WAV file to MP3 format.\n",
        "convert_wav_to_mp3(\"/content/converted_audio.wav\")'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Import the whisper library for speech recognition and transcription tasks.\n",
        "import whisper\n",
        "\n",
        "# Define a function to transcribe audio files using the Whisper model.\n",
        "# Parameters:\n",
        "# - audio_file: The path to the input audio file (e.g., .mp3 or .wav).\n",
        "# - model_size: The size of the Whisper model to use (default is \"base\").\n",
        "#   Other options include \"tiny\", \"small\", \"medium\", and \"large\" for different accuracy and speed trade-offs.\n",
        "def transcribe_audio(audio_file, model_size=\"base\"):\n",
        "    # Load the Whisper model of the specified size.\n",
        "    model = whisper.load_model(model_size)\n",
        "\n",
        "    # Transcribe the audio file and store the result.\n",
        "    result = model.transcribe(audio_file)\n",
        "\n",
        "    # Generate a file name for saving the transcription by replacing the original file extension with \"_transcript.txt\".\n",
        "    transcript_file = audio_file.rsplit('.', 1)[0] + \"_transcript.txt\"\n",
        "\n",
        "    # Save the transcription text to a file in UTF-8 encoding.\n",
        "    with open(transcript_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(result[\"text\"])\n",
        "\n",
        "    # Print a message indicating where the transcription has been saved.\n",
        "    print(f\"Transcription saved to {transcript_file}\")\n",
        "\n",
        "    # Return the transcription text.\n",
        "    return result[\"text\"]\n",
        "\n",
        "# Example usage of the function:\n",
        "# Specify the path to the audio file to be transcribed.\n",
        "audio_path = \"/content/converted_audio.mp3\"  # Change this to your actual file path\n",
        "\n",
        "# Call the transcription function and print the transcript.\n",
        "transcript = transcribe_audio(audio_path)\n",
        "print(\"Transcript:\", transcript)\n"
      ],
      "metadata": {
        "id": "ebdkkDdLFF9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Install the pydub library, which is used for audio processing tasks like converting, slicing, and merging audio files.\n",
        "# The exclamation mark (!) is used to run shell commands directly from Jupyter Notebook or Google Colab.\n",
        "!pip install pydub\n",
        "\n",
        "# Install the ffmpeg package, which is a powerful tool for handling audio and video files.\n",
        "# ffmpeg is required by pydub to handle various audio formats (e.g., MP3 to WAV conversion).\n",
        "# The 'apt install' command is used to install packages on Debian-based Linux systems, like Ubuntu and Google Colab.\n",
        "!apt install ffmpeg\n"
      ],
      "metadata": {
        "id": "hf3qnnTMKa_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Install the moviepy library, which is used for video editing tasks like cutting, concatenating, and converting videos.\n",
        "# It also supports extracting and manipulating audio from video files.\n",
        "# The exclamation mark (!) is used to run shell commands directly from Jupyter Notebook or Google Colab.\n",
        "!pip install moviepy\n"
      ],
      "metadata": {
        "id": "G1G6UZ1RKtI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Import the VideoFileClip class from the moviepy library for video processing tasks.\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "# Define the path to the video file that you want to process.\n",
        "video_path = \"/content/videoplayback.mp4\"\n",
        "\n",
        "# Load the video file into a VideoFileClip object.\n",
        "video = VideoFileClip(video_path)\n",
        "\n",
        "# Extract the audio track from the video.\n",
        "audio = video.audio\n",
        "\n",
        "# Define the path and format for the extracted audio file.\n",
        "# Here, we save the audio in MP3 format, but you can change it to WAV or other formats if needed.\n",
        "audio_path = \"extracted_audio.mp3\"\n",
        "audio.write_audiofile(audio_path)\n",
        "\n",
        "# Print a message indicating that the audio extraction is complete and specify the file name.\n",
        "print(f\"Audio extracted and saved as {audio_path}\")\n"
      ],
      "metadata": {
        "id": "XFm7dKkLKzEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display as ipd\n",
        "\n",
        "# Play the extracted audio\n",
        "ipd.Audio(\"/content/extracted_audio.mp3\")"
      ],
      "metadata": {
        "id": "Un7j6og8LRQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "# Load the audio file\n",
        "audio = AudioSegment.from_mp3(\"/content/extracted_audio.mp3\")\n",
        "\n",
        "# Get the duration of the audio in milliseconds\n",
        "duration = len(audio)\n",
        "\n",
        "# Calculate the midpoint\n",
        "midpoint = duration // 2\n",
        "\n",
        "# Split the audio into two halves\n",
        "first_half = audio[:midpoint]\n",
        "second_half = audio[midpoint:]\n",
        "\n",
        "# Export the two halves as separate files\n",
        "first_half.export(\"/content/first_half.mp3\", format=\"mp3\")\n",
        "second_half.export(\"/content/second_half.mp3\", format=\"mp3\")\n",
        "\n",
        "print(\"Audio has been split into two halves successfully!\")"
      ],
      "metadata": {
        "id": "xI1gQCw9PbHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display as ipd\n",
        "\n",
        "# Play the first half\n",
        "print(\"Playing the first half:\")\n",
        "ipd.display(ipd.Audio(\"/content/first_half.mp3\"))\n",
        "\n",
        "# Play the second half\n",
        "print(\"Playing the second half:\")\n",
        "ipd.display(ipd.Audio(\"/content/second_half.mp3\"))\n"
      ],
      "metadata": {
        "id": "Eg3it4qaP1Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "def transcribe_audio(audio_file, model_size=\"base\"):\n",
        "    # Load the Whisper model\n",
        "    model = whisper.load_model(model_size)\n",
        "\n",
        "    # Transcribe the audio file\n",
        "    result = model.transcribe(audio_file)\n",
        "\n",
        "    # Save transcript to a file\n",
        "    transcript_file = audio_file.rsplit('.', 1)[0] + \"_transcript.txt\"\n",
        "    with open(transcript_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(result[\"text\"])\n",
        "\n",
        "    print(f\"Transcription saved to {transcript_file}\")\n",
        "    return result[\"text\"]\n",
        "\n",
        "# Example usage\n",
        "audio_path = \"/content/first_half.mp3\"  # Change this to your actual file\n",
        "transcript = transcribe_audio(audio_path)\n",
        "print(\"Transcript:\",transcript)"
      ],
      "metadata": {
        "id": "xRBko2HnLuJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "def transcribe_audio(audio_file, model_size=\"base\"):\n",
        "    # Load the Whisper model\n",
        "    model = whisper.load_model(model_size)\n",
        "\n",
        "    # Transcribe the audio file\n",
        "    result = model.transcribe(audio_file)\n",
        "\n",
        "    # Save transcript to a file\n",
        "    transcript_file = audio_file.rsplit('.', 1)[0] + \"_transcript.txt\"\n",
        "    with open(transcript_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(result[\"text\"])\n",
        "\n",
        "    print(f\"Transcription saved to {transcript_file}\")\n",
        "    return result[\"text\"]\n",
        "\n",
        "# Example usage\n",
        "audio_path = \"/content/second_half.mp3\"  # Change this to your actual file\n",
        "transcript = transcribe_audio(audio_path)\n",
        "print(\"Transcript:\",transcript)"
      ],
      "metadata": {
        "id": "oeM6tsrSMe_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate code to read the video url and extract the transcript of a video\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install youtube-transcript-api\n",
        "\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "def get_transcript(video_url):\n",
        "    \"\"\"\n",
        "    Extracts the transcript from a YouTube video URL.\n",
        "\n",
        "    Args:\n",
        "        video_url: The URL of the YouTube video.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, where each dictionary represents a segment of the transcript\n",
        "        with 'text' and 'start' keys, or None if an error occurs.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Extract video ID from URL\n",
        "        video_id = video_url.split(\"v=\")[-1]\n",
        "        if \"&\" in video_id:\n",
        "          video_id = video_id[:video_id.index(\"&\")]\n",
        "\n",
        "        # Get transcript\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        return transcript\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "video_url = input(\"Enter the YouTube video URL: \")\n",
        "transcript = get_transcript(video_url)\n",
        "\n",
        "if transcript:\n",
        "  print(\"\\nTranscript:\")\n",
        "  for segment in transcript:\n",
        "    print(f\"{segment['text']}\")"
      ],
      "metadata": {
        "id": "UEl7ZeRrRd-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the final code"
      ],
      "metadata": {
        "id": "0zBb_YAXAwqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube_transcript_api"
      ],
      "metadata": {
        "id": "l1BHLTz8xUD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytube speechrecognition pydub"
      ],
      "metadata": {
        "id": "DmyTmUzHvsiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import urllib.parse\n",
        "import requests\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from pytube import YouTube\n",
        "import speech_recognition as sr\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "def extract_video_id(video_url):\n",
        "    \"\"\"\n",
        "    Extracts the YouTube video ID from various URL formats.\n",
        "    \"\"\"\n",
        "    parsed_url = urllib.parse.urlparse(video_url)\n",
        "    query_params = urllib.parse.parse_qs(parsed_url.query)\n",
        "\n",
        "    if \"v\" in query_params:\n",
        "        return query_params[\"v\"][0]\n",
        "\n",
        "    match = re.search(r\"(youtu\\.be/|youtube\\.com/embed/|youtube\\.com/shorts/)([\\w-]+)\", video_url)\n",
        "    if match:\n",
        "        return match.group(2)\n",
        "\n",
        "    return None\n",
        "\n",
        "def download_audio(video_url):\n",
        "    \"\"\"\n",
        "    Downloads the audio using yt-dlp with cookies and returns the file path.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ydl_opts = {\n",
        "            'format': 'bestaudio/best',\n",
        "            'outtmpl': 'audio.%(ext)s',\n",
        "            'cookiefile': 'cookies (1).txt',  # Use the exported cookies\n",
        "            'postprocessors': [{\n",
        "                'key': 'FFmpegExtractAudio',\n",
        "                'preferredcodec': 'mp3',\n",
        "                'preferredquality': '192',\n",
        "            }],\n",
        "        }\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(video_url, download=True)\n",
        "            return \"audio.mp3\"\n",
        "    except Exception as e:\n",
        "        return f\"Error downloading audio: {str(e)}\"\n",
        "\n",
        "def convert_audio_to_wav(audio_file):\n",
        "    \"\"\"\n",
        "    Converts the downloaded MP3 audio to WAV format using pydub.\n",
        "    \"\"\"\n",
        "    wav_file = \"audio.wav\"\n",
        "    try:\n",
        "        AudioSegment.from_mp3(audio_file).export(wav_file, format=\"wav\")\n",
        "        return wav_file\n",
        "    except Exception as e:\n",
        "        return f\"Error converting to WAV: {str(e)}\"\n",
        "\n",
        "def transcribe_audio(audio_path, chunk_length=30):\n",
        "    \"\"\"\n",
        "    Splits audio into smaller chunks and transcribes each chunk separately.\n",
        "    Args:\n",
        "        audio_path (str): Path to the audio file.\n",
        "        chunk_length (int): Length of each chunk in seconds (default: 30).\n",
        "    Returns:\n",
        "        str: Transcribed text from the audio.\n",
        "    \"\"\"\n",
        "    recognizer = sr.Recognizer()\n",
        "    audio = AudioSegment.from_wav(audio_path)\n",
        "    total_duration = len(audio) / 1000  # Convert to seconds\n",
        "    transcribed_text = []\n",
        "\n",
        "    print(\"Transcribing audio in chunks...\")\n",
        "\n",
        "    # Split and transcribe audio in chunks\n",
        "    for start in range(0, int(total_duration), chunk_length):\n",
        "        end = min(start + chunk_length, int(total_duration))\n",
        "        chunk = audio[start * 1000:end * 1000]  # Extract chunk in milliseconds\n",
        "        chunk.export(\"chunk.wav\", format=\"wav\")  # Save chunk temporarily\n",
        "\n",
        "        with sr.AudioFile(\"chunk.wav\") as source:\n",
        "            try:\n",
        "                audio_data = recognizer.record(source)\n",
        "                text = recognizer.recognize_google(audio_data)\n",
        "                transcribed_text.append(text)\n",
        "            except sr.UnknownValueError:\n",
        "                transcribed_text.append(\"[Unintelligible]\")\n",
        "            except sr.RequestError as e:\n",
        "                return f\"Error with the speech recognition service: {str(e)}\"\n",
        "\n",
        "    os.remove(\"chunk.wav\")  # Clean up temporary chunk file\n",
        "    return \"\\n\".join(transcribed_text)\n",
        "\n",
        "def get_transcript_unlisted(video_url):\n",
        "    \"\"\"\n",
        "    Tries to fetch the transcript using youtube_transcript_api first,\n",
        "    then falls back to downloading and transcribing audio if necessary.\n",
        "    \"\"\"\n",
        "    video_id = extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "        return \"Invalid YouTube URL.\"\n",
        "\n",
        "    # Try to fetch transcript using youtube_transcript_api\n",
        "    try:\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        return \"\\n\".join([item['text'] for item in transcript])\n",
        "    except:\n",
        "        print(\"Transcript not available via API, attempting audio transcription...\")\n",
        "\n",
        "    # Download and transcribe audio if no transcript is available\n",
        "    audio_file = download_audio(video_url)\n",
        "    if \"Error\" in audio_file:\n",
        "        return audio_file\n",
        "\n",
        "    wav_file = convert_audio_to_wav(audio_file)\n",
        "    if \"Error\" in wav_file:\n",
        "        return wav_file\n",
        "\n",
        "    transcription = transcribe_audio(wav_file)\n",
        "\n",
        "    # Cleanup temporary files\n",
        "    os.remove(audio_file)\n",
        "    os.remove(wav_file)\n",
        "\n",
        "    return transcription\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    video_url = input(\"Enter the YouTube video URL: \")\n",
        "    transcript = get_transcript_unlisted(video_url)\n",
        "    print(\"\\nTranscript:\\n\", transcript)\n"
      ],
      "metadata": {
        "id": "q-x3x0UT0qMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install yt-dlp"
      ],
      "metadata": {
        "id": "9K-ihhxF1Bw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yt_dlp"
      ],
      "metadata": {
        "id": "L6hBfp9g1wKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "import os"
      ],
      "metadata": {
        "id": "hiuwHRwI4bI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def get_transcript():\n",
        "    \"\"\"Prompt user to input the transcript text.\"\"\"\n",
        "    print(\"Enter the transcript (end input with 'END'):\")\n",
        "    lines = []\n",
        "    while True:\n",
        "        line = input()\n",
        "        if line.strip().upper() == \"END\":\n",
        "            break\n",
        "        lines.append(line)\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def extract_topics(transcript, num_topics=5):\n",
        "    \"\"\"Extract topics from the transcript using TF-IDF and clustering.\"\"\"\n",
        "    sentences = sent_tokenize(transcript)\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    kmeans = KMeans(n_clusters=num_topics, random_state=42, n_init=10)\n",
        "    kmeans.fit(X)\n",
        "\n",
        "    topic_sentences = {i: [] for i in range(num_topics)}\n",
        "    for i, label in enumerate(kmeans.labels_):\n",
        "        topic_sentences[label].append(sentences[i])\n",
        "\n",
        "    topics = {f\"Topic {i+1}\": \" \".join(topic_sentences[i][:3]) for i in range(num_topics)}\n",
        "    return topics\n",
        "\n",
        "def segment_transcript(transcript):\n",
        "    \"\"\"Segment the transcript based on extracted topics.\"\"\"\n",
        "    topics = extract_topics(transcript)\n",
        "\n",
        "    segmented_text = \"\"\n",
        "    for heading, example_text in topics.items():\n",
        "        segmented_text += f\"\\n### {heading}\\n- {example_text}...\\n\"\n",
        "\n",
        "    return segmented_text\n",
        "\n",
        "def main():\n",
        "    nltk.download('punkt')  # Ensure required tokenizer is available\n",
        "    transcript = get_transcript()\n",
        "    segmented_text = segment_transcript(transcript)\n",
        "\n",
        "    output_file = input(\"Enter the desired output file name (or press Enter for default: segmented_transcript.txt): \")\n",
        "    output_file = output_file if output_file else \"segmented_transcript.txt\"\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(segmented_text)\n",
        "\n",
        "    print(f\"Segmentation complete! Output saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "bV-MlqdS13oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def read_transcript(file_path):\n",
        "    \"\"\"Read the transcript from a file.\"\"\"\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        return file.read()\n",
        "\n",
        "def extract_topics(transcript, num_topics=5):\n",
        "    \"\"\"Extract topics from the transcript using TF-IDF and clustering.\"\"\"\n",
        "    sentences = sent_tokenize(transcript)\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    kmeans = KMeans(n_clusters=num_topics, random_state=42, n_init=10)\n",
        "    kmeans.fit(X)\n",
        "\n",
        "    topic_sentences = {i: [] for i in range(num_topics)}\n",
        "    for i, label in enumerate(kmeans.labels_):\n",
        "        topic_sentences[label].append(sentences[i])\n",
        "\n",
        "    topics = {f\"Topic {i+1}\": \" \".join(topic_sentences[i][:3]) for i in range(num_topics)}\n",
        "    return topics\n",
        "\n",
        "def segment_transcript(transcript):\n",
        "    \"\"\"Segment the transcript based on extracted topics.\"\"\"\n",
        "    topics = extract_topics(transcript)\n",
        "\n",
        "    segmented_text = \"\"\n",
        "    for heading, example_text in topics.items():\n",
        "        segmented_text += f\"\\n### {heading}\\n- {example_text}...\\n\"\n",
        "\n",
        "    return segmented_text\n",
        "\n",
        "def main():\n",
        "    nltk.download('punkt')  # Ensure required tokenizer is available\n",
        "    input_file = input(\"Enter the path of the transcript file: \")\n",
        "    transcript = read_transcript(input_file)\n",
        "    segmented_text = segment_transcript(transcript)\n",
        "\n",
        "    output_file = input(\"Enter the desired output file name (or press Enter for default: segmented_transcript.txt): \")\n",
        "    output_file = output_file if output_file else \"segmented_transcript.txt\"\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(segmented_text)\n",
        "\n",
        "    print(f\"Segmentation complete! Output saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "zaxdAq-s1egW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def read_transcript(file_path):\n",
        "    \"\"\"Read the transcript from a file.\"\"\"\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        return file.read()\n",
        "\n",
        "def extract_topics(transcript, num_topics=5):\n",
        "    \"\"\"Extract topics from the transcript using TF-IDF and clustering.\"\"\"\n",
        "    nltk.download('punkt')  # Ensure required tokenizer is available\n",
        "    sentences = sent_tokenize(transcript)\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    kmeans = KMeans(n_clusters=num_topics, random_state=42, n_init=10)\n",
        "    kmeans.fit(X)\n",
        "\n",
        "    topic_sentences = {i: [] for i in range(num_topics)}\n",
        "    for i, label in enumerate(kmeans.labels_):\n",
        "        topic_sentences[label].append(sentences[i])\n",
        "\n",
        "    topics = {f\"Topic {i+1}\": \" \".join(topic_sentences[i][:3]) for i in range(num_topics)}\n",
        "    return topics\n",
        "\n",
        "def segment_transcript(transcript):\n",
        "    \"\"\"Segment the transcript based on extracted topics.\"\"\"\n",
        "    topics = extract_topics(transcript)\n",
        "\n",
        "    segmented_text = \"\"\n",
        "    for heading, example_text in topics.items():\n",
        "        segmented_text += f\"\\n### {heading}\\n- {example_text}...\\n\"\n",
        "\n",
        "    return segmented_text\n",
        "\n",
        "def main():\n",
        "    nltk.download('punkt')  # Ensure required tokenizer is available\n",
        "    input_file = input(\"Enter the path of the transcript file: \")\n",
        "    transcript = read_transcript(input_file)\n",
        "    segmented_text = segment_transcript(transcript)\n",
        "\n",
        "    output_file = input(\"Enter the desired output file name (or press Enter for default: segmented_transcript.txt): \")\n",
        "    output_file = output_file if output_file else \"segmented_transcript.txt\"\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(segmented_text)\n",
        "\n",
        "    print(f\"Segmentation complete! Output saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "zNPE72Hg376G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "7S9rQNsQ4sRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Remove existing nltk_data directory\n",
        "nltk_data_path = os.path.expanduser('~/nltk_data')\n",
        "if os.path.exists(nltk_data_path):\n",
        "    shutil.rmtree(nltk_data_path)\n",
        "\n",
        "# Download necessary resources again\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "P4a-GXLI4xsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def read_transcript(file_path):\n",
        "    \"\"\"Read the transcript from a file.\"\"\"\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        return file.read()\n",
        "\n",
        "def extract_topics(transcript, num_topics=5):\n",
        "    \"\"\"Extract topics from the transcript using TF-IDF and clustering.\"\"\"\n",
        "    nltk.download('punkt')  # Ensure required tokenizer is available\n",
        "    sentences = sent_tokenize(transcript)\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    kmeans = KMeans(n_clusters=num_topics, random_state=42, n_init=10)\n",
        "    kmeans.fit(X)\n",
        "\n",
        "    topic_sentences = {i: [] for i in range(num_topics)}\n",
        "    for i, label in enumerate(kmeans.labels_):\n",
        "        topic_sentences[label].append(sentences[i])\n",
        "\n",
        "    topics = {f\"Topic {i+1}\": \" \".join(topic_sentences[i][:3]) for i in range(num_topics)}\n",
        "    return topics\n",
        "\n",
        "def segment_transcript(transcript):\n",
        "    \"\"\"Segment the transcript based on extracted topics.\"\"\"\n",
        "    topics = extract_topics(transcript)\n",
        "\n",
        "    segmented_text = \"\"\n",
        "    for heading, example_text in topics.items():\n",
        "        segmented_text += f\"\\n### {heading}\\n- {example_text}...\\n\"\n",
        "\n",
        "    return segmented_text\n",
        "\n",
        "def main():\n",
        "    nltk.download('punkt')  # Ensure required tokenizer is available\n",
        "    input_file = input(\"Enter the path of the transcript file: \")\n",
        "    transcript = read_transcript(input_file)\n",
        "    segmented_text = segment_transcript(transcript)\n",
        "\n",
        "    output_file = input(\"Enter the desired output file name (or press Enter for default: segmented_transcript.txt): \")\n",
        "    output_file = output_file if output_file else \"segmented_transcript.txt\"\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(segmented_text)\n",
        "\n",
        "    print(f\"Segmentation complete! Output saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "KHyQhSf75Itl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "ufJ74-oe5Puo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "print(nltk.data.path)"
      ],
      "metadata": {
        "id": "ixpo1bL05bBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.data.path.append('/usr/local/share/nltk_data')"
      ],
      "metadata": {
        "id": "O84hdxac5dSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "import nltk\n",
        "\n",
        "nltk_data_path = os.path.expanduser('~/nltk_data')\n",
        "if os.path.exists(nltk_data_path):\n",
        "    shutil.rmtree(nltk_data_path)\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "BPxdjDk55fzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(nltk.data.find(\"tokenizers\")))"
      ],
      "metadata": {
        "id": "4GDkr2Of5ifx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "print(nltk.data.path)"
      ],
      "metadata": {
        "id": "s-3_0sEW5k63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "nltk_data_path = os.path.expanduser(\"~/nltk_data/tokenizers\")\n",
        "if os.path.exists(nltk_data_path):\n",
        "    print(os.listdir(nltk_data_path))\n",
        "else:\n",
        "    print(\"NLTK tokenizer directory not found!\")"
      ],
      "metadata": {
        "id": "RSYKt6cV5xm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "nltk_data_path = os.path.expanduser(\"~/nltk_data/tokenizers\")\n",
        "if not os.path.exists(nltk_data_path):\n",
        "    os.makedirs(nltk_data_path)\n",
        "\n",
        "print(f\"Created directory: {nltk_data_path}\")"
      ],
      "metadata": {
        "id": "-FBOprtC5zla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "3mvqFwFh5-gK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "nltk_data_path = os.path.expanduser(\"~/nltk_data/tokenizers\")\n",
        "if os.path.exists(nltk_data_path):\n",
        "    print(os.listdir(nltk_data_path))\n",
        "else:\n",
        "    print(\"NLTK tokenizer directory still missing!\")"
      ],
      "metadata": {
        "id": "I-rFi6Ul6BCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import os\n",
        "\n",
        "# Set a specific NLTK data directory\n",
        "nltk_data_dir = os.path.expanduser(\"~/nltk_data\")\n",
        "if not os.path.exists(nltk_data_dir):\n",
        "    os.makedirs(nltk_data_dir)\n",
        "\n",
        "# Append the path manually in case it's not detected\n",
        "nltk.data.path.append(nltk_data_dir)\n",
        "\n",
        "# Download the 'punkt' tokenizer\n",
        "nltk.download('punkt', download_dir=nltk_data_dir)\n",
        "\n",
        "# Verify that the tokenizer exists now\n",
        "tokenizer_path = os.path.join(nltk_data_dir, \"tokenizers\")\n",
        "print(f\"Checking for tokenizers in: {tokenizer_path}\")\n",
        "print(os.listdir(tokenizer_path) if os.path.exists(tokenizer_path) else \"Tokenizer directory not found!\")"
      ],
      "metadata": {
        "id": "BbC3Cnht6DKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.data import find\n",
        "\n",
        "try:\n",
        "    punkt_path = find('tokenizers/punkt')\n",
        "    print(f\"'punkt' found at: {punkt_path}\")\n",
        "except LookupError:\n",
        "    print(\"Could not find 'punkt' in nltk data.\")"
      ],
      "metadata": {
        "id": "_O1RhvhH6TWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "nltk_data_path = os.path.expanduser(\"~/nltk_data\")\n",
        "\n",
        "if os.path.exists(nltk_data_path):\n",
        "    shutil.rmtree(nltk_data_path)\n",
        "\n",
        "print(\"Deleted existing NLTK data. Ready for fresh installation.\")"
      ],
      "metadata": {
        "id": "nG-km4JX6fUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import os\n",
        "\n",
        "nltk_data_path = os.path.expanduser(\"~/nltk_data\")\n",
        "os.makedirs(nltk_data_path, exist_ok=True)\n",
        "\n",
        "nltk.download('punkt', download_dir=nltk_data_path)\n",
        "\n",
        "print(\"Downloaded 'punkt' to:\", nltk_data_path)"
      ],
      "metadata": {
        "id": "sw9QcBNF6soa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "tokenizer_path = os.path.expanduser(\"~/nltk_data/tokenizers\")\n",
        "print(\"Checking for tokenizers in:\", tokenizer_path)\n",
        "print(os.listdir(tokenizer_path) if os.path.exists(tokenizer_path) else \"Tokenizer directory not found!\")"
      ],
      "metadata": {
        "id": "IXNUawzt6wc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: segment the transcript from the path with relevant topics\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import os\n",
        "\n",
        "def read_transcript(file_path):\n",
        "    \"\"\"Read the transcript from a file.\"\"\"\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        return file.read()\n",
        "\n",
        "def extract_topics(transcript, num_topics=5):\n",
        "    \"\"\"Extract topics from the transcript using TF-IDF and clustering.\"\"\"\n",
        "    try:\n",
        "        sentences = sent_tokenize(transcript)\n",
        "        vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "        kmeans = KMeans(n_clusters=num_topics, random_state=42, n_init=10)\n",
        "        kmeans.fit(X)\n",
        "\n",
        "        topic_sentences = {i: [] for i in range(num_topics)}\n",
        "        for i, label in enumerate(kmeans.labels_):\n",
        "            topic_sentences[label].append(sentences[i])\n",
        "\n",
        "        topics = {f\"Topic {i+1}\": \" \".join(topic_sentences[i][:3]) for i in range(num_topics)}\n",
        "        return topics\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during topic extraction: {e}\")\n",
        "        return None\n",
        "\n",
        "def segment_transcript(transcript):\n",
        "    \"\"\"Segment the transcript based on extracted topics.\"\"\"\n",
        "    topics = extract_topics(transcript)\n",
        "    if topics is None:\n",
        "        return \"Topic extraction failed.\"\n",
        "\n",
        "    segmented_text = \"\"\n",
        "    for heading, example_text in topics.items():\n",
        "        segmented_text += f\"\\n### {heading}\\n- {example_text}...\\n\"\n",
        "\n",
        "    return segmented_text\n",
        "\n",
        "def main():\n",
        "    nltk.download('punkt', quiet=True) # Download punkt if not present, suppress output\n",
        "    input_file = input(\"Enter the path of the transcript file: \")\n",
        "\n",
        "    if not os.path.exists(input_file):\n",
        "        print(f\"Error: File '{input_file}' not found.\")\n",
        "        return\n",
        "\n",
        "    transcript = read_transcript(input_file)\n",
        "    segmented_text = segment_transcript(transcript)\n",
        "\n",
        "    output_file = input(\"Enter the desired output file name (or press Enter for default: segmented_transcript.txt): \")\n",
        "    output_file = output_file if output_file else \"segmented_transcript.txt\"\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(segmented_text)\n",
        "\n",
        "    print(f\"Segmentation complete! Output saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "pxF3jbP36z-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Another approach"
      ],
      "metadata": {
        "id": "Om2RbA_47t-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers nltk"
      ],
      "metadata": {
        "id": "kly8IWR1LqU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "from transformers import pipeline\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import torch\n",
        "\n",
        "# Download the Punkt tokenizer for sentence splitting if not already downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define file paths\n",
        "input_file = '/content/Kalsi sir transcript.txt'        # Replace with your transcript file name\n",
        "output_file = 'segmented_transcript.txt'\n",
        "\n",
        "# Load the summarization model for heading generation\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "# Read the transcript\n",
        "with open(input_file, 'r', encoding='utf-8') as file:\n",
        "    transcript = file.read()\n",
        "\n",
        "# Split transcript into sentences\n",
        "sentences = sent_tokenize(transcript)\n",
        "\n",
        "# Segment transcript into chunks of approximately 500 words for manageable heading generation\n",
        "chunk_size = 300\n",
        "chunks = []\n",
        "current_chunk = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    current_chunk.append(sentence)\n",
        "    if sum(len(s.split()) for s in current_chunk) >= chunk_size:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "        current_chunk = []\n",
        "\n",
        "if current_chunk:\n",
        "    chunks.append(' '.join(current_chunk))\n",
        "\n",
        "# Generate headings and organize content\n",
        "segmented_content = \"\"\n",
        "for idx, chunk in enumerate(chunks, 1):\n",
        "    if not chunk.strip():\n",
        "        continue  # Skip empty chunks\n",
        "\n",
        "    # Limit chunk to 1024 tokens if it's too large\n",
        "    tokens = tokenizer(chunk)['input_ids']\n",
        "    if len(tokens) > 1024:\n",
        "        chunk = tokenizer.decode(tokens[:1024])\n",
        "\n",
        "    # Generate a heading using summarization\n",
        "    heading = summarizer(chunk, max_length=10, min_length=3, do_sample=False)[0]['summary_text']\n",
        "    segmented_content += f\"### {heading}\\n\\n{chunk}\\n\\n\"\n",
        "\n",
        "# Save segmented content to a new file\n",
        "with open(output_file, 'w', encoding='utf-8') as file:\n",
        "    file.write(segmented_content)\n",
        "\n",
        "print(f\"Segmented transcript saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "dgGKqrU8Lq7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "PyQldndvL1bE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
      ],
      "metadata": {
        "id": "1woex7A3MVn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import torch\n",
        "\n",
        "# Download the Punkt tokenizer for sentence splitting if not already downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define file paths\n",
        "input_file = '/content/Kalsi sir transcript.txt'  # Replace with your transcript file name\n",
        "output_file = 'segmented_transcript.txt'\n",
        "\n",
        "# Load the summarization model and tokenizer for heading generation\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0 if torch.cuda.is_available() else -1)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\", use_fast=True)\n",
        "\n",
        "# Read and clean the transcript\n",
        "with open(input_file, 'r', encoding='utf-8') as file:\n",
        "    transcript = file.read()\n",
        "\n",
        "# Clean special characters and excessive spaces\n",
        "transcript = re.sub(r'[^\\x00-\\x7F]+', ' ', transcript)  # Remove non-ASCII characters\n",
        "transcript = re.sub(r'\\s+', ' ', transcript).strip()     # Replace multiple spaces with a single space\n",
        "\n",
        "# Split transcript into paragraphs based on double newlines or fallback to sentence tokenization\n",
        "paragraphs = transcript.split('\\n\\n') if '\\n\\n' in transcript else [transcript]\n",
        "\n",
        "# Create clearer and more coherent chunks\n",
        "chunk_size = 300  # Limit chunk size to ~400 tokens for clarity and coherence\n",
        "chunks = []\n",
        "\n",
        "for paragraph in paragraphs:\n",
        "    sentences = sent_tokenize(paragraph)\n",
        "    current_chunk = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        current_chunk.append(sentence)\n",
        "        # Check token length instead of word count for more reliable chunking\n",
        "        token_count = len(tokenizer(' '.join(current_chunk))['input_ids'])\n",
        "        if token_count >= chunk_size:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = []\n",
        "\n",
        "    # Append remaining sentences in the current chunk\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "# Generate headings and organize content\n",
        "segmented_content = \"\"\n",
        "for idx, chunk in enumerate(chunks, 1):\n",
        "    if not chunk.strip() or len(chunk.split()) < 5:\n",
        "        continue  # Skip empty or very short chunks\n",
        "\n",
        "    # Limit chunk to 1024 tokens if it's too large\n",
        "    tokens = tokenizer(chunk)['input_ids']\n",
        "    if len(tokens) > 1024:\n",
        "        chunk = tokenizer.decode(tokens[:1024])\n",
        "\n",
        "    # Debugging print to track token info\n",
        "    print(f\"Processing chunk {idx}/{len(chunks)} with {len(tokens)} tokens.\")\n",
        "\n",
        "    # Generate a heading using summarization\n",
        "    heading = summarizer(chunk, max_length=10, min_length=3, do_sample=False)[0]['summary_text']\n",
        "    segmented_content += f\"### {heading.strip()}\\n\\n{chunk.strip()}\\n\\n\"\n",
        "\n",
        "# Save segmented content to a new file\n",
        "with open(output_file, 'w', encoding='utf-8') as file:\n",
        "    file.write(segmented_content)\n",
        "\n",
        "print(f\"Segmented transcript saved to {output_file}\")"
      ],
      "metadata": {
        "id": "X8mObjlXNsIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import torch\n",
        "\n",
        "# Download the Punkt tokenizer for sentence splitting if not already downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define file paths\n",
        "input_file = '/content/Kalsi sir transcript.txt'  # Replace with your transcript file name\n",
        "output_file = 'segmented_transcript.txt'\n",
        "\n",
        "# Load the summarization model and tokenizer for heading generation\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0 if torch.cuda.is_available() else -1)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\", use_fast=True)\n",
        "\n",
        "# Re-download the tokenizer files to fix potential corruption\n",
        "tokenizer.save_pretrained(\"./tokenizer\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./tokenizer\")\n",
        "\n",
        "# Read and clean the transcript\n",
        "with open(input_file, 'r', encoding='utf-8') as file:\n",
        "    transcript = file.read()\n",
        "\n",
        "# Clean special characters and excessive spaces\n",
        "transcript = re.sub(r'[^\\x00-\\x7F]+', ' ', transcript)  # Remove non-ASCII characters\n",
        "transcript = re.sub(r'\\s+', ' ', transcript).strip()     # Replace multiple spaces with a single space\n",
        "\n",
        "# Split transcript into paragraphs based on double newlines or fallback to sentence tokenization\n",
        "paragraphs = transcript.split('\\n\\n') if '\\n\\n' in transcript else [transcript]\n",
        "\n",
        "# Create clearer and more coherent chunks\n",
        "chunk_size = 400  # Limit chunk size to ~400 tokens for clarity and coherence\n",
        "chunks = []\n",
        "\n",
        "for paragraph in paragraphs:\n",
        "    sentences = sent_tokenize(paragraph)\n",
        "    current_chunk = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        current_chunk.append(sentence)\n",
        "        # Check token length instead of word count for more reliable chunking\n",
        "        token_count = len(tokenizer(' '.join(current_chunk))['input_ids'])\n",
        "        if token_count >= chunk_size:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = []\n",
        "\n",
        "    # Append remaining sentences in the current chunk\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "# Generate headings and organize content\n",
        "segmented_content = \"\"\n",
        "for idx, chunk in enumerate(chunks, 1):\n",
        "    if not chunk.strip() or len(chunk.split()) < 5:\n",
        "        continue  # Skip empty or very short chunks\n",
        "\n",
        "    # Tokenize the chunk\n",
        "    tokens = tokenizer(chunk)['input_ids']\n",
        "\n",
        "    # Split chunk into smaller parts if it exceeds 1024 tokens\n",
        "    if len(tokens) > 1024:\n",
        "        sub_chunks = [tokens[i:i + 1024] for i in range(0, len(tokens), 1024)]\n",
        "    else:\n",
        "        sub_chunks = [tokens]\n",
        "\n",
        "    # Process each sub-chunk separately\n",
        "    for sub_idx, sub_chunk in enumerate(sub_chunks, 1):\n",
        "        # Decode sub-chunk back to text\n",
        "        sub_chunk_text = tokenizer.decode(sub_chunk, skip_special_tokens=True)\n",
        "\n",
        "        # Debugging print to track token info\n",
        "        print(f\"Processing chunk {idx}.{sub_idx}/{len(chunks)} with {len(sub_chunk)} tokens.\")\n",
        "\n",
        "        # Generate a heading using summarization\n",
        "        try:\n",
        "            heading = summarizer(sub_chunk_text, max_length=10, min_length=3, do_sample=False)[0]['summary_text']\n",
        "            segmented_content += f\"### {heading.strip()}\\n\\n{sub_chunk_text.strip()}\\n\\n\"\n",
        "        except IndexError as e:\n",
        "            print(f\"Skipping a chunk due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "# Save segmented content to a new file\n",
        "with open(output_file, 'w', encoding='utf-8') as file:\n",
        "    file.write(segmented_content)\n",
        "\n",
        "print(f\"Segmented transcript saved to {output_file}\")"
      ],
      "metadata": {
        "id": "u7pSZSyjQBEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import torch\n",
        "\n",
        "# Download the Punkt tokenizer for sentence splitting if not already downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define file paths\n",
        "input_file = '/content/Kalsi sir transcript.txt'  # Replace with your transcript file name\n",
        "output_file = 'segmented_transcript.txt'\n",
        "\n",
        "# Load summarization and zero-shot classification models\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0 if torch.cuda.is_available() else -1)\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\", use_fast=True)\n",
        "\n",
        "# Load sentence-transformers model for semantic similarity\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Read and clean the transcript\n",
        "with open(input_file, 'r', encoding='utf-8') as file:\n",
        "    transcript = file.read()\n",
        "\n",
        "# Clean special characters and excessive spaces\n",
        "transcript = re.sub(r'[^\\x00-\\x7F]+', ' ', transcript)  # Remove non-ASCII characters\n",
        "transcript = re.sub(r'\\s+', ' ', transcript).strip()     # Replace multiple spaces with a single space\n",
        "\n",
        "# Split transcript into sentences\n",
        "sentences = sent_tokenize(transcript)\n",
        "\n",
        "# Create chunks based on semantic similarity and concept shifts\n",
        "chunk_size = 300\n",
        "chunks = []\n",
        "current_chunk = []\n",
        "current_embedding = None\n",
        "\n",
        "for sentence in sentences:\n",
        "    # Embed the current sentence\n",
        "    sentence_embedding = embedder.encode(sentence, convert_to_tensor=True)\n",
        "\n",
        "    # Check if this is the first sentence\n",
        "    if current_embedding is None:\n",
        "        current_embedding = sentence_embedding\n",
        "        current_chunk.append(sentence)\n",
        "        continue\n",
        "\n",
        "    # Calculate similarity with the last chunk's embedding\n",
        "    similarity = util.pytorch_cos_sim(current_embedding, sentence_embedding).item()\n",
        "\n",
        "    # If similarity is low or chunk size exceeds limit, start a new chunk\n",
        "    if similarity < 0.6 or len(tokenizer(' '.join(current_chunk))['input_ids']) >= chunk_size:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "        current_chunk = [sentence]\n",
        "        current_embedding = sentence_embedding\n",
        "    else:\n",
        "        current_chunk.append(sentence)\n",
        "        current_embedding = (current_embedding + sentence_embedding) / 2  # Update embedding with average\n",
        "\n",
        "# Append remaining chunk if exists\n",
        "if current_chunk:\n",
        "    chunks.append(' '.join(current_chunk))\n",
        "\n",
        "# Generate headings and organize content\n",
        "segmented_content = \"\"\n",
        "for idx, chunk in enumerate(chunks, 1):\n",
        "    if not chunk.strip() or len(chunk.split()) < 5:\n",
        "        continue  # Skip empty or very short chunks\n",
        "\n",
        "    # Generate a heading using zero-shot classification for topic detection\n",
        "    topics = [\"Technology\", \"Education\", \"AI and Machine Learning\", \"Research\", \"Career Guidance\", \"Software Development\", \"Leadership\"]\n",
        "    classification = classifier(chunk, candidate_labels=topics, multi_label=False)\n",
        "    main_topic = classification['labels'][0]\n",
        "\n",
        "    # Generate a concise heading based on main topic\n",
        "    heading = summarizer(chunk, max_length=15, min_length=5, do_sample=False)[0]['summary_text']\n",
        "    segmented_content += f\"### {main_topic}: {heading.strip()}\\n\\n{chunk.strip()}\\n\\n\"\n",
        "\n",
        "# Save segmented content to a new file\n",
        "with open(output_file, 'w', encoding='utf-8') as file:\n",
        "    file.write(segmented_content)\n",
        "\n",
        "print(f\"Segmented transcript saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "s9bKEbYWQ99q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transcript With timestamp"
      ],
      "metadata": {
        "id": "tfP-d-gcwK76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import urllib.parse\n",
        "import requests\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from pytube import YouTube\n",
        "import speech_recognition as sr\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "def extract_video_id(video_url):\n",
        "    \"\"\"\n",
        "    Extracts the YouTube video ID from various URL formats.\n",
        "    \"\"\"\n",
        "    parsed_url = urllib.parse.urlparse(video_url)\n",
        "    query_params = urllib.parse.parse_qs(parsed_url.query)\n",
        "\n",
        "    if \"v\" in query_params:\n",
        "        return query_params[\"v\"][0]\n",
        "\n",
        "    match = re.search(r\"(youtu\\.be/|youtube\\.com/embed/|youtube\\.com/shorts/)([\\w-]+)\", video_url)\n",
        "    if match:\n",
        "        return match.group(2)\n",
        "\n",
        "    return None\n",
        "\n",
        "def download_audio(video_url):\n",
        "    \"\"\"\n",
        "    Downloads the audio using yt-dlp with cookies and returns the file path.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ydl_opts = {\n",
        "            'format': 'bestaudio/best',\n",
        "            'outtmpl': 'audio.%(ext)s',\n",
        "            'cookiefile': 'cookies (1).txt',  # Use the exported cookies\n",
        "            'postprocessors': [{\n",
        "                'key': 'FFmpegExtractAudio',\n",
        "                'preferredcodec': 'mp3',\n",
        "                'preferredquality': '192',\n",
        "            }],\n",
        "        }\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(video_url, download=True)\n",
        "            return \"audio.mp3\"\n",
        "    except Exception as e:\n",
        "        return f\"Error downloading audio: {str(e)}\"\n",
        "\n",
        "def convert_audio_to_wav(audio_file):\n",
        "    \"\"\"\n",
        "    Converts the downloaded MP3 audio to WAV format using pydub.\n",
        "    \"\"\"\n",
        "    wav_file = \"audio.wav\"\n",
        "    try:\n",
        "        AudioSegment.from_mp3(audio_file).export(wav_file, format=\"wav\")\n",
        "        return wav_file\n",
        "    except Exception as e:\n",
        "        return f\"Error converting to WAV: {str(e)}\"\n",
        "\n",
        "def transcribe_audio(audio_path, chunk_length=30):\n",
        "    \"\"\"\n",
        "    Splits audio into smaller chunks and transcribes each chunk separately.\n",
        "    Args:\n",
        "        audio_path (str): Path to the audio file.\n",
        "        chunk_length (int): Length of each chunk in seconds (default: 30).\n",
        "    Returns:\n",
        "        list: List of dictionaries containing transcribed text and timestamps.\n",
        "    \"\"\"\n",
        "    recognizer = sr.Recognizer()\n",
        "    audio = AudioSegment.from_wav(audio_path)\n",
        "    total_duration = len(audio) / 1000  # Convert to seconds\n",
        "    transcribed_segments = []\n",
        "\n",
        "    print(\"Transcribing audio in chunks...\")\n",
        "\n",
        "    # Split and transcribe audio in chunks\n",
        "    for start in range(0, int(total_duration), chunk_length):\n",
        "        end = min(start + chunk_length, int(total_duration))\n",
        "        chunk = audio[start * 1000:end * 1000]  # Extract chunk in milliseconds\n",
        "        chunk.export(\"chunk.wav\", format=\"wav\")  # Save chunk temporarily\n",
        "\n",
        "        with sr.AudioFile(\"chunk.wav\") as source:\n",
        "            try:\n",
        "                audio_data = recognizer.record(source)\n",
        "                text = recognizer.recognize_google(audio_data)\n",
        "                transcribed_segments.append({\n",
        "                    \"start\": start,\n",
        "                    \"end\": end,\n",
        "                    \"text\": text\n",
        "                })\n",
        "            except sr.UnknownValueError:\n",
        "                transcribed_segments.append({\n",
        "                    \"start\": start,\n",
        "                    \"end\": end,\n",
        "                    \"text\": \"[Unintelligible]\"\n",
        "                })\n",
        "            except sr.RequestError as e:\n",
        "                return f\"Error with the speech recognition service: {str(e)}\"\n",
        "\n",
        "    os.remove(\"chunk.wav\")  # Clean up temporary chunk file\n",
        "    return transcribed_segments\n",
        "\n",
        "def get_transcript_unlisted(video_url):\n",
        "    \"\"\"\n",
        "    Tries to fetch the transcript using youtube_transcript_api first,\n",
        "    then falls back to downloading and transcribing audio if necessary.\n",
        "    \"\"\"\n",
        "    video_id = extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "        return \"Invalid YouTube URL.\"\n",
        "\n",
        "    # Try to fetch transcript using youtube_transcript_api\n",
        "    try:\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        # Add 'end' time to each segment\n",
        "        for segment in transcript:\n",
        "            segment[\"end\"] = segment[\"start\"] + segment[\"duration\"]\n",
        "        return transcript  # Return transcript with timestamps\n",
        "    except:\n",
        "        print(\"Transcript not available via API, attempting audio transcription...\")\n",
        "\n",
        "    # Download and transcribe audio if no transcript is available\n",
        "    audio_file = download_audio(video_url)\n",
        "    if \"Error\" in audio_file:\n",
        "        return audio_file\n",
        "\n",
        "    wav_file = convert_audio_to_wav(audio_file)\n",
        "    if \"Error\" in wav_file:\n",
        "        return wav_file\n",
        "\n",
        "    transcription = transcribe_audio(wav_file)\n",
        "\n",
        "    # Cleanup temporary files\n",
        "    os.remove(audio_file)\n",
        "    os.remove(wav_file)\n",
        "\n",
        "    return transcription\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    video_url = input(\"Enter the YouTube video URL: \")\n",
        "    transcript = get_transcript_unlisted(video_url)\n",
        "\n",
        "    if isinstance(transcript, list):\n",
        "        print(\"\\nTranscript with Timestamps:\")\n",
        "        for segment in transcript:\n",
        "            print(f\"{segment['start']} - {segment['end']}: {segment['text']}\")\n",
        "    else:\n",
        "        print(\"\\nTranscript:\\n\", transcript)"
      ],
      "metadata": {
        "id": "i41CzNx0G2oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install youtube_transcript_api"
      ],
      "metadata": {
        "id": "3YA6rC0MIj4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install yt-dlp"
      ],
      "metadata": {
        "id": "4gSlKnHkInxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytube"
      ],
      "metadata": {
        "id": "TG0dyPecJHDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install SpeechRecognition"
      ],
      "metadata": {
        "id": "UMugkZUdKJgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pydub"
      ],
      "metadata": {
        "id": "u7qAE_uPMljM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import urllib.parse\n",
        "import requests\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from pytube import YouTube\n",
        "import speech_recognition as sr\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "def extract_video_id(video_url):\n",
        "    \"\"\"\n",
        "    Extracts the YouTube video ID from various URL formats.\n",
        "    \"\"\"\n",
        "    parsed_url = urllib.parse.urlparse(video_url)\n",
        "    query_params = urllib.parse.parse_qs(parsed_url.query)\n",
        "\n",
        "    if \"v\" in query_params:\n",
        "        return query_params[\"v\"][0]\n",
        "\n",
        "    match = re.search(r\"(youtu\\.be/|youtube\\.com/embed/|youtube\\.com/shorts/)([\\w-]+)\", video_url)\n",
        "    if match:\n",
        "        return match.group(2)\n",
        "\n",
        "    return None\n",
        "\n",
        "def download_audio(video_url):\n",
        "    \"\"\"\n",
        "    Downloads the audio using yt-dlp with cookies and returns the file path.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ydl_opts = {\n",
        "            'format': 'bestaudio/best',\n",
        "            'outtmpl': 'audio.%(ext)s',\n",
        "            'cookiefile': 'cookies (1).txt',  # Use the exported cookies\n",
        "            'postprocessors': [{\n",
        "                'key': 'FFmpegExtractAudio',\n",
        "                'preferredcodec': 'mp3',\n",
        "                'preferredquality': '192',\n",
        "            }],\n",
        "        }\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(video_url, download=True)\n",
        "            return \"audio.mp3\"\n",
        "    except Exception as e:\n",
        "        return f\"Error downloading audio: {str(e)}\"\n",
        "\n",
        "def convert_audio_to_wav(audio_file):\n",
        "    \"\"\"\n",
        "    Converts the downloaded MP3 audio to WAV format using pydub.\n",
        "    \"\"\"\n",
        "    wav_file = \"audio.wav\"\n",
        "    try:\n",
        "        AudioSegment.from_mp3(audio_file).export(wav_file, format=\"wav\")\n",
        "        return wav_file\n",
        "    except Exception as e:\n",
        "        return f\"Error converting to WAV: {str(e)}\"\n",
        "\n",
        "def transcribe_audio(audio_path, chunk_length=30):\n",
        "    \"\"\"\n",
        "    Splits audio into smaller chunks and transcribes each chunk separately.\n",
        "    Args:\n",
        "        audio_path (str): Path to the audio file.\n",
        "        chunk_length (int): Length of each chunk in seconds (default: 30).\n",
        "    Returns:\n",
        "        list: List of dictionaries containing transcribed text and timestamps.\n",
        "    \"\"\"\n",
        "    recognizer = sr.Recognizer()\n",
        "    audio = AudioSegment.from_wav(audio_path)\n",
        "    total_duration = len(audio) / 1000  # Convert to seconds\n",
        "    transcribed_segments = []\n",
        "\n",
        "    print(\"Transcribing audio in chunks...\")\n",
        "\n",
        "    # Split and transcribe audio in chunks\n",
        "    for start in range(0, int(total_duration), chunk_length):\n",
        "        end = min(start + chunk_length, int(total_duration))\n",
        "        chunk = audio[start * 1000:end * 1000]  # Extract chunk in milliseconds\n",
        "        chunk.export(\"chunk.wav\", format=\"wav\")  # Save chunk temporarily\n",
        "\n",
        "        with sr.AudioFile(\"chunk.wav\") as source:\n",
        "            try:\n",
        "                audio_data = recognizer.record(source)\n",
        "                text = recognizer.recognize_google(audio_data)\n",
        "                transcribed_segments.append({\n",
        "                    \"start\": start,\n",
        "                    \"end\": end,\n",
        "                    \"text\": text\n",
        "                })\n",
        "            except sr.UnknownValueError:\n",
        "                transcribed_segments.append({\n",
        "                    \"start\": start,\n",
        "                    \"end\": end,\n",
        "                    \"text\": \"[Unintelligible]\"\n",
        "                })\n",
        "            except sr.RequestError as e:\n",
        "                return f\"Error with the speech recognition service: {str(e)}\"\n",
        "\n",
        "    os.remove(\"chunk.wav\")  # Clean up temporary chunk file\n",
        "    return transcribed_segments\n",
        "\n",
        "def get_transcript_unlisted(video_url):\n",
        "    \"\"\"\n",
        "    Tries to fetch the transcript using youtube_transcript_api first,\n",
        "    then falls back to downloading and transcribing audio if necessary.\n",
        "    \"\"\"\n",
        "    video_id = extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "        return \"Invalid YouTube URL.\"\n",
        "\n",
        "    # Try to fetch transcript using youtube_transcript_api\n",
        "    try:\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        # Add 'end' time to each segment\n",
        "        for segment in transcript:\n",
        "            segment[\"end\"] = segment[\"start\"] + segment[\"duration\"]\n",
        "        return transcript  # Return transcript with timestamps\n",
        "    except:\n",
        "        print(\"Transcript not available via API, attempting audio transcription...\")\n",
        "\n",
        "    # Download and transcribe audio if no transcript is available\n",
        "    audio_file = download_audio(video_url)\n",
        "    if \"Error\" in audio_file:\n",
        "        return audio_file\n",
        "\n",
        "    wav_file = convert_audio_to_wav(audio_file)\n",
        "    if \"Error\" in wav_file:\n",
        "        return wav_file\n",
        "\n",
        "    transcription = transcribe_audio(wav_file)\n",
        "\n",
        "    # Cleanup temporary files\n",
        "    os.remove(audio_file)\n",
        "    os.remove(wav_file)\n",
        "\n",
        "    return transcription\n",
        "\n",
        "def save_transcript_to_file(transcript, filename=\"transcript.txt\"):\n",
        "    \"\"\"\n",
        "    Saves the transcript to a text file.\n",
        "    Args:\n",
        "        transcript (list or str): The transcript to save.\n",
        "        filename (str): The name of the output file.\n",
        "    \"\"\"\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
        "        if isinstance(transcript, list):\n",
        "            for segment in transcript:\n",
        "                file.write(f\"{segment['start']} - {segment['end']}: {segment['text']}\\n\")\n",
        "        else:\n",
        "            file.write(transcript)\n",
        "    print(f\"Transcript saved to {filename}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    video_url = input(\"Enter the YouTube video URL: \")\n",
        "    transcript = get_transcript_unlisted(video_url)\n",
        "\n",
        "    if isinstance(transcript, list):\n",
        "        print(\"\\nTranscript with Timestamps:\")\n",
        "        for segment in transcript:\n",
        "            print(f\"{segment['start']} - {segment['end']}: {segment['text']}\")\n",
        "    else:\n",
        "        print(\"\\nTranscript:\\n\", transcript)\n",
        "\n",
        "    # Save transcript to a text file\n",
        "    save_transcript_to_file(transcript, \"transcript.txt\")"
      ],
      "metadata": {
        "id": "oJRkMFi5XiQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.cluster import KMeans\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "def load_transcript_from_file(file_path):\n",
        "    \"\"\"\n",
        "    Loads the transcript from a file.\n",
        "    Args:\n",
        "        file_path (str): Path to the transcript file.\n",
        "    Returns:\n",
        "        list: List of dictionaries with 'start', 'end', and 'text' keys.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            if file_path.endswith(\".json\"):\n",
        "                transcript = json.load(file)\n",
        "            else:\n",
        "                # For plain text files, assume each line is in the format: [start] - [end]: [text]\n",
        "                transcript = []\n",
        "                for line in file:\n",
        "                    match = re.match(r\"(\\d+\\.\\d+) - (\\d+\\.\\d+): (.+)\", line.strip())\n",
        "                    if match:\n",
        "                        start, end, text = match.groups()\n",
        "                        transcript.append({\n",
        "                            \"start\": float(start),\n",
        "                            \"end\": float(end),\n",
        "                            \"text\": text\n",
        "                        })\n",
        "            return transcript\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading transcript: {e}\")\n",
        "        return None\n",
        "\n",
        "def segment_transcript_semantically(transcript, num_clusters=5):\n",
        "    \"\"\"\n",
        "    Segments the transcript into semantically meaningful clusters.\n",
        "    Args:\n",
        "        transcript (list): List of dictionaries with 'start', 'end', and 'text' keys.\n",
        "        num_clusters (int): Number of clusters to create (default: 5).\n",
        "    Returns:\n",
        "        list: List of dictionaries with 'start', 'end', and 'text' for each segment.\n",
        "    \"\"\"\n",
        "    # Extract sentences and their timestamps\n",
        "    sentences = [segment[\"text\"] for segment in transcript]\n",
        "    timestamps = [(segment[\"start\"], segment[\"end\"]) for segment in transcript]\n",
        "\n",
        "    # Load a pre-trained sentence embedding model\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "    # Perform K-Means clustering\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    clusters = kmeans.fit_predict(sentence_embeddings)\n",
        "\n",
        "    # Group sentences by cluster\n",
        "    clustered_segments = {}\n",
        "    for i, cluster in enumerate(clusters):\n",
        "        if cluster not in clustered_segments:\n",
        "            clustered_segments[cluster] = []\n",
        "        clustered_segments[cluster].append({\n",
        "            \"start\": timestamps[i][0],\n",
        "            \"end\": timestamps[i][1],\n",
        "            \"text\": sentences[i]\n",
        "        })\n",
        "\n",
        "    # Merge segments in each cluster\n",
        "    segmented_transcript = []\n",
        "    for cluster, segments in clustered_segments.items():\n",
        "        start_time = segments[0][\"start\"]\n",
        "        end_time = segments[-1][\"end\"]\n",
        "        combined_text = \" \".join([segment[\"text\"] for segment in segments])\n",
        "        segmented_transcript.append({\n",
        "            \"start\": start_time,\n",
        "            \"end\": end_time,\n",
        "            \"text\": combined_text\n",
        "        })\n",
        "\n",
        "    # Sort segments by start time\n",
        "    segmented_transcript.sort(key=lambda x: x[\"start\"])\n",
        "\n",
        "    return segmented_transcript\n",
        "\n",
        "def save_segmented_transcript(segmented_transcript, output_path=\"segmented_transcript.txt\"):\n",
        "    \"\"\"\n",
        "    Saves the segmented transcript to a text file.\n",
        "    Args:\n",
        "        segmented_transcript (list): List of dictionaries with 'start', 'end', and 'text'.\n",
        "        output_path (str): The path to save the output file.\n",
        "    \"\"\"\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        for segment in segmented_transcript:\n",
        "            file.write(f\"{segment['start']} - {segment['end']}: {segment['text']}\\n\")\n",
        "    print(f\"Segmented transcript saved to {output_path}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Ask the user for the transcript file path\n",
        "    transcript_path = input(\"Enter the path to the transcript file: \")\n",
        "    output_path = input(\"Enter the path to save the segmented transcript (default: segmented_transcript.txt): \") or \"segmented_transcript.txt\"\n",
        "\n",
        "    # Load the transcript from the file\n",
        "    transcript = load_transcript_from_file(transcript_path)\n",
        "    if not transcript:\n",
        "        print(\"Failed to load transcript. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # Segment the transcript semantically\n",
        "    segmented_transcript = segment_transcript_semantically(transcript, num_clusters=3)\n",
        "\n",
        "    # Print the segmented transcript\n",
        "    print(\"\\nSegmented Transcript:\")\n",
        "    for segment in segmented_transcript:\n",
        "        print(f\"{segment['start']} - {segment['end']}: {segment['text']}\")\n",
        "\n",
        "    # Save the segmented transcript to a file\n",
        "    save_segmented_transcript(segmented_transcript, output_path)"
      ],
      "metadata": {
        "id": "yNe5JE1XbjAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def load_transcript_from_file(file_path):\n",
        "    \"\"\"\n",
        "    Loads the transcript from a file.\n",
        "    Args:\n",
        "        file_path (str): Path to the transcript file.\n",
        "    Returns:\n",
        "        list: List of dictionaries with 'start', 'end', and 'text' keys.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            if file_path.endswith(\".json\"):\n",
        "                transcript = json.load(file)\n",
        "            else:\n",
        "                # For plain text files, assume each line is in the format: [start] - [end]: [text]\n",
        "                transcript = []\n",
        "                for line in file:\n",
        "                    match = re.match(r\"(\\d+\\.\\d+) - (\\d+\\.\\d+): (.+)\", line.strip())\n",
        "                    if match:\n",
        "                        start, end, text = match.groups()\n",
        "                        transcript.append({\n",
        "                            \"start\": float(start),\n",
        "                            \"end\": float(end),\n",
        "                            \"text\": text\n",
        "                        })\n",
        "            return transcript\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading transcript: {e}\")\n",
        "        return None\n",
        "\n",
        "def segment_transcript_with_topics(transcript, num_topics=5):\n",
        "    \"\"\"\n",
        "    Segments the transcript into topics using BERTopic.\n",
        "    Args:\n",
        "        transcript (list): List of dictionaries with 'start', 'end', and 'text' keys.\n",
        "        num_topics (int): Number of topics to create (default: 5).\n",
        "    Returns:\n",
        "        list: List of dictionaries with 'start', 'end', 'text', and 'topic' for each segment.\n",
        "    \"\"\"\n",
        "    # Extract sentences and their timestamps\n",
        "    sentences = [segment[\"text\"] for segment in transcript]\n",
        "    timestamps = [(segment[\"start\"], segment[\"end\"]) for segment in transcript]\n",
        "\n",
        "    # Load a pre-trained sentence embedding model\n",
        "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    # Initialize BERTopic\n",
        "    vectorizer_model = CountVectorizer(stop_words=\"english\")  # Remove stopwords for better topic modeling\n",
        "    topic_model = BERTopic(embedding_model=embedding_model, vectorizer_model=vectorizer_model, nr_topics=num_topics)\n",
        "\n",
        "    # Fit the model and transform the sentences\n",
        "    topics, _ = topic_model.fit_transform(sentences)\n",
        "\n",
        "    # Assign topics to each sentence\n",
        "    segmented_transcript = []\n",
        "    for i, (start, end) in enumerate(timestamps):\n",
        "        segmented_transcript.append({\n",
        "            \"start\": start,\n",
        "            \"end\": end,\n",
        "            \"text\": sentences[i],\n",
        "            \"topic\": int(topics[i])\n",
        "        })\n",
        "\n",
        "    return segmented_transcript, topic_model\n",
        "\n",
        "def save_segmented_transcript(segmented_transcript, topic_model, output_path=\"segmented_transcript.txt\"):\n",
        "    \"\"\"\n",
        "    Saves the segmented transcript with topics to a text file.\n",
        "    Args:\n",
        "        segmented_transcript (list): List of dictionaries with 'start', 'end', 'text', and 'topic'.\n",
        "        topic_model (BERTopic): The trained BERTopic model.\n",
        "        output_path (str): The path to save the output file.\n",
        "    \"\"\"\n",
        "    # Get topic labels\n",
        "    topic_info = topic_model.get_topic_info()\n",
        "    topic_labels = {row[\"Topic\"]: row[\"Name\"] for _, row in topic_info.iterrows()}\n",
        "\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        for segment in segmented_transcript:\n",
        "            topic_label = topic_labels.get(segment[\"topic\"], \"Unknown Topic\")\n",
        "            file.write(f\"{segment['start']} - {segment['end']} [{topic_label}]: {segment['text']}\\n\")\n",
        "    print(f\"Segmented transcript saved to {output_path}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Ask the user for the transcript file path\n",
        "    transcript_path = input(\"Enter the path to the transcript file: \")\n",
        "    output_path = input(\"Enter the path to save the segmented transcript (default: segmented_transcript.txt): \") or \"segmented_transcript.txt\"\n",
        "\n",
        "    # Load the transcript from the file\n",
        "    transcript = load_transcript_from_file(transcript_path)\n",
        "    if not transcript:\n",
        "        print(\"Failed to load transcript. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # Segment the transcript into topics\n",
        "    segmented_transcript, topic_model = segment_transcript_with_topics(transcript, num_topics=5)\n",
        "\n",
        "    # Print the segmented transcript\n",
        "    print(\"\\nSegmented Transcript with Topics:\")\n",
        "    for segment in segmented_transcript:\n",
        "        print(f\"{segment['start']} - {segment['end']} [Topic {segment['topic']}]: {segment['text']}\")\n",
        "\n",
        "    # Save the segmented transcript to a file\n",
        "    save_segmented_transcript(segmented_transcript, topic_model, output_path)"
      ],
      "metadata": {
        "id": "7hiMiOGAelK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bertopic"
      ],
      "metadata": {
        "id": "qeVDyDMKho46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "import re\n",
        "\n",
        "# Load pre-trained model for sentence embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Function to extract sentences and starting timestamps\n",
        "def extract_sentences_and_timestamps(file_path):\n",
        "    sentences = []\n",
        "    timestamps = []\n",
        "    # Enhanced regex pattern for range-based timestamps\n",
        "    pattern = re.compile(r\"(\\d{1,2}(?:\\.\\d+)?)\\s*-\\s*(\\d{1,2}(?:\\.\\d+)?):\\s*(.*)\")\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    match = pattern.match(line)\n",
        "                    if match:\n",
        "                        start_time = float(match.group(1))\n",
        "                        # Convert start time to hh:mm:ss format\n",
        "                        hours = int(start_time // 3600)\n",
        "                        minutes = int((start_time % 3600) // 60)\n",
        "                        seconds = start_time % 60\n",
        "                        timestamp = f\"{hours:02}:{minutes:02}:{seconds:05.2f}\"\n",
        "                        timestamps.append(timestamp)\n",
        "                        sentences.append(match.group(3).strip())\n",
        "                    else:\n",
        "                        print(f\" Unmatched line format: {line}\")  # Debug print for unmatched lines\n",
        "    except FileNotFoundError:\n",
        "        print(f\" Error: The file '{file_path}' was not found.\")\n",
        "        return [], []\n",
        "\n",
        "    if not sentences:\n",
        "        print(\" Warning: No sentences found. Check if the file is empty or if the format is incorrect.\")\n",
        "    return sentences, timestamps\n",
        "\n",
        "# Extract sentences and timestamps\n",
        "input_file = 'transcript.txt'\n",
        "sentences, timestamps = extract_sentences_and_timestamps(input_file)\n",
        "\n",
        "# Debug: Print extracted sentences and timestamps\n",
        "print(\"\\nExtracted Sentences:\", sentences)\n",
        "print(\"Extracted Timestamps:\", timestamps)\n",
        "\n",
        "# Check if sentences are empty\n",
        "if not sentences:\n",
        "    print(\" Error: No valid sentences were extracted. Exiting the script.\")\n",
        "else:\n",
        "    # Generate sentence embeddings\n",
        "    embeddings = model.encode(sentences)\n",
        "\n",
        "    # Debug: Check if embeddings are generated correctly\n",
        "    if embeddings.size == 0:\n",
        "        print(\" Error: No embeddings were generated. Check if the input text is valid.\")\n",
        "    else:\n",
        "        # Determine number of clusters (segments)\n",
        "        num_clusters = 3  # You can adjust this based on your transcript length\n",
        "        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
        "        labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "        # Create segmented output with timestamps\n",
        "        segments = [[] for _ in range(num_clusters)]\n",
        "        for i, label in enumerate(labels):\n",
        "            segments[label].append(f\"[{timestamps[i]}] {sentences[i]}\")\n",
        "\n",
        "        # Save segmented output to a new file\n",
        "        output_file = 'segmented_transcript.txt'\n",
        "        with open(output_file, 'w') as file:\n",
        "            for idx, segment in enumerate(segments):\n",
        "                file.write(f\"--- Segment {idx + 1} ---\\n\")\n",
        "                file.write(\"\\n\".join(segment))\n",
        "                file.write(\"\\n\\n\")\n",
        "\n",
        "        print(f\" Segmented transcript saved to {output_file}\")"
      ],
      "metadata": {
        "id": "pM_LXBJDhvQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "import re\n",
        "\n",
        "# Load pre-trained model for sentence embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Function to extract sentences and starting timestamps\n",
        "def extract_sentences_and_timestamps(file_path):\n",
        "    sentences = []\n",
        "    timestamps = []\n",
        "    # Enhanced regex pattern for range-based timestamps\n",
        "    pattern = re.compile(r\"(\\d{1,2}(?:\\.\\d+)?)\\s*-\\s*(\\d{1,2}(?:\\.\\d+)?):\\s*(.*)\")\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    match = pattern.match(line)\n",
        "                    if match:\n",
        "                        start_time = float(match.group(1))\n",
        "                        # Convert start time to hh:mm:ss format\n",
        "                        hours = int(start_time // 3600)\n",
        "                        minutes = int((start_time % 3600) // 60)\n",
        "                        seconds = start_time % 60\n",
        "                        timestamp = f\"{hours:02}:{minutes:02}:{seconds:05.2f}\"\n",
        "                        timestamps.append(timestamp)\n",
        "                        sentences.append(match.group(3).strip())\n",
        "                    else:\n",
        "                        print(f\" Unmatched line format: {line}\")  # Debug print for unmatched lines\n",
        "    except FileNotFoundError:\n",
        "        print(f\" Error: The file '{file_path}' was not found.\")\n",
        "        return [], []\n",
        "\n",
        "    if not sentences:\n",
        "        print(\" Warning: No sentences found. Check if the file is empty or if the format is incorrect.\")\n",
        "    return sentences, timestamps\n",
        "\n",
        "# Extract sentences and timestamps\n",
        "input_file = 'transcript.txt'\n",
        "sentences, timestamps = extract_sentences_and_timestamps(input_file)\n",
        "\n",
        "# Debug: Print extracted sentences and timestamps\n",
        "print(\"\\nExtracted Sentences:\", sentences)\n",
        "print(\"Extracted Timestamps:\", timestamps)\n",
        "\n",
        "# Check if sentences are empty\n",
        "if not sentences:\n",
        "    print(\" Error: No valid sentences were extracted. Exiting the script.\")\n",
        "else:\n",
        "    # Generate sentence embeddings\n",
        "    embeddings = model.encode(sentences)\n",
        "\n",
        "    # Debug: Check if embeddings are generated correctly\n",
        "    if embeddings.size == 0:\n",
        "        print(\" Error: No embeddings were generated. Check if the input text is valid.\")\n",
        "    else:\n",
        "        # Determine number of clusters based on sentence count\n",
        "        # Here, we aim for topic-based segmentation, so we adjust clusters based on transcript length\n",
        "        num_clusters = max(1, len(sentences) // 5)  # Example: 1 cluster per 5 sentences\n",
        "        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
        "        labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "        # Create segmented output with timestamps\n",
        "        segments = [[] for _ in range(num_clusters)]\n",
        "        for i, label in enumerate(labels):\n",
        "            segments[label].append(f\"[{timestamps[i]}] {sentences[i]}\")\n",
        "\n",
        "        # Save segmented output to a new file\n",
        "        output_file = 'segmented_transcript.txt'\n",
        "        with open(output_file, 'w') as file:\n",
        "            for idx, segment in enumerate(segments):\n",
        "                file.write(f\"--- Topic Segment {idx + 1} ---\\n\")\n",
        "                file.write(\"\\n\".join(segment))\n",
        "                file.write(\"\\n\\n\")\n",
        "\n",
        "        print(f\" Topic-segmented transcript saved to {output_file}\")"
      ],
      "metadata": {
        "id": "PjgM8hftw1jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "import re\n",
        "\n",
        "# Load pre-trained model for sentence embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Function to extract sentences and starting timestamps\n",
        "def extract_sentences_and_timestamps(file_path):\n",
        "    sentences = []\n",
        "    timestamps = []\n",
        "    # Enhanced regex pattern for range-based timestamps\n",
        "    pattern = re.compile(r\"(\\d{1,2}(?:\\.\\d+)?)\\s*-\\s*(\\d{1,2}(?:\\.\\d+)?):\\s*(.*)\")\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    match = pattern.match(line)\n",
        "                    if match:\n",
        "                        start_time = float(match.group(1))\n",
        "                        # Convert start time to hh:mm:ss format\n",
        "                        hours = int(start_time // 3600)\n",
        "                        minutes = int((start_time % 3600) // 60)\n",
        "                        seconds = start_time % 60\n",
        "                        timestamp = f\"{hours:02}:{minutes:02}:{seconds:05.2f}\"\n",
        "                        timestamps.append(timestamp)\n",
        "                        sentences.append(match.group(3).strip())\n",
        "                    else:\n",
        "                        print(f\" Unmatched line format: {line}\")  # Debug print for unmatched lines\n",
        "    except FileNotFoundError:\n",
        "        print(f\" Error: The file '{file_path}' was not found.\")\n",
        "        return [], []\n",
        "\n",
        "    if not sentences:\n",
        "        print(\" Warning: No sentences found. Check if the file is empty or if the format is incorrect.\")\n",
        "    return sentences, timestamps\n",
        "\n",
        "# Extract sentences and timestamps\n",
        "input_file = 'transcript.txt'\n",
        "sentences, timestamps = extract_sentences_and_timestamps(input_file)\n",
        "\n",
        "# Debug: Print extracted sentences and timestamps\n",
        "print(\"\\nExtracted Sentences:\", sentences)\n",
        "print(\"Extracted Timestamps:\", timestamps)\n",
        "\n",
        "# Check if sentences are empty\n",
        "if not sentences:\n",
        "    print(\" Error: No valid sentences were extracted. Exiting the script.\")\n",
        "else:\n",
        "    # Generate sentence embeddings\n",
        "    embeddings = model.encode(sentences)\n",
        "\n",
        "    # Debug: Check if embeddings are generated correctly\n",
        "    if embeddings.size == 0:\n",
        "        print(\" Error: No embeddings were generated. Check if the input text is valid.\")\n",
        "    else:\n",
        "        # Determine number of clusters dynamically based on conceptual shifts\n",
        "        num_clusters = max(1, len(sentences) // 5)  # Example: 1 cluster per 5 sentences\n",
        "        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
        "        labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "        # Create segmented output with timestamps\n",
        "        segments = [[] for _ in range(num_clusters)]\n",
        "        for i, label in enumerate(labels):\n",
        "            segments[label].append(f\"[{timestamps[i]}] {sentences[i]}\")\n",
        "\n",
        "        # Save segmented output to a new file\n",
        "        output_file = 'segmented_transcript.txt'\n",
        "        with open(output_file, 'w') as file:\n",
        "            for idx, segment in enumerate(segments):\n",
        "                file.write(f\"--- Conceptually Relevant Segment {idx + 1} ---\\n\")\n",
        "                file.write(\"\\n\".join(segment))\n",
        "                file.write(\"\\n\\n\")\n",
        "\n",
        "        print(f\" Conceptually relevant segmented transcript saved to {output_file}\")"
      ],
      "metadata": {
        "id": "vNPgNmsSyJ5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "Eb8EYMQDzAMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install texttiling_segmenter.py"
      ],
      "metadata": {
        "id": "eUuKprkL2LbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.tokenize.texttiling import TextTilingTokenizer\n",
        "from google.colab import files  # For file upload\n",
        "\n",
        "# Ensure you have the necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the text by tokenizing, lowercasing, and removing stopwords.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "    words = [[word for word in sentence if word.isalnum() and word not in stop_words] for sentence in words]\n",
        "    return sentences, words\n",
        "\n",
        "def segment_transcript(file_path):\n",
        "    \"\"\"\n",
        "    Read a transcript from a file, preprocess it, and apply TextTiling to segment it.\n",
        "    \"\"\"\n",
        "    # Read the file\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        transcript = file.read()\n",
        "\n",
        "    # Preprocess the text\n",
        "    sentences, words = preprocess_text(transcript)\n",
        "\n",
        "    # Apply TextTiling\n",
        "    tt = TextTilingTokenizer()\n",
        "    segments = tt.tokenize(transcript)\n",
        "\n",
        "    # Print the segments\n",
        "    for i, segment in enumerate(segments):\n",
        "        print(f\"Segment {i+1}:\\n{segment}\\n\")\n",
        "\n",
        "# Upload the file to Colab\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the file name\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# Segment the transcript\n",
        "segment_transcript(file_name)"
      ],
      "metadata": {
        "id": "ON0i1umu2O29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "jVFE3ibb23vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.tokenize.texttiling import TextTilingTokenizer\n",
        "from google.colab import files\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the text by tokenizing, lowercasing, and removing stopwords.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "    words = [[word for word in sentence if word.isalnum() and word not in stop_words] for sentence in words]\n",
        "    return sentences, words\n",
        "\n",
        "def insert_paragraph_breaks(text, sentences_per_paragraph=5):\n",
        "    \"\"\"\n",
        "    Insert paragraph breaks after every `sentences_per_paragraph` sentences.\n",
        "    \"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    paragraphs = []\n",
        "    for i in range(0, len(sentences), sentences_per_paragraph):\n",
        "        paragraph = \" \".join(sentences[i:i + sentences_per_paragraph])\n",
        "        paragraphs.append(paragraph)\n",
        "    return \"\\n\\n\".join(paragraphs)\n",
        "\n",
        "def segment_transcript(file_path):\n",
        "    \"\"\"\n",
        "    Read a transcript from a file, preprocess it, and apply TextTiling to segment it.\n",
        "    \"\"\"\n",
        "    # Read the file\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        transcript = file.read()\n",
        "\n",
        "    # Insert paragraph breaks\n",
        "    transcript_with_breaks = insert_paragraph_breaks(transcript)\n",
        "\n",
        "    # Preprocess the text\n",
        "    sentences, words = preprocess_text(transcript_with_breaks)\n",
        "\n",
        "    # Apply TextTiling\n",
        "    tt = TextTilingTokenizer()\n",
        "    segments = tt.tokenize(transcript_with_breaks)\n",
        "\n",
        "    # Print the segments\n",
        "    for i, segment in enumerate(segments):\n",
        "        print(f\"Segment {i+1}:\\n{segment}\\n\")\n",
        "\n",
        "# Upload the file to Colab\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the file name\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# Segment the transcript\n",
        "segment_transcript(file_name)"
      ],
      "metadata": {
        "id": "Xb2_5jrC3g_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the text by splitting it into sentences and cleaning.\n",
        "    \"\"\"\n",
        "    # Split into sentences\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    # Remove short sentences (optional)\n",
        "    sentences = [s.strip() for s in sentences if len(s.split()) > 5]\n",
        "    return sentences\n",
        "\n",
        "def identify_topics(sentences, num_topics=5):\n",
        "    \"\"\"\n",
        "    Identify topics in the text using Latent Dirichlet Allocation (LDA).\n",
        "    \"\"\"\n",
        "    # Create a document-term matrix\n",
        "    vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=2)\n",
        "    doc_term_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    # Fit LDA model\n",
        "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
        "    lda.fit(doc_term_matrix)\n",
        "\n",
        "    # Get topic distribution for each sentence\n",
        "    topic_distribution = lda.transform(doc_term_matrix)\n",
        "    topic_labels = np.argmax(topic_distribution, axis=1)\n",
        "\n",
        "    # Get top words for each topic\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    topics = []\n",
        "    for topic_idx, topic in enumerate(lda.components_):\n",
        "        top_words = [feature_names[i] for i in topic.argsort()[-5:]]\n",
        "        topics.append(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
        "\n",
        "    return topic_labels, topics\n",
        "\n",
        "def segment_transcript(input_file, output_file, num_topics=5):\n",
        "    \"\"\"\n",
        "    Read a transcript from a text file, analyze it, and generate segmented output.\n",
        "    \"\"\"\n",
        "    # Read the input file\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        transcript = file.read()\n",
        "\n",
        "    # Preprocess the text\n",
        "    sentences = preprocess_text(transcript)\n",
        "\n",
        "    # Identify topics\n",
        "    topic_labels, topics = identify_topics(sentences, num_topics)\n",
        "\n",
        "    # Group sentences by topic\n",
        "    segments = {}\n",
        "    for i, label in enumerate(topic_labels):\n",
        "        if label not in segments:\n",
        "            segments[label] = []\n",
        "        segments[label].append(sentences[i])\n",
        "\n",
        "    # Write the segmented output to a file\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        for label, segment_sentences in segments.items():\n",
        "            file.write(f\"Segment {label + 1}: {topics[label]}\\n\")\n",
        "            file.write(\"\\n\".join(segment_sentences) + \"\\n\\n\")\n",
        "\n",
        "# Input and output file paths\n",
        "input_file = \"/content/Kalsi sir transcript.txt\"  # Replace with your input file path\n",
        "output_file = \"segmented_transcript.txt\"  # Output file path\n",
        "\n",
        "# Segment the transcript\n",
        "segment_transcript(input_file, output_file, num_topics=5)\n",
        "\n",
        "print(f\"Segmented transcript saved to {output_file}\")"
      ],
      "metadata": {
        "id": "RAljrw_U4QZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the text by splitting it into sentences and cleaning.\n",
        "    \"\"\"\n",
        "    # Split into sentences\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    # Remove short sentences (optional)\n",
        "    sentences = [s.strip() for s in sentences if len(s.split()) > 5]\n",
        "    return sentences\n",
        "\n",
        "def identify_topics(sentences, num_topics=5):\n",
        "    \"\"\"\n",
        "    Identify topics in the text using Latent Dirichlet Allocation (LDA).\n",
        "    \"\"\"\n",
        "    # Create a document-term matrix\n",
        "    vectorizer = CountVectorizer(stop_words='english', max_df=1.0, min_df=1)\n",
        "    doc_term_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    # Fit LDA model\n",
        "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
        "    lda.fit(doc_term_matrix)\n",
        "\n",
        "    # Get topic distribution for each sentence\n",
        "    topic_distribution = lda.transform(doc_term_matrix)\n",
        "    topic_labels = np.argmax(topic_distribution, axis=1)\n",
        "\n",
        "    # Get top words for each topic\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    topics = []\n",
        "    for topic_idx, topic in enumerate(lda.components_):\n",
        "        top_words = [feature_names[i] for i in topic.argsort()[-5:]]\n",
        "        topics.append(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
        "\n",
        "    return topic_labels, topics\n",
        "\n",
        "def segment_transcript(input_file, output_file, num_topics=5):\n",
        "    \"\"\"\n",
        "    Read a transcript from a text file, analyze it, and generate segmented output.\n",
        "    \"\"\"\n",
        "    # Read the input file\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        transcript = file.read()\n",
        "\n",
        "    # Preprocess the text\n",
        "    sentences = preprocess_text(transcript)\n",
        "\n",
        "    # Check if there are enough sentences for topic modeling\n",
        "    if len(sentences) < num_topics:\n",
        "        raise ValueError(f\"Not enough sentences ({len(sentences)}) for {num_topics} topics.\")\n",
        "\n",
        "    # Identify topics\n",
        "    topic_labels, topics = identify_topics(sentences, num_topics)\n",
        "\n",
        "    # Group sentences by topic\n",
        "    segments = {}\n",
        "    for i, label in enumerate(topic_labels):\n",
        "        if label not in segments:\n",
        "            segments[label] = []\n",
        "        segments[label].append(sentences[i])\n",
        "\n",
        "    # Write the segmented output to a file\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        for label, segment_sentences in segments.items():\n",
        "            file.write(f\"Segment {label + 1}: {topics[label]}\\n\")\n",
        "            file.write(\"\\n\".join(segment_sentences) + \"\\n\\n\")\n",
        "\n",
        "# Input and output file paths\n",
        "input_file = \"/content/Kalsi sir transcript.txt\"  # Replace with your input file path\n",
        "output_file = \"segmented_transcript.txt\"  # Output file path\n",
        "\n",
        "# Segment the transcript\n",
        "segment_transcript(input_file, output_file, num_topics=5)\n",
        "\n",
        "print(f\"Segmented transcript saved to {output_file}\")"
      ],
      "metadata": {
        "id": "tWAK_8o17u4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the text by splitting it into sentences and cleaning.\n",
        "    \"\"\"\n",
        "    # Split into sentences\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    # Remove short sentences (optional)\n",
        "    sentences = [s.strip() for s in sentences if len(s.split()) > 5]\n",
        "    return sentences\n",
        "\n",
        "def identify_topics(sentences, num_topics=2):\n",
        "    \"\"\"\n",
        "    Identify topics in the text using Latent Dirichlet Allocation (LDA).\n",
        "    \"\"\"\n",
        "    # Create a document-term matrix\n",
        "    vectorizer = CountVectorizer(stop_words='english', max_df=1.0, min_df=1)\n",
        "    doc_term_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    # Fit LDA model\n",
        "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
        "    lda.fit(doc_term_matrix)\n",
        "\n",
        "    # Get topic distribution for each sentence\n",
        "    topic_distribution = lda.transform(doc_term_matrix)\n",
        "    topic_labels = np.argmax(topic_distribution, axis=1)\n",
        "\n",
        "    # Get top words for each topic\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    topics = []\n",
        "    for topic_idx, topic in enumerate(lda.components_):\n",
        "        top_words = [feature_names[i] for i in topic.argsort()[-5:]]\n",
        "        topics.append(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
        "\n",
        "    return topic_labels, topics\n",
        "\n",
        "def segment_transcript(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Read a transcript from a text file, analyze it, and generate segmented output.\n",
        "    \"\"\"\n",
        "    # Read the input file\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        transcript = file.read()\n",
        "\n",
        "    # Preprocess the text\n",
        "    sentences = preprocess_text(transcript)\n",
        "\n",
        "    # Dynamically adjust the number of topics\n",
        "    num_topics = min(len(sentences), 5)  # Use up to 5 topics or the number of sentences, whichever is smaller\n",
        "    if num_topics < 1:\n",
        "        raise ValueError(\"Not enough sentences for topic modeling.\")\n",
        "\n",
        "    print(f\"Using {num_topics} topics for segmentation.\")\n",
        "\n",
        "    # Identify topics\n",
        "    topic_labels, topics = identify_topics(sentences, num_topics)\n",
        "\n",
        "    # Group sentences by topic\n",
        "    segments = {}\n",
        "    for i, label in enumerate(topic_labels):\n",
        "        if label not in segments:\n",
        "            segments[label] = []\n",
        "        segments[label].append(sentences[i])\n",
        "\n",
        "    # Write the segmented output to a file\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        for label, segment_sentences in segments.items():\n",
        "            file.write(f\"Segment {label + 1}: {topics[label]}\\n\")\n",
        "            file.write(\"\\n\".join(segment_sentences) + \"\\n\\n\")\n",
        "\n",
        "# Input and output file paths\n",
        "input_file = \"/content/Kalsi sir transcript.txt\"  # Replace with your input file path\n",
        "output_file = \"segmented_transcript.txt\"  # Output file path\n",
        "\n",
        "# Segment the transcript\n",
        "segment_transcript(input_file, output_file)\n",
        "\n",
        "print(f\"Segmented transcript saved to {output_file}\")"
      ],
      "metadata": {
        "id": "pQVHTYDa8Jdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize.texttiling import TextTilingTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk import download\n",
        "\n",
        "# Download necessary NLTK data\n",
        "download('punkt')\n",
        "download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the text by removing stopwords and tokenizing into sentences.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    sentences = sent_tokenize(text)\n",
        "    cleaned_sentences = []\n",
        "    for sentence in sentences:\n",
        "        words = [word.lower() for word in re.findall(r'\\b\\w+\\b', sentence) if word.lower() not in stop_words]\n",
        "        cleaned_sentences.append(\" \".join(words))\n",
        "    return cleaned_sentences\n",
        "\n",
        "def segment_transcript(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Segment the transcript using the TextTiling algorithm.\n",
        "    \"\"\"\n",
        "    # Read the input file\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        transcript = file.read()\n",
        "\n",
        "    # Preprocess the text\n",
        "    sentences = preprocess_text(transcript)\n",
        "\n",
        "    # Combine sentences into a single string for TextTiling\n",
        "    text = \"\\n\\n\".join(sentences)\n",
        "\n",
        "    # Initialize TextTilingTokenizer\n",
        "    tt = TextTilingTokenizer()\n",
        "\n",
        "    # Segment the text\n",
        "    try:\n",
        "        segments = tt.tokenize(text)\n",
        "    except ValueError as e:\n",
        "        print(f\"TextTiling failed: {e}\")\n",
        "        print(\"Falling back to sentence-based segmentation.\")\n",
        "        segments = [\"\\n\".join(sentences)]\n",
        "\n",
        "    # Write the segmented output to a file\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        for i, segment in enumerate(segments):\n",
        "            file.write(f\"Segment {i + 1}:\\n{segment}\\n\\n\")\n",
        "\n",
        "# Input and output file paths\n",
        "input_file = \"/content/Kalsi sir transcript.txt\"  # Replace with your input file path\n",
        "output_file = \"segmented_transcript.txt\"  # Output file path\n",
        "\n",
        "# Segment the transcript\n",
        "segment_transcript(input_file, output_file)\n",
        "\n",
        "print(f\"Segmented transcript saved to {output_file}\")"
      ],
      "metadata": {
        "id": "GK5MjVLR8iLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize.texttiling import TextTilingTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk import download\n",
        "\n",
        "# Download necessary NLTK data\n",
        "download('punkt')\n",
        "download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the text by removing stopwords and tokenizing into sentences.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    sentences = sent_tokenize(text)\n",
        "    cleaned_sentences = []\n",
        "    for sentence in sentences:\n",
        "        words = [word.lower() for word in word_tokenize(sentence) if word.isalnum() and word.lower() not in stop_words]\n",
        "        cleaned_sentences.append(\" \".join(words))\n",
        "    return cleaned_sentences\n",
        "\n",
        "def extract_topic(segment, top_n=3):\n",
        "    \"\"\"\n",
        "    Extract the top keywords from a segment to use as the topic label.\n",
        "    \"\"\"\n",
        "    vectorizer = TfidfVectorizer(max_features=10)\n",
        "    tfidf_matrix = vectorizer.fit_transform([segment])\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    tfidf_scores = tfidf_matrix.toarray()[0]\n",
        "    top_keywords = [feature_names[i] for i in tfidf_scores.argsort()[-top_n:][::-1]]\n",
        "    return \", \".join(top_keywords)\n",
        "\n",
        "def segment_transcript(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Segment the transcript using the TextTiling algorithm and assign topic labels.\n",
        "    \"\"\"\n",
        "    # Read the input file\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        transcript = file.read()\n",
        "\n",
        "    # Preprocess the text\n",
        "    sentences = preprocess_text(transcript)\n",
        "\n",
        "    # Combine sentences into a single string for TextTiling\n",
        "    text = \"\\n\\n\".join(sentences)\n",
        "\n",
        "    # Initialize TextTilingTokenizer\n",
        "    tt = TextTilingTokenizer()\n",
        "\n",
        "    # Segment the text\n",
        "    try:\n",
        "        segments = tt.tokenize(text)\n",
        "    except ValueError as e:\n",
        "        print(f\"TextTiling failed: {e}\")\n",
        "        print(\"Falling back to sentence-based segmentation.\")\n",
        "        segments = [\"\\n\".join(sentences)]\n",
        "\n",
        "    # Write the segmented output to a file with topic labels\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        for i, segment in enumerate(segments):\n",
        "            # Extract the topic for the segment\n",
        "            topic = extract_topic(segment)\n",
        "            file.write(f\"Segment {i + 1} - Topic: {topic}\\n\")\n",
        "            file.write(f\"{segment}\\n\\n\")\n",
        "\n",
        "# Input and output file paths\n",
        "input_file = \"/content/Kalsi sir transcript.txt\"  # Replace with your input file path\n",
        "output_file = \"segmented_transcript.txt\"  # Output file path\n",
        "\n",
        "# Segment the transcript\n",
        "segment_transcript(input_file, output_file)\n",
        "\n",
        "print(f\"Segmented transcript saved to {output_file}\")"
      ],
      "metadata": {
        "id": "G1ROMSBB9UF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def preprocess(text):\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove punctuation and stopwords\n",
        "    tokens = [word for word in tokens if word.isalnum() and word not in stopwords.words('english')]\n",
        "\n",
        "    # Lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Example transcript\n",
        "transcript = \"\"\"\n",
        "Your transcript text goes here. It can be multiple paragraphs long.\n",
        "This is just an example to show how the preprocessing works.\n",
        "\"\"\"\n",
        "\n",
        "# Preprocess the transcript\n",
        "processed_transcript = preprocess(transcript)"
      ],
      "metadata": {
        "id": "zukZoySN91h-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary and corpus\n",
        "dictionary = Dictionary([processed_transcript])\n",
        "corpus = [dictionary.doc2bow(processed_transcript)]\n",
        "\n",
        "# Train an LDA model\n",
        "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
        "\n",
        "# Tokenize the transcript into sentences\n",
        "sentences = sent_tokenize(transcript)\n",
        "\n",
        "# Segment the transcript based on topics\n",
        "segments = []\n",
        "for sentence in sentences:\n",
        "    bow = dictionary.doc2bow(preprocess(sentence))\n",
        "    topic_distribution = lda_model.get_document_topics(bow)\n",
        "    dominant_topic = max(topic_distribution, key=lambda x: x[1])[0]\n",
        "    segments.append((sentence, dominant_topic))\n",
        "\n",
        "# Write the segments to a text file\n",
        "with open(\"segmented_transcript_lda.txt\", \"w\") as output_file:\n",
        "    for sentence, topic in segments:\n",
        "        output_file.write(f\"Topic {topic}: {sentence}\\n\\n\")\n",
        "\n",
        "print(\"Segmentation complete. Output saved to 'segmented_transcript_lda.txt'.\")"
      ],
      "metadata": {
        "id": "uy3vBjq49kM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora import Dictionary  # Import Dictionary from gensim.corpora"
      ],
      "metadata": {
        "id": "_L1LHFD5_sbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import LdaModel"
      ],
      "metadata": {
        "id": "J_CzAYQ4_-ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "id": "AA9X0Ih0AJqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalnum() and word not in stopwords.words('english')]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return tokens\n",
        "\n",
        "# Load the transcript from a file\n",
        "with open(\"Kalsi sir transcript.txt\", \"r\") as file:\n",
        "    transcript = file.read()\n",
        "\n",
        "# Preprocess the transcript\n",
        "processed_transcript = preprocess(transcript)\n",
        "\n",
        "# Create a dictionary and corpus\n",
        "dictionary = Dictionary([processed_transcript])\n",
        "corpus = [dictionary.doc2bow(processed_transcript)]\n",
        "\n",
        "# Train an LDA model\n",
        "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
        "\n",
        "# Tokenize the transcript into sentences\n",
        "sentences = sent_tokenize(transcript)\n",
        "\n",
        "# Segment the transcript based on topics\n",
        "segments = []\n",
        "for sentence in sentences:\n",
        "    bow = dictionary.doc2bow(preprocess(sentence))\n",
        "    topic_distribution = lda_model.get_document_topics(bow)\n",
        "    dominant_topic = max(topic_distribution, key=lambda x: x[1])[0]\n",
        "    segments.append((sentence, dominant_topic))\n",
        "\n",
        "# Write the segments to a text file\n",
        "with open(\"segmented_transcript_lda.txt\", \"w\") as output_file:\n",
        "    for sentence, topic in segments:\n",
        "        output_file.write(f\"Topic {topic}: {sentence}\\n\\n\")\n",
        "\n",
        "print(\"Segmentation complete. Output saved to 'segmented_transcript_lda.txt'.\")"
      ],
      "metadata": {
        "id": "-tB_y3hp-cnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bertopic"
      ],
      "metadata": {
        "id": "NZnvlXW5CabN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "\n",
        "# Load the transcript from a text file\n",
        "input_file = \"/content/Kalsi sir transcript.txt\"  # Replace with your input file path\n",
        "with open(input_file, \"r\") as file:\n",
        "    transcript = file.read()\n",
        "\n",
        "# Split the transcript into sentences\n",
        "sentences = transcript.split('. ')  # Simple split for demonstration\n",
        "sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "# Train BERTopic model\n",
        "topic_model = BERTopic()\n",
        "topics, _ = topic_model.fit_transform(sentences)\n",
        "\n",
        "# Get topic information\n",
        "topic_info = topic_model.get_topic_info()\n",
        "\n",
        "# Save the topics to a file\n",
        "output_file = \"bertopic_clustered_transcript.txt\"  # Replace with your desired output file path\n",
        "with open(output_file, \"w\") as f:\n",
        "    for topic_id in topic_info['Topic']:\n",
        "        if topic_id != -1:  # Ignore outliers\n",
        "            f.write(f\"Topic {topic_id}:\\n\")\n",
        "            for sentence, assigned_topic in zip(sentences, topics):\n",
        "                if assigned_topic == topic_id:\n",
        "                    f.write(f\" - {sentence}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "print(f\"Clustering complete. Output saved to '{output_file}'.\")"
      ],
      "metadata": {
        "id": "VtoMF9xq_UfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "import nltk\n",
        "\n",
        "# Download the Punkt tokenizer for sentence splitting\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the transcript from a text file\n",
        "input_file = \"/content/Kalsi sir transcript.txt\"  # Replace with your input file path\n",
        "with open(input_file, \"r\") as file:\n",
        "    transcript = file.read()\n",
        "\n",
        "# Split the transcript into sentences using NLTK's sent_tokenize\n",
        "sentences = nltk.sent_tokenize(transcript)  # More robust sentence splitting\n",
        "sentences = [s.strip() for s in sentences if s.strip()]  # Remove empty strings\n",
        "\n",
        "# Debugging: Print the sentences to verify\n",
        "print(\"Sentences extracted from the transcript:\")\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"{i + 1}: {sentence}\")\n",
        "\n",
        "# Check if sentences are empty\n",
        "if not sentences:\n",
        "    raise ValueError(\"No sentences found in the transcript. Please check the input file.\")\n",
        "\n",
        "# Train BERTopic model\n",
        "topic_model = BERTopic()\n",
        "topics, _ = topic_model.fit_transform(sentences)\n",
        "\n",
        "# Get topic information\n",
        "topic_info = topic_model.get_topic_info()\n",
        "\n",
        "# Save the topics to a file\n",
        "output_file = \"bertopic_clustered_transcript.txt\"  # Replace with your desired output file path\n",
        "with open(output_file, \"w\") as f:\n",
        "    for topic_id in topic_info['Topic']:\n",
        "        if topic_id != -1:  # Ignore outliers\n",
        "            f.write(f\"Topic {topic_id}:\\n\")\n",
        "            for sentence, assigned_topic in zip(sentences, topics):\n",
        "                if assigned_topic == topic_id:\n",
        "                    f.write(f\" - {sentence}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "print(f\"Clustering complete. Output saved to '{output_file}'.\")"
      ],
      "metadata": {
        "id": "Og8C3fT7CYYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "import nltk\n",
        "\n",
        "# Download the Punkt tokenizer for sentence splitting\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the transcript from a text file\n",
        "input_file = \"/content/Kalsi sir transcript.txt\"  # Replace with your input file path\n",
        "with open(input_file, \"r\") as file:\n",
        "    transcript = file.read()\n",
        "\n",
        "# Check if the transcript is empty\n",
        "if not transcript.strip():\n",
        "    raise ValueError(\"The input file is empty. Please provide a valid transcript.\")\n",
        "\n",
        "# Split the transcript into sentences using NLTK's sent_tokenize\n",
        "sentences = nltk.sent_tokenize(transcript)  # More robust sentence splitting\n",
        "sentences = [s.strip() for s in sentences if s.strip()]  # Remove empty strings\n",
        "\n",
        "# Debugging: Print the sentences to verify\n",
        "print(\"Sentences extracted from the transcript:\")\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"{i + 1}: {sentence}\")\n",
        "\n",
        "# Check if sentences are empty\n",
        "if not sentences:\n",
        "    raise ValueError(\"No sentences found in the transcript. Please check the input file.\")\n",
        "\n",
        "# Train BERTopic model\n",
        "topic_model = BERTopic()\n",
        "topics, _ = topic_model.fit_transform(sentences)\n",
        "\n",
        "# Get topic information\n",
        "topic_info = topic_model.get_topic_info()\n",
        "\n",
        "# Save the topics to a file\n",
        "output_file = \"bertopic_clustered_transcript.txt\"  # Replace with your desired output file path\n",
        "with open(output_file, \"w\") as f:\n",
        "    for topic_id in topic_info['Topic']:\n",
        "        if topic_id != -1:  # Ignore outliers\n",
        "            f.write(f\"Topic {topic_id}:\\n\")\n",
        "            for sentence, assigned_topic in zip(sentences, topics):\n",
        "                if assigned_topic == topic_id:\n",
        "                    f.write(f\" - {sentence}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "print(f\"Clustering complete. Output saved to '{output_file}'.\")"
      ],
      "metadata": {
        "id": "Swe7N5naEMpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BERTOPIC"
      ],
      "metadata": {
        "id": "IcrKVo9EGk4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "import nltk\n",
        "\n",
        "# Download the Punkt tokenizer for sentence splitting\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the transcript from a text file\n",
        "input_file = \"/content/Kalsi sir transcript.txt\"  # Replace with your input file path\n",
        "with open(input_file, \"r\") as file:\n",
        "    transcript = file.read()\n",
        "\n",
        "# Check if the transcript is empty\n",
        "if not transcript.strip():\n",
        "    raise ValueError(\"The input file is empty. Please provide a valid transcript.\")\n",
        "\n",
        "# Split the transcript into sentences using NLTK's sent_tokenize\n",
        "sentences = nltk.sent_tokenize(transcript)  # More robust sentence splitting\n",
        "sentences = [s.strip() for s in sentences if s.strip()]  # Remove empty strings\n",
        "\n",
        "# Debugging: Print the sentences to verify\n",
        "print(\"Sentences extracted from the transcript:\")\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"{i + 1}: {sentence}\")\n",
        "\n",
        "# Check if sentences are empty\n",
        "if not sentences:\n",
        "    raise ValueError(\"No sentences found in the transcript. Please check the input file.\")\n",
        "\n",
        "# Train BERTopic model\n",
        "topic_model = BERTopic()\n",
        "topics, _ = topic_model.fit_transform(sentences)\n",
        "\n",
        "# Get topic information\n",
        "topic_info = topic_model.get_topic_info()\n",
        "\n",
        "# Save the topics to a file\n",
        "output_file = \"bertopic_clustered_transcript.txt\"  # Replace with your desired output file path\n",
        "with open(output_file, \"w\") as f:\n",
        "    for topic_id in topic_info['Topic']:\n",
        "        if topic_id != -1:  # Ignore outliers\n",
        "            f.write(f\"Topic {topic_id}:\\n\")\n",
        "            for sentence, assigned_topic in zip(sentences, topics):\n",
        "                if assigned_topic == topic_id:\n",
        "                    f.write(f\" - {sentence}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "print(f\"Clustering complete. Output saved to '{output_file}'.\")"
      ],
      "metadata": {
        "id": "fiWR52TNLOIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "import nltk\n",
        "\n",
        "# Download the Punkt tokenizer for sentence splitting\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the transcript from a text file\n",
        "input_file = \"/content/Kalsi sir transcript.txt\"  # Replace with your input file path\n",
        "with open(input_file, \"r\") as file:\n",
        "    transcript = file.read()\n",
        "\n",
        "# Check if the transcript is empty\n",
        "if not transcript.strip():\n",
        "    raise ValueError(\"The input file is empty. Please provide a valid transcript.\")\n",
        "\n",
        "# Split the transcript into sentences using NLTK's sent_tokenize\n",
        "sentences = nltk.sent_tokenize(transcript)  # More robust sentence splitting\n",
        "sentences = [s.strip() for s in sentences if s.strip()]  # Remove empty strings\n",
        "\n",
        "# Debugging: Print the sentences to verify\n",
        "print(\"Sentences extracted from the transcript:\")\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"{i + 1}: {sentence}\")\n",
        "\n",
        "# Check if sentences are empty\n",
        "if not sentences:\n",
        "    raise ValueError(\"No sentences found in the transcript. Please check the input file.\")\n",
        "\n",
        "# Train BERTopic model\n",
        "topic_model = BERTopic()\n",
        "topics, _ = topic_model.fit_transform(sentences)\n",
        "\n",
        "# Get topic information\n",
        "topic_info = topic_model.get_topic_info()\n",
        "\n",
        "# Save the topics to a file\n",
        "output_file = \"bertopic_clustered_transcript.txt\"  # Replace with your desired output file path\n",
        "with open(output_file, \"w\") as f:\n",
        "    for topic_id in topic_info['Topic']:\n",
        "        if topic_id != -1:  # Ignore outliers\n",
        "            f.write(f\"Topic {topic_id}:\\n\")\n",
        "            for sentence, assigned_topic in zip(sentences, topics):\n",
        "                if assigned_topic == topic_id:\n",
        "                    f.write(f\" - {sentence}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "print(f\"Clustering complete. Output saved to '{output_file}'.\")"
      ],
      "metadata": {
        "id": "vF-u_lKJLQ7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai scikit-learn nltk"
      ],
      "metadata": {
        "id": "-csUFQJzLwH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = \"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\"  # Replace with your OpenAI API key\n",
        "\n",
        "# Load the transcript from a text file\n",
        "input_file = \"/content/Kalsi sir transcript.txt\"  # Replace with your input file path\n",
        "with open(input_file, \"r\") as file:\n",
        "    transcript = file.read()\n",
        "\n",
        "# Check if the transcript is empty\n",
        "if not transcript.strip():\n",
        "    raise ValueError(\"The input file is empty. Please provide a valid transcript.\")\n",
        "\n",
        "# Split the transcript into sentences using NLTK's sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "sentences = nltk.sent_tokenize(transcript)  # More robust sentence splitting\n",
        "sentences = [s.strip() for s in sentences if s.strip()]  # Remove empty strings\n",
        "\n",
        "# Debugging: Print the sentences to verify\n",
        "print(\"Sentences extracted from the transcript:\")\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"{i + 1}: {sentence}\")\n",
        "\n",
        "# Check if sentences are empty\n",
        "if not sentences:\n",
        "    raise ValueError(\"No sentences found in the transcript. Please check the input file.\")\n",
        "\n",
        "# Generate embeddings using OpenAI's text-embedding-ada-002 model\n",
        "def get_embeddings(texts):\n",
        "    response = openai.Embedding.create(\n",
        "        input=texts,\n",
        "        model=\"text-embedding-ada-002\"\n",
        "    )\n",
        "    return [item['embedding'] for item in response['data']]\n",
        "\n",
        "# Get embeddings for all sentences\n",
        "sentence_embeddings = get_embeddings(sentences)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "num_clusters = 5  # Number of topics\n",
        "kmeans = KMeans(n_clusters=num_clusters)\n",
        "kmeans.fit(sentence_embeddings)\n",
        "cluster_labels = kmeans.labels_\n",
        "\n",
        "# Group sentences by cluster\n",
        "clustered_sentences = {i: [] for i in range(num_clusters)}\n",
        "for sentence, label in zip(sentences, cluster_labels):\n",
        "    clustered_sentences[label].append(sentence)\n",
        "\n",
        "# Save the clusters to a file\n",
        "output_file = \"openai_clustered_transcript.txt\"  # Replace with your desired output file path\n",
        "with open(output_file, \"w\") as f:\n",
        "    for cluster_id, sentences_in_cluster in clustered_sentences.items():\n",
        "        f.write(f\"Cluster {cluster_id}:\\n\")\n",
        "        for sentence in sentences_in_cluster:\n",
        "            f.write(f\" - {sentence}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "print(f\"Clustering complete. Output saved to '{output_file}'.\")"
      ],
      "metadata": {
        "id": "Q0IAmjtTMMpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI(api_key=\"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\")  # Replace with your OpenAI API key\n",
        "\n",
        "# Load the transcript from a text file\n",
        "input_file = \"/content/Kalsi sir transcript.txt\"  # Replace with your input file path\n",
        "with open(input_file, \"r\") as file:\n",
        "    transcript = file.read()\n",
        "\n",
        "# Check if the transcript is empty\n",
        "if not transcript.strip():\n",
        "    raise ValueError(\"The input file is empty. Please provide a valid transcript.\")\n",
        "\n",
        "# Split the transcript into sentences using NLTK's sent_tokenize\n",
        "nltk.download('punkt')\n",
        "sentences = nltk.sent_tokenize(transcript)  # More robust sentence splitting\n",
        "sentences = [s.strip() for s in sentences if s.strip()]  # Remove empty strings\n",
        "\n",
        "# Debugging: Print the sentences to verify\n",
        "print(\"Sentences extracted from the transcript:\")\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"{i + 1}: {sentence}\")\n",
        "\n",
        "# Check if sentences are empty\n",
        "if not sentences:\n",
        "    raise ValueError(\"No sentences found in the transcript. Please check the input file.\")\n",
        "\n",
        "# Generate embeddings using OpenAI's text-embedding-ada-002 model\n",
        "def get_embeddings(texts):\n",
        "    response = client.embeddings.create(\n",
        "        input=texts,\n",
        "        model=\"text-embedding-ada-002\"\n",
        "    )\n",
        "    return [item.embedding for item in response.data]\n",
        "\n",
        "# Get embeddings for all sentences\n",
        "sentence_embeddings = get_embeddings(sentences)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "num_clusters = 5  # Number of topics\n",
        "kmeans = KMeans(n_clusters=num_clusters)\n",
        "kmeans.fit(sentence_embeddings)\n",
        "cluster_labels = kmeans.labels_\n",
        "\n",
        "# Group sentences by cluster\n",
        "clustered_sentences = {i: [] for i in range(num_clusters)}\n",
        "for sentence, label in zip(sentences, cluster_labels):\n",
        "    clustered_sentences[label].append(sentence)\n",
        "\n",
        "# Save the clusters to a file\n",
        "output_file = \"openai_clustered_transcript.txt\"  # Replace with your desired output file path\n",
        "with open(output_file, \"w\") as f:\n",
        "    for cluster_id, sentences_in_cluster in clustered_sentences.items():\n",
        "        f.write(f\"Cluster {cluster_id}:\\n\")\n",
        "        for sentence in sentences_in_cluster:\n",
        "            f.write(f\" - {sentence}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "print(f\"Clustering complete. Output saved to '{output_file}'.\")"
      ],
      "metadata": {
        "id": "gjhJpovHNEMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "id": "-Om9DZxYNe8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "# Load a pre-trained sentence embedding model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Load the transcript from a text file\n",
        "input_file = \"/content/Kalsi sir transcript.txt\"  # Replace with your input file path\n",
        "with open(input_file, \"r\") as file:\n",
        "    transcript = file.read()\n",
        "\n",
        "# Check if the transcript is empty\n",
        "if not transcript.strip():\n",
        "    raise ValueError(\"The input file is empty. Please provide a valid transcript.\")\n",
        "\n",
        "# Split the transcript into sentences using NLTK's sent_tokenize\n",
        "nltk.download('punkt')\n",
        "sentences = nltk.sent_tokenize(transcript)  # More robust sentence splitting\n",
        "sentences = [s.strip() for s in sentences if s.strip()]  # Remove empty strings\n",
        "\n",
        "# Debugging: Print the sentences to verify\n",
        "print(\"Sentences extracted from the transcript:\")\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"{i + 1}: {sentence}\")\n",
        "\n",
        "# Check if sentences are empty\n",
        "if not sentences:\n",
        "    raise ValueError(\"No sentences found in the transcript. Please check the input file.\")\n",
        "\n",
        "# Generate embeddings using Sentence Transformers\n",
        "sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "num_clusters = 5  # Number of topics\n",
        "kmeans = KMeans(n_clusters=num_clusters)\n",
        "kmeans.fit(sentence_embeddings)\n",
        "cluster_labels = kmeans.labels_\n",
        "\n",
        "# Group sentences by cluster\n",
        "clustered_sentences = {i: [] for i in range(num_clusters)}\n",
        "for sentence, label in zip(sentences, cluster_labels):\n",
        "    clustered_sentences[label].append(sentence)\n",
        "\n",
        "# Save the clusters to a file\n",
        "output_file = \"clustered_transcript.txt\"  # Replace with your desired output file path\n",
        "with open(output_file, \"w\") as f:\n",
        "    for cluster_id, sentences_in_cluster in clustered_sentences.items():\n",
        "        f.write(f\"Cluster {cluster_id}:\\n\")\n",
        "        for sentence in sentences_in_cluster:\n",
        "            f.write(f\" - {sentence}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "print(f\"Clustering complete. Output saved to '{output_file}'.\")"
      ],
      "metadata": {
        "id": "dTvWsY11OI1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"WebOrganizer/TopicClassifier\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"WebOrganizer/TopicClassifier\",\n",
        "    trust_remote_code=True,\n",
        "    use_memory_efficient_attention=False\n",
        ")\n",
        "\n",
        "# Read the content from the uploaded text file\n",
        "file_path = '/content/Kalsi sir transcript.txt'  # Replace with your actual file name\n",
        "with open(file_path, 'r') as file:\n",
        "    web_page = file.read()\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer([web_page], return_tensors=\"pt\")\n",
        "\n",
        "# Get the model's predictions\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Compute the probabilities and get the predicted topic\n",
        "probs = outputs.logits.softmax(dim=-1)\n",
        "predicted_topic = probs.argmax(dim=-1).item()\n",
        "\n",
        "# Print the predicted topic\n",
        "print(f\"Predicted Topic: {predicted_topic}\")\n",
        "# The predicted topic will correspond to the label (e.g., 5 for \"Hardware\")"
      ],
      "metadata": {
        "id": "v1DXjfXEOPSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"WebOrganizer/TopicClassifier\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"WebOrganizer/TopicClassifier\",\n",
        "    trust_remote_code=True,\n",
        "    use_memory_efficient_attention=False\n",
        ")\n",
        "\n",
        "# Read the content from the uploaded text file\n",
        "input_file_path = '/content/Kalsi sir transcript.txt'  # Replace with your actual file name\n",
        "output_file_path = 'predicted_segments.txt'  # Output file name\n",
        "\n",
        "# Open the input file and read its content\n",
        "with open(input_file_path, 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Split the content into segments (e.g., by paragraphs)\n",
        "segments = content.split('\\n\\n')  # Adjust the delimiter as needed\n",
        "\n",
        "# Process each segment and write the results to the output file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    for segment in segments:\n",
        "        if segment.strip():  # Skip empty segments\n",
        "            # Tokenize the segment\n",
        "            inputs = tokenizer([segment], return_tensors=\"pt\")\n",
        "\n",
        "            # Get the model's predictions\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            # Compute the probabilities and get the predicted topic\n",
        "            probs = outputs.logits.softmax(dim=-1)\n",
        "            predicted_topic = probs.argmax(dim=-1).item()\n",
        "\n",
        "            # Write the segment and its predicted topic to the output file\n",
        "            output_file.write(f\"Segment:\\n{segment}\\n\")\n",
        "            output_file.write(f\"Predicted Topic: {predicted_topic}\\n\")\n",
        "            output_file.write(\"-\" * 50 + \"\\n\")  # Separator for readability\n",
        "\n",
        "print(f\"Predicted segments have been saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "GcWjdt4eTUSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# Download necessary NLTK data (if you haven't already)\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "except LookupError:\n",
        "    nltk.download(\"punkt\")\n",
        "\n",
        "\n",
        "def segment_text(file_path, num_clusters=5, clustering_method='agglomerative'):\n",
        "    \"\"\"\n",
        "    Segments a text file into sections based on topic clustering,\n",
        "    using either K-Means or Agglomerative Clustering.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the text file.\n",
        "        num_clusters (int): The number of topic clusters to create.\n",
        "        clustering_method (str):  'kmeans' or 'agglomerative' (default: 'agglomerative').\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are topic labels (cluster numbers) and\n",
        "              values are lists of sentences belonging to that topic.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "    except FileNotFoundError:\n",
        "        return \"Error: File not found.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error reading file: {e}\"\n",
        "\n",
        "    # 1. Sentence Tokenization\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # 2. Text Vectorization (TF-IDF)\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)  # Remove common English stop words\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    # 3. Clustering\n",
        "    if clustering_method == 'kmeans':\n",
        "        from sklearn.cluster import KMeans  # Import here to avoid unnecessary dependency if not used\n",
        "\n",
        "        kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)  # Set random_state for reproducibility\n",
        "        kmeans.fit(tfidf_matrix)\n",
        "        clusters = kmeans.labels_\n",
        "\n",
        "    elif clustering_method == 'agglomerative':\n",
        "        # Convert sparse matrix to dense matrix\n",
        "        tfidf_matrix_dense = tfidf_matrix.toarray()\n",
        "\n",
        "        agglomerative = AgglomerativeClustering(n_clusters=num_clusters, linkage='ward')  # 'ward' linkage minimizes variance\n",
        "        clusters = agglomerative.fit_predict(tfidf_matrix_dense)\n",
        "\n",
        "    else:\n",
        "        return \"Error: Invalid clustering method. Choose 'kmeans' or 'agglomerative'.\"\n",
        "\n",
        "    # 4. Segment Creation\n",
        "    segmented_text = {}\n",
        "    for i, cluster in enumerate(clusters):\n",
        "        if cluster not in segmented_text:\n",
        "            segmented_text[cluster] = []\n",
        "        segmented_text[cluster].append(sentences[i])\n",
        "\n",
        "    return segmented_text\n",
        "\n",
        "\n",
        "def write_segmented_text_to_file(segmented_text, output_file=\"segmented_text.txt\"):\n",
        "    \"\"\"\n",
        "    Writes the segmented text to a file, with each topic as a section.\n",
        "\n",
        "    Args:\n",
        "        segmented_text (dict): The dictionary containing segmented text.\n",
        "        output_file (str): The name of the output file (default: \"segmented_text.txt\").\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "            for cluster, sentences in segmented_text.items():\n",
        "                outfile.write(f\"Topic {cluster + 1}:\\n\")  # Topics are numbered starting from 1\n",
        "                for sentence in sentences:\n",
        "                    outfile.write(sentence + \"\\n\")\n",
        "                outfile.write(\"\\n\")  # Add a blank line between topics\n",
        "        print(f\"Segmented text written to {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to file: {e}\")\n",
        "\n",
        "\n",
        "def clean_filename(filename):\n",
        "    \"\"\"\n",
        "    Sanitizes a filename by removing or replacing invalid characters.\n",
        "\n",
        "    Args:\n",
        "        filename (str): The original filename.\n",
        "\n",
        "    Returns:\n",
        "        str: A cleaned filename.\n",
        "    \"\"\"\n",
        "    # Replace spaces with underscores\n",
        "    filename = filename.replace(\" \", \"_\")\n",
        "    # Remove any characters that are not alphanumeric, underscores, or periods\n",
        "    filename = re.sub(r\"[^a-zA-Z0-9_.]\", \"\", filename)\n",
        "    return filename\n",
        "\n",
        "\n",
        "def main():\n",
        "    file_path = input(\"Enter the path to the text file: \")\n",
        "    num_topics = int(input(\"Enter the desired number of topics (clusters): \"))  # Get the desired number of topics from the user\n",
        "\n",
        "    clustering_method = input(\"Enter the clustering method ('kmeans' or 'agglomerative'): \").lower()  # Get the desired clustering method from the user\n",
        "\n",
        "    segmented_data = segment_text(file_path, num_topics, clustering_method)\n",
        "\n",
        "    if isinstance(segmented_data, str):  # Handle error messages from segment_text\n",
        "        print(segmented_data)\n",
        "        return\n",
        "\n",
        "    # Extract filename from path for a more descriptive output filename\n",
        "    file_name_base = file_path.split('/')[-1].split('.')[0]  # Get filename without extension\n",
        "    cleaned_filename = clean_filename(file_name_base)\n",
        "    output_file_name = f\"{cleaned_filename}_segmented.txt\"\n",
        "\n",
        "    write_segmented_text_to_file(segmented_data, output_file_name)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "d9IJo0fQUlpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "\n",
        "# Load and preprocess the text file\n",
        "file_path = '/content/Kalsi sir transcript.txt'  # Replace with your text file path\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    paragraphs = file.read().split('\\n\\n')\n",
        "\n",
        "# Convert paragraphs to TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(paragraphs)\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "# Create a graph based on similarity thresholds\n",
        "csim_threshold = 0.2  # Compare similarity value\n",
        "graph = nx.Graph()\n",
        "for i in range(len(paragraphs)):\n",
        "    for j in range(i + 1, len(paragraphs)):\n",
        "        if cosine_sim[i, j] > csim_threshold:\n",
        "            graph.add_edge(i, j, weight=cosine_sim[i, j])\n",
        "\n",
        "# Detect themes using connected components\n",
        "themes = [list(component) for component in nx.connected_components(graph)]\n",
        "\n",
        "# Generate a summary based on themes\n",
        "summary = []\n",
        "for theme in themes:\n",
        "    theme_text = \" \".join([paragraphs[i] for i in theme])\n",
        "    summary.append(theme_text)\n",
        "\n",
        "# Display themes and summary\n",
        "for idx, theme in enumerate(summary):\n",
        "    print(f\"Theme {idx + 1}:\\n{theme[:500]}...\\n{'-'*50}\")\n"
      ],
      "metadata": {
        "id": "dWn1iJdmbuQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "\n",
        "# Load and preprocess the text file\n",
        "file_path = '/content/Kalsi sir transcript.txt'  # Replace with your text file path\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    paragraphs = [p.strip() for p in file.read().split('\\n\\n') if p.strip()]\n",
        "\n",
        "# Convert paragraphs to TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(paragraphs)\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "# Create a graph based on similarity thresholds\n",
        "csim_threshold = 0.1  # Compare similarity value\n",
        "graph = nx.Graph()\n",
        "for i in range(len(paragraphs)):\n",
        "    for j in range(i + 1, len(paragraphs)):\n",
        "        if cosine_sim[i, j] > csim_threshold:\n",
        "            graph.add_edge(i, j, weight=cosine_sim[i, j])\n",
        "\n",
        "# Detect themes using connected components\n",
        "themes = [list(component) for component in nx.connected_components(graph)]\n",
        "\n",
        "# Generate a summary based on themes\n",
        "summary = []\n",
        "for theme in themes:\n",
        "    # Extract a representative paragraph for each theme (first paragraph in the theme)\n",
        "    theme_text = \" \".join([paragraphs[i] for i in theme])\n",
        "    summary.append(theme_text)\n",
        "\n",
        "# Save themes to a text file\n",
        "with open('themes_output.txt', 'w', encoding='utf-8') as out_file:\n",
        "    for idx, theme in enumerate(summary):\n",
        "        out_file.write(f\"Theme {idx + 1}:\\n{theme}\\n{'-'*50}\\n\")\n",
        "\n",
        "# Print a summary of themes\n",
        "print(f\"Detected {len(themes)} themes. Check 'themes_output.txt' for details.\")\n",
        "for idx, theme in enumerate(summary):\n",
        "    print(f\"Theme {idx + 1} (Preview):\\n{theme[:300]}...\\n{'-'*50}\")\n"
      ],
      "metadata": {
        "id": "e7Alymd25uG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cosine Similarity Matrix:\\n\", cosine_sim)"
      ],
      "metadata": {
        "id": "oQ_EKz086IqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Graph Info: Nodes = {graph.number_of_nodes()}, Edges = {graph.number_of_edges()}\")"
      ],
      "metadata": {
        "id": "7tO6tOFG60Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install networkx"
      ],
      "metadata": {
        "id": "9nZrTMos62bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def segment_transcript_local(text):\n",
        "    \"\"\"Uses a local LLM (Hugging Face) to segment the transcript into meaningful topics.\"\"\"\n",
        "\n",
        "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "    chunks = [text[i:i+1024] for i in range(0, len(text), 1024)]\n",
        "    summaries = [summarizer(chunk, max_length=150, min_length=50, do_sample=False)[0]['summary_text'] for chunk in chunks]\n",
        "\n",
        "    return summaries\n",
        "\n",
        "# Load transcript\n",
        "file_path = \"/content/transcript.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    transcript_text = file.read()\n",
        "\n",
        "# Process segmentation using local model\n",
        "segments = segment_transcript_local(transcript_text)\n",
        "\n",
        "# Save the output\n",
        "output_path = \"segmented_transcript.txt\"\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
        "    for i, segment in enumerate(segments, 1):\n",
        "        file.write(f\"Segment {i}:\\n{segment}\\n\\n\")\n",
        "\n",
        "print(f\"Segmented transcript saved to: {output_path}\")"
      ],
      "metadata": {
        "id": "cK6ymGe37PXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Modify the above code by using llm and segment the transcript based on the topic being discussed\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "def segment_transcript(transcript_path, model_name=\"facebook/bart-large-cnn\"):\n",
        "    \"\"\"Segments a transcript into topics using a summarization pipeline.\"\"\"\n",
        "\n",
        "    try:\n",
        "        with open(transcript_path, 'r', encoding='utf-8') as file:\n",
        "            transcript = file.read()\n",
        "    except FileNotFoundError:\n",
        "        return \"Error: Transcript file not found.\"\n",
        "\n",
        "    summarizer = pipeline(\"summarization\", model=model_name)\n",
        "\n",
        "    # Split the transcript into chunks suitable for the model\n",
        "    chunk_size = 1024  # Adjust as needed based on the model's limitations\n",
        "    chunks = [transcript[i:i + chunk_size] for i in range(0, len(transcript), chunk_size)]\n",
        "\n",
        "    segments = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        try:\n",
        "            summary = summarizer(chunk, max_length=150, min_length=50, do_sample=False)[0]['summary_text']\n",
        "            segments.append(f\"Segment {i+1}:\\nSummary: {summary}\\n\\nOriginal Text:\\n{chunk}\\n{'-' * 50}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing chunk {i+1}: {e}\")\n",
        "            segments.append(f\"Segment {i+1}: Error processing this segment.\\n\\n\") # Append an error message for problematic chunks\n",
        "\n",
        "    return \"\".join(segments)\n",
        "\n",
        "\n",
        "def main():\n",
        "    transcript_file = input(\"Enter the path to the transcript file: \")\n",
        "    segmented_transcript = segment_transcript(transcript_file)\n",
        "\n",
        "    if isinstance(segmented_transcript, str) and \"Error:\" in segmented_transcript:\n",
        "        print(segmented_transcript)  # Print the error message\n",
        "        return\n",
        "\n",
        "    output_file = \"segmented_transcript_output.txt\"\n",
        "    try:\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(segmented_transcript)\n",
        "        print(f\"Segmented transcript saved to {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to output file: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "DdEuvZt8_KOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "from keybert import KeyBERT  # For keyword extraction\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def segment_transcript(transcript_path):\n",
        "    \"\"\"Segments a transcript into topics using keyword extraction for dynamic topic generation.\"\"\"\n",
        "    try:\n",
        "        with open(transcript_path, 'r', encoding='utf-8') as file:\n",
        "            transcript = file.read()\n",
        "    except FileNotFoundError:\n",
        "        return \"Error: Transcript file not found.\"\n",
        "\n",
        "    # Initialize KeyBERT for keyword extraction\n",
        "    kw_model = KeyBERT()\n",
        "\n",
        "    # Split the transcript into chunks suitable for processing\n",
        "    chunk_size = 1024  # Adjust as needed\n",
        "    chunks = [transcript[i:i + chunk_size] for i in range(0, len(transcript), chunk_size)]\n",
        "\n",
        "    segments = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        try:\n",
        "            # Extract keywords/phrases as the topic for the chunk\n",
        "            keywords = kw_model.extract_keywords(\n",
        "                chunk,\n",
        "                vectorizer=CountVectorizer(stop_words='english'),  # Remove stopwords\n",
        "                top_n=1  # Get the most relevant keyword/phrase\n",
        "            )\n",
        "            assigned_topic = keywords[0][0]  # Get the top keyword/phrase\n",
        "            segments.append(f\"Segment {i+1}:\\nTopic: {assigned_topic}\\n\\nOriginal Text:\\n{chunk}\\n{'-' * 50}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing chunk {i+1}: {e}\")\n",
        "            segments.append(f\"Segment {i+1}: Error processing this segment.\\n\\n\")  # Append an error message for problematic chunks\n",
        "\n",
        "    return \"\".join(segments)\n",
        "\n",
        "def main():\n",
        "    transcript_file = input(\"Enter the path to the transcript file: \")\n",
        "    segmented_transcript = segment_transcript(transcript_file)\n",
        "\n",
        "    if isinstance(segmented_transcript, str) and \"Error:\" in segmented_transcript:\n",
        "        print(segmented_transcript)  # Print the error message\n",
        "        return\n",
        "\n",
        "    output_file = \"segmented_transcript_output.txt\"\n",
        "    try:\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(segmented_transcript)\n",
        "        print(f\"Segmented transcript saved to {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to output file: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "9-Q3FLVFIGI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keybert"
      ],
      "metadata": {
        "id": "jiRxCGtaU1j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Segmentation of transcript having semantic meaning"
      ],
      "metadata": {
        "id": "Fr5aG5rqU2T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step1: Sentence tokenization\n",
        "\n",
        "import re\n",
        "\n",
        "# Get file path from the user\n",
        "file_path = input(\"Enter the file path of your transcript (e.g., C:/Users/YourName/Documents/transcript.txt): \")\n",
        "\n",
        "# Read the transcript from the file\n",
        "try:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        transcript = file.read()\n",
        "\n",
        "    # Split transcript into sentences using regex\n",
        "    sentences = re.split(r'(?<=[.!?]) +', transcript)\n",
        "\n",
        "    # Join sentences with newlines\n",
        "    formatted_transcript = \"\\n\".join(sentences)\n",
        "\n",
        "    # Save the formatted transcript to a new file\n",
        "    output_path = file_path.replace('.txt', '_formatted.txt')\n",
        "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
        "        output_file.write(formatted_transcript)\n",
        "\n",
        "    print(f\"Formatted transcript saved successfully at: {output_path}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"The specified file path is incorrect. Please try again.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "bSSOmAmrbOsc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}